2025-05-11 05:36:27,751	INFO worker.py:1832 -- Started a local Ray instance. View the dashboard at [1m[32m127.0.0.1:8265 [39m[22m
[36m(TaskRunner pid=3237266)[0m {'actor_rollout_ref': {'actor': {'checkpoint': {'contents': ['model',
[36m(TaskRunner pid=3237266)[0m                                                              'optimizer',
[36m(TaskRunner pid=3237266)[0m                                                              'extra']},
[36m(TaskRunner pid=3237266)[0m                                  'clip_ratio': 0.2,
[36m(TaskRunner pid=3237266)[0m                                  'clip_ratio_c': 3.0,
[36m(TaskRunner pid=3237266)[0m                                  'clip_ratio_high': 0.2,
[36m(TaskRunner pid=3237266)[0m                                  'clip_ratio_low': 0.2,
[36m(TaskRunner pid=3237266)[0m                                  'entropy_coeff': 0,
[36m(TaskRunner pid=3237266)[0m                                  'fsdp_config': {'fsdp_size': -1,
[36m(TaskRunner pid=3237266)[0m                                                  'optimizer_offload': False,
[36m(TaskRunner pid=3237266)[0m                                                  'param_offload': False,
[36m(TaskRunner pid=3237266)[0m                                                  'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=3237266)[0m                                  'grad_clip': 1.0,
[36m(TaskRunner pid=3237266)[0m                                  'kl_loss_coef': 0.001,
[36m(TaskRunner pid=3237266)[0m                                  'kl_loss_type': 'low_var_kl',
[36m(TaskRunner pid=3237266)[0m                                  'loss_agg_mode': 'seq-mean-token-sum-norm',
[36m(TaskRunner pid=3237266)[0m                                  'optim': {'lr': 1e-06,
[36m(TaskRunner pid=3237266)[0m                                            'lr_warmup_steps': -1,
[36m(TaskRunner pid=3237266)[0m                                            'lr_warmup_steps_ratio': 0.0,
[36m(TaskRunner pid=3237266)[0m                                            'min_lr_ratio': None,
[36m(TaskRunner pid=3237266)[0m                                            'total_training_steps': -1,
[36m(TaskRunner pid=3237266)[0m                                            'warmup_style': 'constant',
[36m(TaskRunner pid=3237266)[0m                                            'weight_decay': 0.01},
[36m(TaskRunner pid=3237266)[0m                                  'ppo_epochs': 1,
[36m(TaskRunner pid=3237266)[0m                                  'ppo_max_token_len_per_gpu': 16384,
[36m(TaskRunner pid=3237266)[0m                                  'ppo_micro_batch_size': None,
[36m(TaskRunner pid=3237266)[0m                                  'ppo_micro_batch_size_per_gpu': 16,
[36m(TaskRunner pid=3237266)[0m                                  'ppo_mini_batch_size': 64,
[36m(TaskRunner pid=3237266)[0m                                  'shuffle': False,
[36m(TaskRunner pid=3237266)[0m                                  'strategy': 'fsdp',
[36m(TaskRunner pid=3237266)[0m                                  'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=3237266)[0m                                  'use_dynamic_bsz': False,
[36m(TaskRunner pid=3237266)[0m                                  'use_kl_loss': True,
[36m(TaskRunner pid=3237266)[0m                                  'use_torch_compile': True},
[36m(TaskRunner pid=3237266)[0m                        'hybrid_engine': True,
[36m(TaskRunner pid=3237266)[0m                        'model': {'enable_gradient_checkpointing': True,
[36m(TaskRunner pid=3237266)[0m                                  'external_lib': None,
[36m(TaskRunner pid=3237266)[0m                                  'override_config': {},
[36m(TaskRunner pid=3237266)[0m                                  'path': '/home/wangyc/.cache/modelscope/hub/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B',
[36m(TaskRunner pid=3237266)[0m                                  'use_liger': False,
[36m(TaskRunner pid=3237266)[0m                                  'use_remove_padding': True},
[36m(TaskRunner pid=3237266)[0m                        'ref': {'fsdp_config': {'param_offload': True,
[36m(TaskRunner pid=3237266)[0m                                                'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=3237266)[0m                                'log_prob_max_token_len_per_gpu': 16384,
[36m(TaskRunner pid=3237266)[0m                                'log_prob_micro_batch_size': None,
[36m(TaskRunner pid=3237266)[0m                                'log_prob_micro_batch_size_per_gpu': 32,
[36m(TaskRunner pid=3237266)[0m                                'log_prob_use_dynamic_bsz': False,
[36m(TaskRunner pid=3237266)[0m                                'strategy': 'fsdp',
[36m(TaskRunner pid=3237266)[0m                                'ulysses_sequence_parallel_size': 1},
[36m(TaskRunner pid=3237266)[0m                        'rollout': {'disable_log_stats': True,
[36m(TaskRunner pid=3237266)[0m                                    'do_sample': True,
[36m(TaskRunner pid=3237266)[0m                                    'dtype': 'bfloat16',
[36m(TaskRunner pid=3237266)[0m                                    'enable_chunked_prefill': True,
[36m(TaskRunner pid=3237266)[0m                                    'enforce_eager': True,
[36m(TaskRunner pid=3237266)[0m                                    'engine_kwargs': {'swap_space': None},
[36m(TaskRunner pid=3237266)[0m                                    'free_cache_engine': True,
[36m(TaskRunner pid=3237266)[0m                                    'gpu_memory_utilization': 0.6,
[36m(TaskRunner pid=3237266)[0m                                    'ignore_eos': False,
[36m(TaskRunner pid=3237266)[0m                                    'load_format': 'dummy_dtensor',
[36m(TaskRunner pid=3237266)[0m                                    'log_prob_max_token_len_per_gpu': 16384,
[36m(TaskRunner pid=3237266)[0m                                    'log_prob_micro_batch_size': None,
[36m(TaskRunner pid=3237266)[0m                                    'log_prob_micro_batch_size_per_gpu': 32,
[36m(TaskRunner pid=3237266)[0m                                    'log_prob_use_dynamic_bsz': False,
[36m(TaskRunner pid=3237266)[0m                                    'max_model_len': None,
[36m(TaskRunner pid=3237266)[0m                                    'max_num_batched_tokens': 8192,
[36m(TaskRunner pid=3237266)[0m                                    'max_num_seqs': 1024,
[36m(TaskRunner pid=3237266)[0m                                    'n': 6,
[36m(TaskRunner pid=3237266)[0m                                    'name': 'vllm',
[36m(TaskRunner pid=3237266)[0m                                    'prompt_length': 1024,
[36m(TaskRunner pid=3237266)[0m                                    'response_length': 1024,
[36m(TaskRunner pid=3237266)[0m                                    'temperature': 1.0,
[36m(TaskRunner pid=3237266)[0m                                    'tensor_model_parallel_size': 2,
[36m(TaskRunner pid=3237266)[0m                                    'top_k': -1,
[36m(TaskRunner pid=3237266)[0m                                    'top_p': 1,
[36m(TaskRunner pid=3237266)[0m                                    'use_fire_sampling': False,
[36m(TaskRunner pid=3237266)[0m                                    'val_kwargs': {'do_sample': False,
[36m(TaskRunner pid=3237266)[0m                                                   'n': 1,
[36m(TaskRunner pid=3237266)[0m                                                   'temperature': 0,
[36m(TaskRunner pid=3237266)[0m                                                   'top_k': -1,
[36m(TaskRunner pid=3237266)[0m                                                   'top_p': 1.0}}},
[36m(TaskRunner pid=3237266)[0m  'algorithm': {'adv_estimator': 'grpo',
[36m(TaskRunner pid=3237266)[0m                'gamma': 1.0,
[36m(TaskRunner pid=3237266)[0m                'kl_ctrl': {'horizon': 10000,
[36m(TaskRunner pid=3237266)[0m                            'kl_coef': 0.001,
[36m(TaskRunner pid=3237266)[0m                            'target_kl': 0.1,
[36m(TaskRunner pid=3237266)[0m                            'type': 'fixed'},
[36m(TaskRunner pid=3237266)[0m                'kl_penalty': 'kl',
[36m(TaskRunner pid=3237266)[0m                'lam': 1.0,
[36m(TaskRunner pid=3237266)[0m                'norm_adv_by_std_in_grpo': False,
[36m(TaskRunner pid=3237266)[0m                'use_kl_in_reward': False},
[36m(TaskRunner pid=3237266)[0m  'critic': {'checkpoint': {'contents': ['model', 'optimizer', 'extra']},
[36m(TaskRunner pid=3237266)[0m             'cliprange_value': 0.5,
[36m(TaskRunner pid=3237266)[0m             'forward_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=3237266)[0m             'forward_micro_batch_size': None,
[36m(TaskRunner pid=3237266)[0m             'forward_micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=3237266)[0m             'grad_clip': 1.0,
[36m(TaskRunner pid=3237266)[0m             'model': {'enable_gradient_checkpointing': True,
[36m(TaskRunner pid=3237266)[0m                       'external_lib': None,
[36m(TaskRunner pid=3237266)[0m                       'fsdp_config': {'fsdp_size': -1,
[36m(TaskRunner pid=3237266)[0m                                       'optimizer_offload': False,
[36m(TaskRunner pid=3237266)[0m                                       'param_offload': False,
[36m(TaskRunner pid=3237266)[0m                                       'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=3237266)[0m                       'override_config': {},
[36m(TaskRunner pid=3237266)[0m                       'path': '~/models/deepseek-llm-7b-chat',
[36m(TaskRunner pid=3237266)[0m                       'tokenizer_path': '/home/wangyc/.cache/modelscope/hub/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B',
[36m(TaskRunner pid=3237266)[0m                       'use_remove_padding': False},
[36m(TaskRunner pid=3237266)[0m             'optim': {'lr': 1e-05,
[36m(TaskRunner pid=3237266)[0m                       'lr_warmup_steps_ratio': 0.0,
[36m(TaskRunner pid=3237266)[0m                       'min_lr_ratio': None,
[36m(TaskRunner pid=3237266)[0m                       'total_training_steps': -1,
[36m(TaskRunner pid=3237266)[0m                       'warmup_style': 'constant',
[36m(TaskRunner pid=3237266)[0m                       'weight_decay': 0.01},
[36m(TaskRunner pid=3237266)[0m             'ppo_epochs': 1,
[36m(TaskRunner pid=3237266)[0m             'ppo_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=3237266)[0m             'ppo_micro_batch_size': None,
[36m(TaskRunner pid=3237266)[0m             'ppo_micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=3237266)[0m             'ppo_mini_batch_size': 64,
[36m(TaskRunner pid=3237266)[0m             'rollout_n': 6,
[36m(TaskRunner pid=3237266)[0m             'shuffle': False,
[36m(TaskRunner pid=3237266)[0m             'strategy': 'fsdp',
[36m(TaskRunner pid=3237266)[0m             'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=3237266)[0m             'use_dynamic_bsz': False},
[36m(TaskRunner pid=3237266)[0m  'custom_reward_function': {'name': 'compute_score', 'path': None},
[36m(TaskRunner pid=3237266)[0m  'data': {'custom_cls': {'name': None, 'path': None},
[36m(TaskRunner pid=3237266)[0m           'filter_overlong_prompts': False,
[36m(TaskRunner pid=3237266)[0m           'filter_overlong_prompts_workers': 1,
[36m(TaskRunner pid=3237266)[0m           'image_key': 'images',
[36m(TaskRunner pid=3237266)[0m           'max_prompt_length': 1024,
[36m(TaskRunner pid=3237266)[0m           'max_response_length': 1024,
[36m(TaskRunner pid=3237266)[0m           'prompt_key': 'prompt',
[36m(TaskRunner pid=3237266)[0m           'return_raw_chat': False,
[36m(TaskRunner pid=3237266)[0m           'return_raw_input_ids': False,
[36m(TaskRunner pid=3237266)[0m           'reward_fn_key': 'data_source',
[36m(TaskRunner pid=3237266)[0m           'shuffle': True,
[36m(TaskRunner pid=3237266)[0m           'tokenizer': None,
[36m(TaskRunner pid=3237266)[0m           'train_batch_size': 128,
[36m(TaskRunner pid=3237266)[0m           'train_files': '/home/wangyc/verl/data/jec-qa-1-multi-choice/train.parquet',
[36m(TaskRunner pid=3237266)[0m           'truncation': 'error',
[36m(TaskRunner pid=3237266)[0m           'val_batch_size': 1312,
[36m(TaskRunner pid=3237266)[0m           'val_files': '/home/wangyc/verl/data/jec-qa-1-multi-choice/test.parquet',
[36m(TaskRunner pid=3237266)[0m           'video_key': 'videos'},
[36m(TaskRunner pid=3237266)[0m  'ray_init': {'num_cpus': None},
[36m(TaskRunner pid=3237266)[0m  'reward_model': {'enable': False,
[36m(TaskRunner pid=3237266)[0m                   'forward_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=3237266)[0m                   'max_length': None,
[36m(TaskRunner pid=3237266)[0m                   'micro_batch_size': None,
[36m(TaskRunner pid=3237266)[0m                   'micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=3237266)[0m                   'model': {'external_lib': None,
[36m(TaskRunner pid=3237266)[0m                             'fsdp_config': {'fsdp_size': -1,
[36m(TaskRunner pid=3237266)[0m                                             'param_offload': False,
[36m(TaskRunner pid=3237266)[0m                                             'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=3237266)[0m                             'input_tokenizer': '/home/wangyc/.cache/modelscope/hub/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B',
[36m(TaskRunner pid=3237266)[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',
[36m(TaskRunner pid=3237266)[0m                             'use_remove_padding': False},
[36m(TaskRunner pid=3237266)[0m                   'reward_manager': 'naive',
[36m(TaskRunner pid=3237266)[0m                   'strategy': 'fsdp',
[36m(TaskRunner pid=3237266)[0m                   'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=3237266)[0m                   'use_dynamic_bsz': False},
[36m(TaskRunner pid=3237266)[0m  'trainer': {'balance_batch': True,
[36m(TaskRunner pid=3237266)[0m              'critic_warmup': 0,
[36m(TaskRunner pid=3237266)[0m              'default_hdfs_dir': None,
[36m(TaskRunner pid=3237266)[0m              'default_local_dir': 'checkpoints/qwen2.5-7b-grpo-hard-mcq',
[36m(TaskRunner pid=3237266)[0m              'del_local_ckpt_after_load': False,
[36m(TaskRunner pid=3237266)[0m              'experiment_name': 'qwen2.5-7b-grpo-mcq',
[36m(TaskRunner pid=3237266)[0m              'log_val_generations': 0,
[36m(TaskRunner pid=3237266)[0m              'logger': ['console', 'wandb'],
[36m(TaskRunner pid=3237266)[0m              'max_actor_ckpt_to_keep': None,
[36m(TaskRunner pid=3237266)[0m              'max_critic_ckpt_to_keep': None,
[36m(TaskRunner pid=3237266)[0m              'n_gpus_per_node': 8,
[36m(TaskRunner pid=3237266)[0m              'nnodes': 1,
[36m(TaskRunner pid=3237266)[0m              'project_name': 'Lawyer-Zero',
[36m(TaskRunner pid=3237266)[0m              'ray_wait_register_center_timeout': 300,
[36m(TaskRunner pid=3237266)[0m              'resume_from_path': None,
[36m(TaskRunner pid=3237266)[0m              'resume_mode': 'auto',
[36m(TaskRunner pid=3237266)[0m              'save_freq': 50,
[36m(TaskRunner pid=3237266)[0m              'test_freq': 10,
[36m(TaskRunner pid=3237266)[0m              'total_epochs': 2,
[36m(TaskRunner pid=3237266)[0m              'total_training_steps': None,
[36m(TaskRunner pid=3237266)[0m              'val_before_train': True}}
[36m(TaskRunner pid=3237266)[0m WARNING: val_batch_size is deprecated. Validation datasets are sent to inference engines as a whole batch, which will schedule the memory themselves.
[36m(TaskRunner pid=3237266)[0m [validate_config] All configuration checks passed successfully!
[36m(TaskRunner pid=3237266)[0m dataset len: 8448
[36m(TaskRunner pid=3237266)[0m dataset len: 2113
[36m(TaskRunner pid=3237266)[0m DeprecationWarning: `ray.state.available_resources_per_node` is a private attribute and access will be removed in a future Ray version.
[36m(TaskRunner pid=3237266)[0m WARNING:2025-05-11 05:36:38,472:Waiting for register center actor 5P2uS9_register_center to be ready. Elapsed time: 0 seconds out of 300 seconds.
[36m(WorkerDict pid=3246126)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=3246144)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(WorkerDict pid=3246126)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 28.46it/s]
[36m(WorkerDict pid=3246144)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 28.19it/s]
[36m(WorkerDict pid=3246144)[0m [rank6]:[W511 05:36:53.251110853 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 6]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[36m(WorkerDict pid=3240251)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=3246154)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3246154)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][32m [repeated 4x across cluster][0m
[36m(WorkerDict pid=3246142)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 28.70it/s][32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=3246154)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 28.49it/s][32m [repeated 4x across cluster][0m
[36m(WorkerDict pid=3246142)[0m [rank5]:[W511 05:36:54.637957297 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 5]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3246154)[0m Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.26s/it]
[36m(WorkerDict pid=3246133)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3246154)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=3240251)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.16s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.32s/it]
[36m(WorkerDict pid=3246126)[0m Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.52s/it][32m [repeated 7x across cluster][0m
[36m(TaskRunner pid=3237266)[0m Size of train dataloader: 66
[36m(TaskRunner pid=3237266)[0m Total training steps: 132
[36m(WorkerDict pid=3246133)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
[36m(WorkerDict pid=3240251)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=3240251)[0m   "architectures": [
[36m(WorkerDict pid=3240251)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=3240251)[0m   ],
[36m(WorkerDict pid=3240251)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=3240251)[0m   "bos_token_id": 151646,
[36m(WorkerDict pid=3240251)[0m   "eos_token_id": 151643,
[36m(WorkerDict pid=3240251)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=3240251)[0m   "hidden_size": 3584,
[36m(WorkerDict pid=3240251)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=3240251)[0m   "intermediate_size": 18944,
[36m(WorkerDict pid=3240251)[0m   "max_position_embeddings": 131072,
[36m(WorkerDict pid=3240251)[0m   "max_window_layers": 28,
[36m(WorkerDict pid=3240251)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=3240251)[0m   "num_attention_heads": 28,
[36m(WorkerDict pid=3240251)[0m   "num_hidden_layers": 28,
[36m(WorkerDict pid=3240251)[0m   "num_key_value_heads": 4,
[36m(WorkerDict pid=3240251)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=3240251)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=3240251)[0m   "rope_scaling": null,
[36m(WorkerDict pid=3240251)[0m   "rope_theta": 10000,
[36m(WorkerDict pid=3240251)[0m   "sliding_window": 4096,
[36m(WorkerDict pid=3240251)[0m   "tie_word_embeddings": false,
[36m(WorkerDict pid=3240251)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=3240251)[0m   "transformers_version": "4.51.3",
[36m(WorkerDict pid=3240251)[0m   "use_cache": true,
[36m(WorkerDict pid=3240251)[0m   "use_mrope": false,
[36m(WorkerDict pid=3240251)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=3240251)[0m   "vocab_size": 152064
[36m(WorkerDict pid=3240251)[0m }
[36m(WorkerDict pid=3240251)[0m 
[36m(WorkerDict pid=3240251)[0m NCCL version 2.21.5+cuda12.4
[36m(WorkerDict pid=3240251)[0m Qwen2ForCausalLM contains 7.62B parameters
[36m(WorkerDict pid=3240251)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f0632412830>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f0632412710>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=3240251)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3246154)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(WorkerDict pid=3240251)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=3240251)[0m   "architectures": [
[36m(WorkerDict pid=3240251)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=3240251)[0m   ],
[36m(WorkerDict pid=3240251)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=3240251)[0m   "bos_token_id": 151646,
[36m(WorkerDict pid=3240251)[0m   "eos_token_id": 151643,
[36m(WorkerDict pid=3240251)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=3240251)[0m   "hidden_size": 3584,
[36m(WorkerDict pid=3240251)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=3240251)[0m   "intermediate_size": 18944,
[36m(WorkerDict pid=3240251)[0m   "max_position_embeddings": 131072,
[36m(WorkerDict pid=3240251)[0m   "max_window_layers": 28,
[36m(WorkerDict pid=3240251)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=3240251)[0m   "num_attention_heads": 28,
[36m(WorkerDict pid=3240251)[0m   "num_hidden_layers": 28,
[36m(WorkerDict pid=3240251)[0m   "num_key_value_heads": 4,
[36m(WorkerDict pid=3240251)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=3240251)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=3240251)[0m   "rope_scaling": null,
[36m(WorkerDict pid=3240251)[0m   "rope_theta": 10000,
[36m(WorkerDict pid=3240251)[0m   "sliding_window": 4096,
[36m(WorkerDict pid=3240251)[0m   "tie_word_embeddings": false,
[36m(WorkerDict pid=3240251)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=3240251)[0m   "transformers_version": "4.51.3",
[36m(WorkerDict pid=3240251)[0m   "use_cache": true,
[36m(WorkerDict pid=3240251)[0m   "use_mrope": false,
[36m(WorkerDict pid=3240251)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=3240251)[0m   "vocab_size": 152064
[36m(WorkerDict pid=3240251)[0m }
[36m(WorkerDict pid=3240251)[0m 
[36m(WorkerDict pid=3246119)[0m wrap_policy: functools.partial(<function _or_policy at 0x7fae0411e830>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7fae0411e710>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3246119)[0m Actor use_remove_padding=True[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3240251)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
[36m(WorkerDict pid=3246154)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
[36m(WorkerDict pid=3240251)[0m Qwen2ForCausalLM contains 7.62B parameters
[36m(WorkerDict pid=3240251)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f0632412830>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f0632412710>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=3246119)[0m Total steps: 132, num_warmup_steps: 0
[36m(WorkerDict pid=3246119)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3246126)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=3246119)[0m wrap_policy: functools.partial(<function _or_policy at 0x7fae0411e830>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7fae0411e710>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3246133)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3240251)[0m Before building vllm rollout, memory allocated (GB): 3.5471720695495605, memory reserved (GB): 19.66015625
[36m(WorkerDict pid=3246133)[0m WARNING 05-11 05:37:27 [cuda.py:96] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=3240251)[0m Total steps: 132, num_warmup_steps: 0[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3240251)[0m Actor use_remove_padding=True[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=3246133)[0m WARNING 05-11 05:37:28 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f1bf81bb7f0>
[36m(WorkerDict pid=3246144)[0m NCCL version 2.21.5+cuda12.4
[36m(WorkerDict pid=3240251)[0m kwargs: {'n': 6, 'logprobs': 0, 'max_tokens': 1024, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
[36m(WorkerDict pid=3240251)[0m After building vllm rollout, memory allocated (GB): 16.606098175048828, memory reserved (GB): 20.435546875
[36m(WorkerDict pid=3240251)[0m After building sharding manager, memory allocated (GB): 16.606098175048828, memory reserved (GB): 20.435546875
[36m(WorkerDict pid=3240251)[0m WARNING 05-11 05:37:27 [cuda.py:96] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3240251)[0m /home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=3240251)[0m   warnings.warn(
[36m(WorkerDict pid=3246126)[0m Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.56s/it][32m [repeated 7x across cluster][0m
[36m(TaskRunner pid=3237266)[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
[36m(TaskRunner pid=3237266)[0m wandb: Currently logged in as: eang333cms (eang333cms-peking-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(TaskRunner pid=3237266)[0m wandb: Tracking run with wandb version 0.19.9
[36m(TaskRunner pid=3237266)[0m wandb: Run data is saved locally in /home/wangyc/verl/wandb/run-20250511_053741-5ok9zdaj
[36m(TaskRunner pid=3237266)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(TaskRunner pid=3237266)[0m wandb: Syncing run qwen2.5-7b-grpo-mcq
[36m(TaskRunner pid=3237266)[0m wandb: ⭐️ View project at https://wandb.ai/eang333cms-peking-university/Lawyer-Zero
[36m(TaskRunner pid=3237266)[0m wandb: 🚀 View run at https://wandb.ai/eang333cms-peking-university/Lawyer-Zero/runs/5ok9zdaj
[36m(WorkerDict pid=3240251)[0m WARNING 05-11 05:37:28 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f04cc248100>[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3246140)[0m NCCL version 2.21.5+cuda12.4[32m [repeated 2x across cluster][0m
[36m(TaskRunner pid=3237266)[0m Using LocalLogger is deprecated. The constructor API will change 
[36m(TaskRunner pid=3237266)[0m Checkpoint tracker file does not exist: %s /home/wangyc/verl/checkpoints/qwen2.5-7b-grpo-hard-mcq/latest_checkpointed_iteration.txt
[36m(TaskRunner pid=3237266)[0m Training from scratch
[36m(TaskRunner pid=3237266)[0m test_gen_batch meta info: {'eos_token_id': 151643, 'pad_token_id': 151643, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
[36m(WorkerDict pid=3246154)[0m kwargs: {'n': 6, 'logprobs': 0, 'max_tokens': 1024, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}[32m [repeated 7x across cluster][0m
[36m(TaskRunner pid=3237266)[0m validation generation end
[36m(TaskRunner pid=3237266)[0m [prompt] 用户和助手之间的对话。用户提出一个问题，由助手来回答。助手首先在脑海中逐步思考推理过程，然后向用户提供答案。推理过程和答案分别用<思考> </思考>和<回答> </回答>标签括起来，即，<思考> 推理过程 </思考><回答> 答案 </回答><｜User｜>你是一名法学专家。现在请你解答司法考试中的一道选择题，请你找出所有正确的选项。每道题可能有一个或者多个正确答案。在解答之前，你需要先针对每个提供的选项给出详细的解释。你需要在回答的最后用大括号圈出给出的答案，例如"{B}"或者"{ABD}"。
[36m(TaskRunner pid=3237266)[0m 
[36m(TaskRunner pid=3237266)[0m 问题：甲某以为境外窃取、刺探、收买国家秘密的故意，乙以非法获取国家秘密的故意，共谋共同窃取、刺探、收买国家秘密，对于甲乙二人的行为的认定，下列正确的选项是:
[36m(TaskRunner pid=3237266)[0m 
[36m(TaskRunner pid=3237266)[0m 选项：
[36m(TaskRunner pid=3237266)[0m A: 甲与乙因故意不同，不成立共同犯罪
[36m(TaskRunner pid=3237266)[0m B: 甲与乙虽故意不同，仍在非法获取国家秘密罪的范围内成立共同犯罪
[36m(TaskRunner pid=3237266)[0m C: 对甲应以为境外窃取、刺探、收买国家秘密罪论处
[36m(TaskRunner pid=3237266)[0m D: 对乙以非法获取国家秘密罪论处<｜Assistant｜><think>
[36m(TaskRunner pid=3237266)[0m 
[36m(TaskRunner pid=3237266)[0m [response] 嗯，我现在要解答这个司法考试的选择题。首先，我得仔细阅读题目，理解问题。题目是关于共同犯罪的认定，涉及甲和乙的行为。
[36m(TaskRunner pid=3237266)[0m 
[36m(TaskRunner pid=3237266)[0m 题目说，甲以为境外窃取、刺探、收买国家秘密的故意，而乙则是以非法获取国家秘密的故意，他们共谋共同窃取、刺探、收买国家秘密。问题是要判断甲和乙的行为如何认定，选项有四个，A到D。
[36m(TaskRunner pid=3237266)[0m 
[36m(TaskRunner pid=3237266)[0m 首先，我需要明确共同犯罪的构成要件。共同犯罪通常需要两个以上的人共同实施犯罪行为，共同故意是关键。共同故意是指所有参与人必须共同持有的故意，而不同意只是共同犯罪的一种可能性，但不构成共同犯罪的基础。
[36m(TaskRunner pid=3237266)[0m 
[36m(TaskRunner pid=3237266)[0m 甲和乙的行为都是窃取、刺探、收买国家秘密，虽然他们的故意不同，但他们都实施了相同的犯罪行为。因此，他们有共同的犯罪行为，但各自持有不同的故意。根据刑法，共同犯罪不仅需要共同行为，还需要共同的故意。而共同的故意必须是各参与人必须共同持有的，而这里甲和乙的故意不同，所以他们没有共同的故意，因此不构成共同犯罪。所以选项A正确，B不正确。
[36m(TaskRunner pid=3237266)[0m 
[36m(TaskRunner pid=3237266)[0m 接下来，关于甲和乙分别的行为定性。甲以为境外窃取、刺探、收买国家秘密，这可能涉及到国家秘密法中的“境外”规定。根据刑法，国家秘密罪中，境外窃取、刺探、收买国家秘密的，处五年以下有期徒刑、拘役、管制或者剥夺政治权利，情节严重的，处五年以上有期徒刑。而乙的行为是非法获取国家秘密，根据刑法，非法获取国家秘密罪的量刑标准与国家秘密罪相同，都是处五年以下有期徒刑、拘役、管制或者剥夺政治权利，情节严重的，处五年以上有期徒刑。
[36m(TaskRunner pid=3237266)[0m 
[36m(TaskRunner pid=3237266)[0m 因此，甲的行为属于国家秘密罪，而乙的行为属于非法获取国家秘密罪。所以选项C正确，D不正确。
[36m(TaskRunner pid=3237266)[0m 
[36m(TaskRunner pid=3237266)[0m 总结一下，正确的选项是A和C，即选项AC。
[36m(TaskRunner pid=3237266)[0m </think>
[36m(TaskRunner pid=3237266)[0m 
[36m(TaskRunner pid=3237266)[0m <思考>
[36m(TaskRunner pid=3237266)[0m 首先，共同犯罪的构成要件包括共同行为和共同故意。甲和乙的行为都是窃取、刺探、收买国家秘密，虽然他们的故意不同，但都实施了相同的犯罪行为。根据刑法，共同犯罪不仅需要共同行为，还需要共同的故意。而共同的故意必须是各参与人必须共同持有的，这里甲和乙的故意不同，因此他们没有共同的故意，不构成共同犯罪。因此，选项A正确，B不正确。
[36m(TaskRunner pid=3237266)[0m 
[36m(TaskRunner pid=3237266)[0m 其次，甲的行为属于国家秘密罪，因为其故意是以为境外窃取、刺探、收买国家秘密。根据刑法，国家秘密罪中，境外窃取、刺探、收买国家秘密的，处五年以下有期徒刑、拘役、管制或者剥夺政治权利，情节严重的，处五年以上有期徒刑。乙的行为属于非法获取国家秘密罪，因为其故意是非法获取国家秘密。根据刑法，非法获取国家秘密罪的量刑标准与国家秘密罪相同。因此，选项C正确，D不正确。
[36m(TaskRunner pid=3237266)[0m 
[36m(TaskRunner pid=3237266)[0m 综上所述，正确的选项是A和C，即选项AC。
[36m(TaskRunner pid=3237266)[0m </思考>
[36m(TaskRunner pid=3237266)[0m 
[36m(TaskRunner pid=3237266)[0m <回答>
[36m(TaskRunner pid=3237266)[0m {AC}
[36m(TaskRunner pid=3237266)[0m </回答>
[36m(TaskRunner pid=3237266)[0m [ground_truth] ['B', 'C', 'D']
[36m(TaskRunner pid=3237266)[0m [score] 1.0
[36m(TaskRunner pid=3237266)[0m [format_reward] 1.0
[36m(TaskRunner pid=3237266)[0m [answer_reward] 0.0
[36m(TaskRunner pid=3237266)[0m [acc] 0.0
[36m(TaskRunner pid=3237266)[0m [precision] 0.5
[36m(TaskRunner pid=3237266)[0m [recall] 0.3333333333333333
[36m(TaskRunner pid=3237266)[0m [f1] 0.4
[36m(TaskRunner pid=3237266)[0m ("Initial validation metrics: {'val-aux/jec-qa-1-multi-choice/reward/mean@1': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(0.9944628395009707), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/reward/std@1': np.float64(0.0), "
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/reward/best@1/mean': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(0.9944628395009707), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/reward/best@1/std': np.float64(0.0), "
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/reward/worst@1/mean': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(0.9944628395009707), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/reward/worst@1/std': np.float64(0.0), "
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/score/mean@1': "
[36m(TaskRunner pid=3237266)[0m  "np.float64(0.9944628490298155), 'val-aux/jec-qa-1-multi-choice/score/std@1': "
[36m(TaskRunner pid=3237266)[0m  "np.float64(0.0), 'val-aux/jec-qa-1-multi-choice/score/best@1/mean': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(0.9944628490298155), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/score/best@1/std': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(9.491802621795293e-17), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/score/worst@1/mean': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(0.9944628490298155), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/score/worst@1/std': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(9.491802621795293e-17), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/format_reward/mean@1': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(0.7766209181258874), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/format_reward/std@1': np.float64(0.0), "
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/format_reward/best@1/mean': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(0.7766209181258874), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/format_reward/best@1/std': np.float64(0.0), "
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/format_reward/worst@1/mean': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(0.7766209181258874), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/format_reward/worst@1/std': np.float64(0.0), "
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/answer_reward/mean@1': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(0.21784193090392806), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/answer_reward/std@1': np.float64(0.0), "
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/answer_reward/best@1/mean': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(0.21784193090392803), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/answer_reward/best@1/std': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(3.7909413737200686e-17), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/answer_reward/worst@1/mean': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(0.21784193090392803), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/answer_reward/worst@1/std': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(3.7909413737200686e-17), '
[36m(TaskRunner pid=3237266)[0m  "'val-core/jec-qa-1-multi-choice/acc/mean@1': "
[36m(TaskRunner pid=3237266)[0m  "np.float64(0.14955040227165167), 'val-aux/jec-qa-1-multi-choice/acc/std@1': "
[36m(TaskRunner pid=3237266)[0m  "np.float64(0.0), 'val-core/jec-qa-1-multi-choice/acc/best@1/mean': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(0.14955040227165167), '
[36m(TaskRunner pid=3237266)[0m  "'val-core/jec-qa-1-multi-choice/acc/best@1/std': np.float64(0.0), "
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/acc/worst@1/mean': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(0.14955040227165167), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/acc/worst@1/std': np.float64(0.0), "
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/precision/mean@1': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(0.4351238365672819), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/precision/std@1': np.float64(0.0), "
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/precision/best@1/mean': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(0.4351238365672818), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/precision/best@1/std': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(1.1007653746283497e-17), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/precision/worst@1/mean': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(0.4351238365672818), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/precision/worst@1/std': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(1.1007653746283497e-17), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/recall/mean@1': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(0.5093863385392017), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/recall/std@1': np.float64(0.0), "
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/recall/best@1/mean': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(0.5093863385392017), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/recall/best@1/std': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(8.45934249714388e-18), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/recall/worst@1/mean': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(0.5093863385392017), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/recall/worst@1/std': "
[36m(TaskRunner pid=3237266)[0m  "np.float64(8.45934249714388e-18), 'val-aux/jec-qa-1-multi-choice/f1/mean@1': "
[36m(TaskRunner pid=3237266)[0m  "np.float64(0.43836567281905664), 'val-aux/jec-qa-1-multi-choice/f1/std@1': "
[36m(TaskRunner pid=3237266)[0m  "np.float64(0.0), 'val-aux/jec-qa-1-multi-choice/f1/best@1/mean': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(0.43836567281905664), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/f1/best@1/std': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(3.005430998985279e-17), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/f1/worst@1/mean': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(0.43836567281905664), '
[36m(TaskRunner pid=3237266)[0m  "'val-aux/jec-qa-1-multi-choice/f1/worst@1/std': "
[36m(TaskRunner pid=3237266)[0m  'np.float64(3.005430998985279e-17)}')
[36m(TaskRunner pid=3237266)[0m Training Progress:   0%|          | 0/132 [00:00<?, ?it/s]
[36m(WorkerDict pid=3246154)[0m /home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3246154)[0m   warnings.warn([32m [repeated 7x across cluster][0m
[36m(TaskRunner pid=3237266)[0m Training Progress:   1%|          | 1/132 [01:51<4:03:00, 111.30s/it]
[36m(TaskRunner pid=3237266)[0m wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
[36m(TaskRunner pid=3237266)[0m wandb:                                                                                
[36m(TaskRunner pid=3237266)[0m wandb: 
[36m(TaskRunner pid=3237266)[0m wandb: Run history:
[36m(TaskRunner pid=3237266)[0m wandb:                                       actor/entropy_loss ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                          actor/grad_norm ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                            actor/kl_coef ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                            actor/kl_loss ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                                 actor/lr ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                        actor/pg_clipfrac ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                  actor/pg_clipfrac_lower ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                            actor/pg_loss ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                             actor/ppo_kl ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                    critic/advantages/max ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                   critic/advantages/mean ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                    critic/advantages/min ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                       critic/returns/max ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                      critic/returns/mean ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                       critic/returns/min ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                       critic/rewards/max ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                      critic/rewards/mean ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                       critic/rewards/min ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                         critic/score/max ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                        critic/score/mean ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                         critic/score/min ▁
[36m(TaskRunner pid=3237266)[0m wandb:                               global_seqlen/balanced_max ▁
[36m(TaskRunner pid=3237266)[0m wandb:                               global_seqlen/balanced_min ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                        global_seqlen/max ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                       global_seqlen/mean ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                        global_seqlen/min ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                global_seqlen/minmax_diff ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                  perf/cpu_memory_used_gb ▁
[36m(TaskRunner pid=3237266)[0m wandb:                             perf/max_memory_allocated_gb ▁
[36m(TaskRunner pid=3237266)[0m wandb:                              perf/max_memory_reserved_gb ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                           perf/mfu/actor ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                          perf/throughput ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                       perf/time_per_step ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                    perf/total_num_tokens ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                 prompt_length/clip_ratio ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                        prompt_length/max ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                       prompt_length/mean ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                        prompt_length/min ▁
[36m(TaskRunner pid=3237266)[0m wandb:                               response_length/clip_ratio ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                      response_length/max ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                     response_length/mean ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                      response_length/min ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                  timing_per_token_ms/adv ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                  timing_per_token_ms/gen ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                  timing_per_token_ms/ref ▁
[36m(TaskRunner pid=3237266)[0m wandb:                         timing_per_token_ms/update_actor ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                             timing_s/adv ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                             timing_s/gen ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                    timing_s/old_log_prob ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                             timing_s/ref ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                            timing_s/step ▁
[36m(TaskRunner pid=3237266)[0m wandb:                                    timing_s/update_actor ▁
[36m(TaskRunner pid=3237266)[0m wandb:                  val-aux/jec-qa-1-multi-choice/acc/std@1 ▁
[36m(TaskRunner pid=3237266)[0m wandb:           val-aux/jec-qa-1-multi-choice/acc/worst@1/mean ▁
[36m(TaskRunner pid=3237266)[0m wandb:            val-aux/jec-qa-1-multi-choice/acc/worst@1/std ▁
[36m(TaskRunner pid=3237266)[0m wandb:  val-aux/jec-qa-1-multi-choice/answer_reward/best@1/mean ▁
[36m(TaskRunner pid=3237266)[0m wandb:   val-aux/jec-qa-1-multi-choice/answer_reward/best@1/std ▁
[36m(TaskRunner pid=3237266)[0m wandb:       val-aux/jec-qa-1-multi-choice/answer_reward/mean@1 ▁
[36m(TaskRunner pid=3237266)[0m wandb:        val-aux/jec-qa-1-multi-choice/answer_reward/std@1 ▁
[36m(TaskRunner pid=3237266)[0m wandb: val-aux/jec-qa-1-multi-choice/answer_reward/worst@1/mean ▁
[36m(TaskRunner pid=3237266)[0m wandb:  val-aux/jec-qa-1-multi-choice/answer_reward/worst@1/std ▁
[36m(TaskRunner pid=3237266)[0m wandb:             val-aux/jec-qa-1-multi-choice/f1/best@1/mean ▁
[36m(TaskRunner pid=3237266)[0m wandb:              val-aux/jec-qa-1-multi-choice/f1/best@1/std ▁
[36m(TaskRunner pid=3237266)[0m wandb:                  val-aux/jec-qa-1-multi-choice/f1/mean@1 ▁
[36m(TaskRunner pid=3237266)[0m wandb:                   val-aux/jec-qa-1-multi-choice/f1/std@1 ▁
[36m(TaskRunner pid=3237266)[0m wandb:            val-aux/jec-qa-1-multi-choice/f1/worst@1/mean ▁
[36m(TaskRunner pid=3237266)[0m wandb:             val-aux/jec-qa-1-multi-choice/f1/worst@1/std ▁
[36m(TaskRunner pid=3237266)[0m wandb:  val-aux/jec-qa-1-multi-choice/format_reward/best@1/mean ▁
[36m(TaskRunner pid=3237266)[0m wandb:   val-aux/jec-qa-1-multi-choice/format_reward/best@1/std ▁
[36m(TaskRunner pid=3237266)[0m wandb:       val-aux/jec-qa-1-multi-choice/format_reward/mean@1 ▁
[36m(TaskRunner pid=3237266)[0m wandb:        val-aux/jec-qa-1-multi-choice/format_reward/std@1 ▁
[36m(TaskRunner pid=3237266)[0m wandb: val-aux/jec-qa-1-multi-choice/format_reward/worst@1/mean ▁
[36m(TaskRunner pid=3237266)[0m wandb:  val-aux/jec-qa-1-multi-choice/format_reward/worst@1/std ▁
[36m(TaskRunner pid=3237266)[0m wandb:      val-aux/jec-qa-1-multi-choice/precision/best@1/mean ▁
[36m(TaskRunner pid=3237266)[0m wandb:       val-aux/jec-qa-1-multi-choice/precision/best@1/std ▁
[36m(TaskRunner pid=3237266)[0m wandb:           val-aux/jec-qa-1-multi-choice/precision/mean@1 ▁
[36m(TaskRunner pid=3237266)[0m wandb:            val-aux/jec-qa-1-multi-choice/precision/std@1 ▁
[36m(TaskRunner pid=3237266)[0m wandb:     val-aux/jec-qa-1-multi-choice/precision/worst@1/mean ▁
[36m(TaskRunner pid=3237266)[0m wandb:      val-aux/jec-qa-1-multi-choice/precision/worst@1/std ▁
[36m(TaskRunner pid=3237266)[0m wandb:         val-aux/jec-qa-1-multi-choice/recall/best@1/mean ▁
[36m(TaskRunner pid=3237266)[0m wandb:          val-aux/jec-qa-1-multi-choice/recall/best@1/std ▁
[36m(TaskRunner pid=3237266)[0m wandb:              val-aux/jec-qa-1-multi-choice/recall/mean@1 ▁
[36m(TaskRunner pid=3237266)[0m wandb:               val-aux/jec-qa-1-multi-choice/recall/std@1 ▁
[36m(TaskRunner pid=3237266)[0m wandb:        val-aux/jec-qa-1-multi-choice/recall/worst@1/mean ▁
[36m(TaskRunner pid=3237266)[0m wandb:         val-aux/jec-qa-1-multi-choice/recall/worst@1/std ▁
[36m(TaskRunner pid=3237266)[0m wandb:         val-aux/jec-qa-1-multi-choice/reward/best@1/mean ▁
[36m(TaskRunner pid=3237266)[0m wandb:          val-aux/jec-qa-1-multi-choice/reward/best@1/std ▁
[36m(TaskRunner pid=3237266)[0m wandb:              val-aux/jec-qa-1-multi-choice/reward/mean@1 ▁
[36m(TaskRunner pid=3237266)[0m wandb:               val-aux/jec-qa-1-multi-choice/reward/std@1 ▁
[36m(TaskRunner pid=3237266)[0m wandb:        val-aux/jec-qa-1-multi-choice/reward/worst@1/mean ▁
[36m(TaskRunner pid=3237266)[0m wandb:         val-aux/jec-qa-1-multi-choice/reward/worst@1/std ▁
[36m(TaskRunner pid=3237266)[0m wandb:          val-aux/jec-qa-1-multi-choice/score/best@1/mean ▁
[36m(TaskRunner pid=3237266)[0m wandb:           val-aux/jec-qa-1-multi-choice/score/best@1/std ▁
[36m(TaskRunner pid=3237266)[0m wandb:               val-aux/jec-qa-1-multi-choice/score/mean@1 ▁
[36m(TaskRunner pid=3237266)[0m wandb:                val-aux/jec-qa-1-multi-choice/score/std@1 ▁
[36m(TaskRunner pid=3237266)[0m wandb:         val-aux/jec-qa-1-multi-choice/score/worst@1/mean ▁
[36m(TaskRunner pid=3237266)[0m wandb:          val-aux/jec-qa-1-multi-choice/score/worst@1/std ▁
[36m(TaskRunner pid=3237266)[0m wandb:           val-core/jec-qa-1-multi-choice/acc/best@1/mean ▁
[36m(TaskRunner pid=3237266)[0m wandb:            val-core/jec-qa-1-multi-choice/acc/best@1/std ▁
[36m(TaskRunner pid=3237266)[0m wandb:                val-core/jec-qa-1-multi-choice/acc/mean@1 ▁
[36m(TaskRunner pid=3237266)[0m wandb: 
[36m(TaskRunner pid=3237266)[0m wandb: Run summary:
[36m(TaskRunner pid=3237266)[0m wandb:                                       actor/entropy_loss 538.66962
[36m(TaskRunner pid=3237266)[0m wandb:                                          actor/grad_norm 2.38082
[36m(TaskRunner pid=3237266)[0m wandb:                                            actor/kl_coef 0.001
[36m(TaskRunner pid=3237266)[0m wandb:                                            actor/kl_loss 0.0073
[36m(TaskRunner pid=3237266)[0m wandb:                                                 actor/lr 0.0
[36m(TaskRunner pid=3237266)[0m wandb:                                        actor/pg_clipfrac 0
[36m(TaskRunner pid=3237266)[0m wandb:                                  actor/pg_clipfrac_lower 0
[36m(TaskRunner pid=3237266)[0m wandb:                                            actor/pg_loss 1.04818
[36m(TaskRunner pid=3237266)[0m wandb:                                             actor/ppo_kl 0
[36m(TaskRunner pid=3237266)[0m wandb:                                    critic/advantages/max 1.51667
[36m(TaskRunner pid=3237266)[0m wandb:                                   critic/advantages/mean -0.04616
[36m(TaskRunner pid=3237266)[0m wandb:                                    critic/advantages/min -1.33333
[36m(TaskRunner pid=3237266)[0m wandb:                                       critic/returns/max 1.51667
[36m(TaskRunner pid=3237266)[0m wandb:                                      critic/returns/mean -0.04616
[36m(TaskRunner pid=3237266)[0m wandb:                                       critic/returns/min -1.33333
[36m(TaskRunner pid=3237266)[0m wandb:                                       critic/rewards/max 2
[36m(TaskRunner pid=3237266)[0m wandb:                                      critic/rewards/mean 0.63021
[36m(TaskRunner pid=3237266)[0m wandb:                                       critic/rewards/min 0
[36m(TaskRunner pid=3237266)[0m wandb:                                         critic/score/max 2
[36m(TaskRunner pid=3237266)[0m wandb:                                        critic/score/mean 0.63021
[36m(TaskRunner pid=3237266)[0m wandb:                                         critic/score/min 0
[36m(TaskRunner pid=3237266)[0m wandb:                               global_seqlen/balanced_max 93361
[36m(TaskRunner pid=3237266)[0m wandb:                               global_seqlen/balanced_min 93360
[36m(TaskRunner pid=3237266)[0m wandb:                                        global_seqlen/max 103950
[36m(TaskRunner pid=3237266)[0m wandb:                                       global_seqlen/mean 93360.75
[36m(TaskRunner pid=3237266)[0m wandb:                                        global_seqlen/min 88648
[36m(TaskRunner pid=3237266)[0m wandb:                                global_seqlen/minmax_diff 15302
[36m(TaskRunner pid=3237266)[0m wandb:                                  perf/cpu_memory_used_gb 44.78375
[36m(TaskRunner pid=3237266)[0m wandb:                             perf/max_memory_allocated_gb 50.41644
[36m(TaskRunner pid=3237266)[0m wandb:                              perf/max_memory_reserved_gb 56.72461
[36m(TaskRunner pid=3237266)[0m wandb:                                           perf/mfu/actor 0.48077
[36m(TaskRunner pid=3237266)[0m wandb:                                          perf/throughput 842.66612
[36m(TaskRunner pid=3237266)[0m wandb:                                       perf/time_per_step 110.7921
[36m(TaskRunner pid=3237266)[0m wandb:                                    perf/total_num_tokens 746886
[36m(TaskRunner pid=3237266)[0m wandb:                                 prompt_length/clip_ratio 0
[36m(TaskRunner pid=3237266)[0m wandb:                                        prompt_length/max 463
[36m(TaskRunner pid=3237266)[0m wandb:                                       prompt_length/mean 296.26562
[36m(TaskRunner pid=3237266)[0m wandb:                                        prompt_length/min 201
[36m(TaskRunner pid=3237266)[0m wandb:                               response_length/clip_ratio 0.20573
[36m(TaskRunner pid=3237266)[0m wandb:                                      response_length/max 1024
[36m(TaskRunner pid=3237266)[0m wandb:                                     response_length/mean 676.24219
[36m(TaskRunner pid=3237266)[0m wandb:                                      response_length/min 167
[36m(TaskRunner pid=3237266)[0m wandb:                                  timing_per_token_ms/adv 0.00057
[36m(TaskRunner pid=3237266)[0m wandb:                                  timing_per_token_ms/gen 0.06802
[36m(TaskRunner pid=3237266)[0m wandb:                                  timing_per_token_ms/ref 0.01948
[36m(TaskRunner pid=3237266)[0m wandb:                         timing_per_token_ms/update_actor 0.06747
[36m(TaskRunner pid=3237266)[0m wandb:                                             timing_s/adv 0.42906
[36m(TaskRunner pid=3237266)[0m wandb:                                             timing_s/gen 35.3247
[36m(TaskRunner pid=3237266)[0m wandb:                                    timing_s/old_log_prob 10.06996
[36m(TaskRunner pid=3237266)[0m wandb:                                             timing_s/ref 14.55222
[36m(TaskRunner pid=3237266)[0m wandb:                                            timing_s/step 110.7921
[36m(TaskRunner pid=3237266)[0m wandb:                                    timing_s/update_actor 50.39169
[36m(TaskRunner pid=3237266)[0m wandb:                  val-aux/jec-qa-1-multi-choice/acc/std@1 0
[36m(TaskRunner pid=3237266)[0m wandb:           val-aux/jec-qa-1-multi-choice/acc/worst@1/mean 0.14955
[36m(TaskRunner pid=3237266)[0m wandb:            val-aux/jec-qa-1-multi-choice/acc/worst@1/std 0
[36m(TaskRunner pid=3237266)[0m wandb:  val-aux/jec-qa-1-multi-choice/answer_reward/best@1/mean 0.21784
[36m(TaskRunner pid=3237266)[0m wandb:   val-aux/jec-qa-1-multi-choice/answer_reward/best@1/std 0.0
[36m(TaskRunner pid=3237266)[0m wandb:       val-aux/jec-qa-1-multi-choice/answer_reward/mean@1 0.21784
[36m(TaskRunner pid=3237266)[0m wandb:        val-aux/jec-qa-1-multi-choice/answer_reward/std@1 0
[36m(TaskRunner pid=3237266)[0m wandb: val-aux/jec-qa-1-multi-choice/answer_reward/worst@1/mean 0.21784
[36m(TaskRunner pid=3237266)[0m wandb:  val-aux/jec-qa-1-multi-choice/answer_reward/worst@1/std 0.0
[36m(TaskRunner pid=3237266)[0m wandb:             val-aux/jec-qa-1-multi-choice/f1/best@1/mean 0.43837
[36m(TaskRunner pid=3237266)[0m wandb:              val-aux/jec-qa-1-multi-choice/f1/best@1/std 0.0
[36m(TaskRunner pid=3237266)[0m wandb:                  val-aux/jec-qa-1-multi-choice/f1/mean@1 0.43837
[36m(TaskRunner pid=3237266)[0m wandb:                   val-aux/jec-qa-1-multi-choice/f1/std@1 0
[36m(TaskRunner pid=3237266)[0m wandb:            val-aux/jec-qa-1-multi-choice/f1/worst@1/mean 0.43837
[36m(TaskRunner pid=3237266)[0m wandb:             val-aux/jec-qa-1-multi-choice/f1/worst@1/std 0.0
[36m(TaskRunner pid=3237266)[0m wandb:  val-aux/jec-qa-1-multi-choice/format_reward/best@1/mean 0.77662
[36m(TaskRunner pid=3237266)[0m wandb:   val-aux/jec-qa-1-multi-choice/format_reward/best@1/std 0
[36m(TaskRunner pid=3237266)[0m wandb:       val-aux/jec-qa-1-multi-choice/format_reward/mean@1 0.77662
[36m(TaskRunner pid=3237266)[0m wandb:        val-aux/jec-qa-1-multi-choice/format_reward/std@1 0
[36m(TaskRunner pid=3237266)[0m wandb: val-aux/jec-qa-1-multi-choice/format_reward/worst@1/mean 0.77662
[36m(TaskRunner pid=3237266)[0m wandb:  val-aux/jec-qa-1-multi-choice/format_reward/worst@1/std 0
[36m(TaskRunner pid=3237266)[0m wandb:      val-aux/jec-qa-1-multi-choice/precision/best@1/mean 0.43512
[36m(TaskRunner pid=3237266)[0m wandb:       val-aux/jec-qa-1-multi-choice/precision/best@1/std 0.0
[36m(TaskRunner pid=3237266)[0m wandb:           val-aux/jec-qa-1-multi-choice/precision/mean@1 0.43512
[36m(TaskRunner pid=3237266)[0m wandb:            val-aux/jec-qa-1-multi-choice/precision/std@1 0
[36m(TaskRunner pid=3237266)[0m wandb:     val-aux/jec-qa-1-multi-choice/precision/worst@1/mean 0.43512
[36m(TaskRunner pid=3237266)[0m wandb:      val-aux/jec-qa-1-multi-choice/precision/worst@1/std 0.0
[36m(TaskRunner pid=3237266)[0m wandb:         val-aux/jec-qa-1-multi-choice/recall/best@1/mean 0.50939
[36m(TaskRunner pid=3237266)[0m wandb:          val-aux/jec-qa-1-multi-choice/recall/best@1/std 0.0
[36m(TaskRunner pid=3237266)[0m wandb:              val-aux/jec-qa-1-multi-choice/recall/mean@1 0.50939
[36m(TaskRunner pid=3237266)[0m wandb:               val-aux/jec-qa-1-multi-choice/recall/std@1 0
[36m(TaskRunner pid=3237266)[0m wandb:        val-aux/jec-qa-1-multi-choice/recall/worst@1/mean 0.50939
[36m(TaskRunner pid=3237266)[0m wandb:         val-aux/jec-qa-1-multi-choice/recall/worst@1/std 0.0
[36m(TaskRunner pid=3237266)[0m wandb:         val-aux/jec-qa-1-multi-choice/reward/best@1/mean 0.99446
[36m(TaskRunner pid=3237266)[0m wandb:          val-aux/jec-qa-1-multi-choice/reward/best@1/std 0
[36m(TaskRunner pid=3237266)[0m wandb:              val-aux/jec-qa-1-multi-choice/reward/mean@1 0.99446
[36m(TaskRunner pid=3237266)[0m wandb:               val-aux/jec-qa-1-multi-choice/reward/std@1 0
[36m(TaskRunner pid=3237266)[0m wandb:        val-aux/jec-qa-1-multi-choice/reward/worst@1/mean 0.99446
[36m(TaskRunner pid=3237266)[0m wandb:         val-aux/jec-qa-1-multi-choice/reward/worst@1/std 0
[36m(TaskRunner pid=3237266)[0m wandb:          val-aux/jec-qa-1-multi-choice/score/best@1/mean 0.99446
[36m(TaskRunner pid=3237266)[0m wandb:           val-aux/jec-qa-1-multi-choice/score/best@1/std 0.0
[36m(TaskRunner pid=3237266)[0m wandb:               val-aux/jec-qa-1-multi-choice/score/mean@1 0.99446
[36m(TaskRunner pid=3237266)[0m wandb:                val-aux/jec-qa-1-multi-choice/score/std@1 0
[36m(TaskRunner pid=3237266)[0m wandb:         val-aux/jec-qa-1-multi-choice/score/worst@1/mean 0.99446
[36m(TaskRunner pid=3237266)[0m wandb:          val-aux/jec-qa-1-multi-choice/score/worst@1/std 0.0
[36m(TaskRunner pid=3237266)[0m wandb:           val-core/jec-qa-1-multi-choice/acc/best@1/mean 0.14955
[36m(TaskRunner pid=3237266)[0m wandb:            val-core/jec-qa-1-multi-choice/acc/best@1/std 0
[36m(TaskRunner pid=3237266)[0m wandb:                val-core/jec-qa-1-multi-choice/acc/mean@1 0.14955
[36m(TaskRunner pid=3237266)[0m wandb: 
[36m(TaskRunner pid=3237266)[0m wandb: 🚀 View run qwen2.5-7b-grpo-mcq at: https://wandb.ai/eang333cms-peking-university/Lawyer-Zero/runs/5ok9zdaj
[36m(TaskRunner pid=3237266)[0m wandb: ⭐️ View project at: https://wandb.ai/eang333cms-peking-university/Lawyer-Zero
[36m(TaskRunner pid=3237266)[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[36m(TaskRunner pid=3237266)[0m wandb: Find logs at: ./wandb/run-20250511_053741-5ok9zdaj/logs
[36m(TaskRunner pid=3237266)[0m Training Progress:   1%|          | 1/132 [02:52<6:16:35, 172.49s/it]
[36m(TaskRunner pid=3237266)[0m step:0 - val-aux/jec-qa-1-multi-choice/reward/mean@1:0.994 - val-aux/jec-qa-1-multi-choice/reward/std@1:0.000 - val-aux/jec-qa-1-multi-choice/reward/best@1/mean:0.994 - val-aux/jec-qa-1-multi-choice/reward/best@1/std:0.000 - val-aux/jec-qa-1-multi-choice/reward/worst@1/mean:0.994 - val-aux/jec-qa-1-multi-choice/reward/worst@1/std:0.000 - val-aux/jec-qa-1-multi-choice/score/mean@1:0.994 - val-aux/jec-qa-1-multi-choice/score/std@1:0.000 - val-aux/jec-qa-1-multi-choice/score/best@1/mean:0.994 - val-aux/jec-qa-1-multi-choice/score/best@1/std:0.000 - val-aux/jec-qa-1-multi-choice/score/worst@1/mean:0.994 - val-aux/jec-qa-1-multi-choice/score/worst@1/std:0.000 - val-aux/jec-qa-1-multi-choice/format_reward/mean@1:0.777 - val-aux/jec-qa-1-multi-choice/format_reward/std@1:0.000 - val-aux/jec-qa-1-multi-choice/format_reward/best@1/mean:0.777 - val-aux/jec-qa-1-multi-choice/format_reward/best@1/std:0.000 - val-aux/jec-qa-1-multi-choice/format_reward/worst@1/mean:0.777 - val-aux/jec-qa-1-multi-choice/format_reward/worst@1/std:0.000 - val-aux/jec-qa-1-multi-choice/answer_reward/mean@1:0.218 - val-aux/jec-qa-1-multi-choice/answer_reward/std@1:0.000 - val-aux/jec-qa-1-multi-choice/answer_reward/best@1/mean:0.218 - val-aux/jec-qa-1-multi-choice/answer_reward/best@1/std:0.000 - val-aux/jec-qa-1-multi-choice/answer_reward/worst@1/mean:0.218 - val-aux/jec-qa-1-multi-choice/answer_reward/worst@1/std:0.000 - val-core/jec-qa-1-multi-choice/acc/mean@1:0.150 - val-aux/jec-qa-1-multi-choice/acc/std@1:0.000 - val-core/jec-qa-1-multi-choice/acc/best@1/mean:0.150 - val-core/jec-qa-1-multi-choice/acc/best@1/std:0.000 - val-aux/jec-qa-1-multi-choice/acc/worst@1/mean:0.150 - val-aux/jec-qa-1-multi-choice/acc/worst@1/std:0.000 - val-aux/jec-qa-1-multi-choice/precision/mean@1:0.435 - val-aux/jec-qa-1-multi-choice/precision/std@1:0.000 - val-aux/jec-qa-1-multi-choice/precision/best@1/mean:0.435 - val-aux/jec-qa-1-multi-choice/precision/best@1/std:0.000 - val-aux/jec-qa-1-multi-choice/precision/worst@1/mean:0.435 - val-aux/jec-qa-1-multi-choice/precision/worst@1/std:0.000 - val-aux/jec-qa-1-multi-choice/recall/mean@1:0.509 - val-aux/jec-qa-1-multi-choice/recall/std@1:0.000 - val-aux/jec-qa-1-multi-choice/recall/best@1/mean:0.509 - val-aux/jec-qa-1-multi-choice/recall/best@1/std:0.000 - val-aux/jec-qa-1-multi-choice/recall/worst@1/mean:0.509 - val-aux/jec-qa-1-multi-choice/recall/worst@1/std:0.000 - val-aux/jec-qa-1-multi-choice/f1/mean@1:0.438 - val-aux/jec-qa-1-multi-choice/f1/std@1:0.000 - val-aux/jec-qa-1-multi-choice/f1/best@1/mean:0.438 - val-aux/jec-qa-1-multi-choice/f1/best@1/std:0.000 - val-aux/jec-qa-1-multi-choice/f1/worst@1/mean:0.438 - val-aux/jec-qa-1-multi-choice/f1/worst@1/std:0.000
[36m(TaskRunner pid=3237266)[0m list(reward_extra_infos_dict.keys())=['score', 'format_reward', 'answer_reward', 'acc', 'precision', 'recall', 'f1']
[36m(TaskRunner pid=3237266)[0m step:1 - global_seqlen/min:88648.000 - global_seqlen/max:103950.000 - global_seqlen/minmax_diff:15302.000 - global_seqlen/balanced_min:93360.000 - global_seqlen/balanced_max:93361.000 - global_seqlen/mean:93360.750 - actor/entropy_loss:538.670 - actor/kl_loss:0.007 - actor/kl_coef:0.001 - actor/pg_loss:1.048 - actor/pg_clipfrac:0.000 - actor/ppo_kl:0.000 - actor/pg_clipfrac_lower:0.000 - actor/grad_norm:2.381 - perf/mfu/actor:0.481 - perf/max_memory_allocated_gb:50.416 - perf/max_memory_reserved_gb:56.725 - perf/cpu_memory_used_gb:44.784 - actor/lr:0.000 - critic/score/mean:0.630 - critic/score/max:2.000 - critic/score/min:0.000 - critic/rewards/mean:0.630 - critic/rewards/max:2.000 - critic/rewards/min:0.000 - critic/advantages/mean:-0.046 - critic/advantages/max:1.517 - critic/advantages/min:-1.333 - critic/returns/mean:-0.046 - critic/returns/max:1.517 - critic/returns/min:-1.333 - response_length/mean:676.242 - response_length/max:1024.000 - response_length/min:167.000 - response_length/clip_ratio:0.206 - prompt_length/mean:296.266 - prompt_length/max:463.000 - prompt_length/min:201.000 - prompt_length/clip_ratio:0.000 - timing_s/gen:35.325 - timing_s/old_log_prob:10.070 - timing_s/ref:14.552 - timing_s/adv:0.429 - timing_s/update_actor:50.392 - timing_s/step:110.792 - timing_per_token_ms/gen:0.068 - timing_per_token_ms/update_actor:0.067 - timing_per_token_ms/adv:0.001 - timing_per_token_ms/ref:0.019 - perf/total_num_tokens:746886.000 - perf/time_per_step:110.792 - perf/throughput:842.666
[36m(TaskRunner pid=3237266)[0m list(reward_extra_infos_dict.keys())=['score', 'format_reward', 'answer_reward', 'acc', 'precision', 'recall', 'f1']
Error executing job with overrides: ['algorithm.adv_estimator=grpo', 'data.train_files=/home/wangyc/verl/data/jec-qa-1-multi-choice/train.parquet', 'data.val_files=/home/wangyc/verl/data/jec-qa-1-multi-choice/test.parquet', 'data.train_batch_size=128', 'data.val_batch_size=1312', 'data.max_prompt_length=1024', 'data.max_response_length=1024', 'actor_rollout_ref.model.path=/home/wangyc/.cache/modelscope/hub/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', 'actor_rollout_ref.actor.optim.lr=1e-6', 'actor_rollout_ref.model.use_remove_padding=True', 'actor_rollout_ref.actor.ppo_mini_batch_size=64', 'actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=16', 'actor_rollout_ref.actor.loss_agg_mode=seq-mean-token-sum-norm', 'actor_rollout_ref.actor.use_kl_loss=True', 'actor_rollout_ref.actor.kl_loss_coef=0.001', 'actor_rollout_ref.actor.kl_loss_type=low_var_kl', 'actor_rollout_ref.actor.entropy_coeff=0', 'actor_rollout_ref.model.enable_gradient_checkpointing=True', 'actor_rollout_ref.actor.fsdp_config.param_offload=False', 'actor_rollout_ref.actor.fsdp_config.optimizer_offload=False', 'actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=32', 'actor_rollout_ref.rollout.tensor_model_parallel_size=2', 'actor_rollout_ref.rollout.name=vllm', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.6', 'actor_rollout_ref.rollout.n=6', 'actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=32', 'actor_rollout_ref.ref.fsdp_config.param_offload=True', 'algorithm.use_kl_in_reward=False', 'algorithm.norm_adv_by_std_in_grpo=False', 'trainer.critic_warmup=0', "trainer.logger=['console','wandb']", 'trainer.project_name=Lawyer-Zero', 'trainer.experiment_name=qwen2.5-7b-grpo-mcq', 'trainer.n_gpus_per_node=8', 'trainer.nnodes=1', 'trainer.default_local_dir=checkpoints/qwen2.5-7b-grpo-hard-mcq', 'trainer.save_freq=50', 'trainer.test_freq=10', 'trainer.total_epochs=2']
Traceback (most recent call last):
  File "/home/wangyc/verl/verl/trainer/main_ppo.py", line 63, in main
    run_ppo(config)
  File "/home/wangyc/verl/verl/trainer/main_ppo.py", line 80, in run_ppo
    ray.get(runner.run.remote(config))
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/ray/_private/worker.py", line 2771, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/ray/_private/worker.py", line 919, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::TaskRunner.run()[39m (pid=3237266, ip=222.29.51.203, actor_id=c5bb4bfc502d3dab8a0923a301000000, repr=<main_ppo.TaskRunner object at 0x7f42f4eca980>)
  File "/home/wangyc/verl/verl/trainer/main_ppo.py", line 208, in run
    trainer.fit()
  File "/home/wangyc/verl/verl/trainer/ppo/ray_trainer.py", line 1048, in fit
    actor_output = self.actor_rollout_wg.update_actor(batch)
  File "/home/wangyc/verl/verl/single_controller/ray/base.py", line 43, in func
    output = ray.get(output)
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::WorkerDict.actor_rollout_update_actor()[39m (pid=3246154, ip=222.29.51.203, actor_id=55e64524f05c24745bad340701000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7fc3e4cc0850>)
  File "/home/wangyc/verl/verl/single_controller/ray/base.py", line 440, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
  File "/home/wangyc/verl/verl/single_controller/base/decorator.py", line 413, in inner
    return func(*args, **kwargs)
  File "/home/wangyc/verl/verl/workers/fsdp_workers.py", line 514, in update_actor
    metrics = self.actor.update_policy(data=data)
  File "/home/wangyc/verl/verl/utils/debug/performance.py", line 61, in f
    return self.log(decorated_function, *args, **kwargs)
  File "/home/wangyc/verl/verl/utils/debug/performance.py", line 70, in log
    output = func(*args, **kwargs)
  File "/home/wangyc/verl/verl/workers/actor/dp_actor.py", line 381, in update_policy
    loss.backward()
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.88 GiB. GPU 0 has a total capacity of 44.53 GiB of which 1.36 GiB is free. Including non-PyTorch memory, this process has 43.12 GiB memory in use. Of the allocated memory 44.18 GiB is allocated by PyTorch, with 94.41 MiB allocated in private pools (e.g., CUDA Graphs), and 11.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
