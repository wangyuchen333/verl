2025-05-01 05:52:54,131	INFO worker.py:1832 -- Started a local Ray instance. View the dashboard at [1m[32m127.0.0.1:8265 [39m[22m
[36m(TaskRunner pid=3733940)[0m {'actor_rollout_ref': {'actor': {'checkpoint': {'contents': ['model',
[36m(TaskRunner pid=3733940)[0m                                                              'optimizer',
[36m(TaskRunner pid=3733940)[0m                                                              'extra']},
[36m(TaskRunner pid=3733940)[0m                                  'clip_ratio': 0.2,
[36m(TaskRunner pid=3733940)[0m                                  'clip_ratio_c': 3.0,
[36m(TaskRunner pid=3733940)[0m                                  'clip_ratio_high': 0.2,
[36m(TaskRunner pid=3733940)[0m                                  'clip_ratio_low': 0.2,
[36m(TaskRunner pid=3733940)[0m                                  'entropy_coeff': 0,
[36m(TaskRunner pid=3733940)[0m                                  'fsdp_config': {'fsdp_size': -1,
[36m(TaskRunner pid=3733940)[0m                                                  'optimizer_offload': False,
[36m(TaskRunner pid=3733940)[0m                                                  'param_offload': False,
[36m(TaskRunner pid=3733940)[0m                                                  'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=3733940)[0m                                  'grad_clip': 1.0,
[36m(TaskRunner pid=3733940)[0m                                  'kl_loss_coef': 0.001,
[36m(TaskRunner pid=3733940)[0m                                  'kl_loss_type': 'low_var_kl',
[36m(TaskRunner pid=3733940)[0m                                  'loss_agg_mode': 'seq-mean-token-sum-norm',
[36m(TaskRunner pid=3733940)[0m                                  'optim': {'lr': 1e-06,
[36m(TaskRunner pid=3733940)[0m                                            'lr_warmup_steps': -1,
[36m(TaskRunner pid=3733940)[0m                                            'lr_warmup_steps_ratio': 0.0,
[36m(TaskRunner pid=3733940)[0m                                            'min_lr_ratio': None,
[36m(TaskRunner pid=3733940)[0m                                            'total_training_steps': -1,
[36m(TaskRunner pid=3733940)[0m                                            'warmup_style': 'constant',
[36m(TaskRunner pid=3733940)[0m                                            'weight_decay': 0.01},
[36m(TaskRunner pid=3733940)[0m                                  'ppo_epochs': 1,
[36m(TaskRunner pid=3733940)[0m                                  'ppo_max_token_len_per_gpu': 16384,
[36m(TaskRunner pid=3733940)[0m                                  'ppo_micro_batch_size': None,
[36m(TaskRunner pid=3733940)[0m                                  'ppo_micro_batch_size_per_gpu': 16,
[36m(TaskRunner pid=3733940)[0m                                  'ppo_mini_batch_size': 64,
[36m(TaskRunner pid=3733940)[0m                                  'shuffle': False,
[36m(TaskRunner pid=3733940)[0m                                  'strategy': 'fsdp',
[36m(TaskRunner pid=3733940)[0m                                  'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=3733940)[0m                                  'use_dynamic_bsz': False,
[36m(TaskRunner pid=3733940)[0m                                  'use_kl_loss': False,
[36m(TaskRunner pid=3733940)[0m                                  'use_torch_compile': True},
[36m(TaskRunner pid=3733940)[0m                        'hybrid_engine': True,
[36m(TaskRunner pid=3733940)[0m                        'model': {'enable_gradient_checkpointing': True,
[36m(TaskRunner pid=3733940)[0m                                  'external_lib': None,
[36m(TaskRunner pid=3733940)[0m                                  'override_config': {},
[36m(TaskRunner pid=3733940)[0m                                  'path': '/home/wangyc/verl/Qwen/Qwen2.5-7B-Instruct',
[36m(TaskRunner pid=3733940)[0m                                  'use_liger': False,
[36m(TaskRunner pid=3733940)[0m                                  'use_remove_padding': True},
[36m(TaskRunner pid=3733940)[0m                        'ref': {'fsdp_config': {'param_offload': True,
[36m(TaskRunner pid=3733940)[0m                                                'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=3733940)[0m                                'log_prob_max_token_len_per_gpu': 16384,
[36m(TaskRunner pid=3733940)[0m                                'log_prob_micro_batch_size': None,
[36m(TaskRunner pid=3733940)[0m                                'log_prob_micro_batch_size_per_gpu': 32,
[36m(TaskRunner pid=3733940)[0m                                'log_prob_use_dynamic_bsz': False,
[36m(TaskRunner pid=3733940)[0m                                'strategy': 'fsdp',
[36m(TaskRunner pid=3733940)[0m                                'ulysses_sequence_parallel_size': 1},
[36m(TaskRunner pid=3733940)[0m                        'rollout': {'disable_log_stats': True,
[36m(TaskRunner pid=3733940)[0m                                    'do_sample': True,
[36m(TaskRunner pid=3733940)[0m                                    'dtype': 'bfloat16',
[36m(TaskRunner pid=3733940)[0m                                    'enable_chunked_prefill': True,
[36m(TaskRunner pid=3733940)[0m                                    'enforce_eager': True,
[36m(TaskRunner pid=3733940)[0m                                    'engine_kwargs': {'swap_space': None},
[36m(TaskRunner pid=3733940)[0m                                    'free_cache_engine': True,
[36m(TaskRunner pid=3733940)[0m                                    'gpu_memory_utilization': 0.6,
[36m(TaskRunner pid=3733940)[0m                                    'ignore_eos': False,
[36m(TaskRunner pid=3733940)[0m                                    'load_format': 'dummy_dtensor',
[36m(TaskRunner pid=3733940)[0m                                    'log_prob_max_token_len_per_gpu': 16384,
[36m(TaskRunner pid=3733940)[0m                                    'log_prob_micro_batch_size': None,
[36m(TaskRunner pid=3733940)[0m                                    'log_prob_micro_batch_size_per_gpu': 32,
[36m(TaskRunner pid=3733940)[0m                                    'log_prob_use_dynamic_bsz': False,
[36m(TaskRunner pid=3733940)[0m                                    'max_model_len': None,
[36m(TaskRunner pid=3733940)[0m                                    'max_num_batched_tokens': 8192,
[36m(TaskRunner pid=3733940)[0m                                    'max_num_seqs': 1024,
[36m(TaskRunner pid=3733940)[0m                                    'n': 6,
[36m(TaskRunner pid=3733940)[0m                                    'name': 'vllm',
[36m(TaskRunner pid=3733940)[0m                                    'prompt_length': 1024,
[36m(TaskRunner pid=3733940)[0m                                    'response_length': 1024,
[36m(TaskRunner pid=3733940)[0m                                    'temperature': 1.0,
[36m(TaskRunner pid=3733940)[0m                                    'tensor_model_parallel_size': 2,
[36m(TaskRunner pid=3733940)[0m                                    'top_k': -1,
[36m(TaskRunner pid=3733940)[0m                                    'top_p': 1,
[36m(TaskRunner pid=3733940)[0m                                    'use_fire_sampling': False,
[36m(TaskRunner pid=3733940)[0m                                    'val_kwargs': {'do_sample': False,
[36m(TaskRunner pid=3733940)[0m                                                   'n': 1,
[36m(TaskRunner pid=3733940)[0m                                                   'temperature': 0,
[36m(TaskRunner pid=3733940)[0m                                                   'top_k': -1,
[36m(TaskRunner pid=3733940)[0m                                                   'top_p': 1.0}}},
[36m(TaskRunner pid=3733940)[0m  'algorithm': {'adv_estimator': 'grpo',
[36m(TaskRunner pid=3733940)[0m                'gamma': 1.0,
[36m(TaskRunner pid=3733940)[0m                'kl_ctrl': {'horizon': 10000,
[36m(TaskRunner pid=3733940)[0m                            'kl_coef': 0,
[36m(TaskRunner pid=3733940)[0m                            'target_kl': 0.1,
[36m(TaskRunner pid=3733940)[0m DeprecationWarning: `ray.state.available_resources_per_node` is a private attribute and access will be removed in a future Ray version.
[36m(TaskRunner pid=3733940)[0m WARNING:2025-05-01 05:53:04,815:Waiting for register center actor MkiGIA_register_center to be ready. Elapsed time: 0 seconds out of 300 seconds.
[36m(WorkerDict pid=3743639)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=3743639)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=3743648)[0m Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[36m(TaskRunner pid=3733940)[0m                            'type': 'fixed'},
[36m(TaskRunner pid=3733940)[0m                'kl_penalty': 'kl',
[36m(TaskRunner pid=3733940)[0m                'lam': 1.0,
[36m(TaskRunner pid=3733940)[0m                'norm_adv_by_std_in_grpo': False,
[36m(TaskRunner pid=3733940)[0m                'use_kl_in_reward': False},
[36m(TaskRunner pid=3733940)[0m  'critic': {'checkpoint': {'contents': ['model', 'optimizer', 'extra']},
[36m(TaskRunner pid=3733940)[0m             'cliprange_value': 0.5,
[36m(TaskRunner pid=3733940)[0m             'forward_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=3733940)[0m             'forward_micro_batch_size': None,
[36m(TaskRunner pid=3733940)[0m             'forward_micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=3733940)[0m             'grad_clip': 1.0,
[36m(TaskRunner pid=3733940)[0m             'model': {'enable_gradient_checkpointing': True,
[36m(TaskRunner pid=3733940)[0m                       'external_lib': None,
[36m(TaskRunner pid=3733940)[0m                       'fsdp_config': {'fsdp_size': -1,
[36m(TaskRunner pid=3733940)[0m                                       'optimizer_offload': False,
[36m(TaskRunner pid=3733940)[0m                                       'param_offload': False,
[36m(TaskRunner pid=3733940)[0m                                       'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=3733940)[0m                       'override_config': {},
[36m(TaskRunner pid=3733940)[0m                       'path': '~/models/deepseek-llm-7b-chat',
[36m(TaskRunner pid=3733940)[0m                       'tokenizer_path': '/home/wangyc/verl/Qwen/Qwen2.5-7B-Instruct',
[36m(TaskRunner pid=3733940)[0m                       'use_remove_padding': False},
[36m(TaskRunner pid=3733940)[0m             'optim': {'lr': 1e-05,
[36m(TaskRunner pid=3733940)[0m                       'lr_warmup_steps_ratio': 0.0,
[36m(TaskRunner pid=3733940)[0m                       'min_lr_ratio': None,
[36m(TaskRunner pid=3733940)[0m                       'total_training_steps': -1,
[36m(TaskRunner pid=3733940)[0m                       'warmup_style': 'constant',
[36m(TaskRunner pid=3733940)[0m                       'weight_decay': 0.01},
[36m(TaskRunner pid=3733940)[0m             'ppo_epochs': 1,
[36m(TaskRunner pid=3733940)[0m             'ppo_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=3733940)[0m             'ppo_micro_batch_size': None,
[36m(TaskRunner pid=3733940)[0m             'ppo_micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=3733940)[0m             'ppo_mini_batch_size': 64,
[36m(TaskRunner pid=3733940)[0m             'rollout_n': 6,
[36m(TaskRunner pid=3733940)[0m             'shuffle': False,
[36m(TaskRunner pid=3733940)[0m             'strategy': 'fsdp',
[36m(TaskRunner pid=3733940)[0m             'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=3733940)[0m             'use_dynamic_bsz': False},
[36m(TaskRunner pid=3733940)[0m  'custom_reward_function': {'name': 'compute_score', 'path': None},
[36m(TaskRunner pid=3733940)[0m  'data': {'custom_cls': {'name': None, 'path': None},
[36m(TaskRunner pid=3733940)[0m           'filter_overlong_prompts': False,
[36m(TaskRunner pid=3733940)[0m           'filter_overlong_prompts_workers': 1,
[36m(TaskRunner pid=3733940)[0m           'image_key': 'images',
[36m(TaskRunner pid=3733940)[0m           'max_prompt_length': 1024,
[36m(TaskRunner pid=3733940)[0m           'max_response_length': 1024,
[36m(TaskRunner pid=3733940)[0m           'prompt_key': 'prompt',
[36m(TaskRunner pid=3733940)[0m           'return_raw_chat': False,
[36m(TaskRunner pid=3733940)[0m           'return_raw_input_ids': False,
[36m(TaskRunner pid=3733940)[0m           'reward_fn_key': 'data_source',
[36m(TaskRunner pid=3733940)[0m           'shuffle': True,
[36m(TaskRunner pid=3733940)[0m           'tokenizer': None,
[36m(TaskRunner pid=3733940)[0m           'train_batch_size': 128,
[36m(TaskRunner pid=3733940)[0m           'train_files': '/home/wangyc/verl/data/jec-qa-1-multi-choice/train.parquet',
[36m(TaskRunner pid=3733940)[0m           'truncation': 'error',
[36m(TaskRunner pid=3733940)[0m           'val_batch_size': 1312,
[36m(TaskRunner pid=3733940)[0m           'val_files': '/home/wangyc/verl/data/jec-qa-1-multi-choice/test.parquet',
[36m(TaskRunner pid=3733940)[0m           'video_key': 'videos'},
[36m(TaskRunner pid=3733940)[0m  'ray_init': {'num_cpus': None},
[36m(TaskRunner pid=3733940)[0m  'reward_model': {'enable': False,
[36m(TaskRunner pid=3733940)[0m                   'forward_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=3733940)[0m                   'max_length': None,
[36m(TaskRunner pid=3733940)[0m                   'micro_batch_size': None,
[36m(TaskRunner pid=3733940)[0m                   'micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=3733940)[0m                   'model': {'external_lib': None,
[36m(TaskRunner pid=3733940)[0m                             'fsdp_config': {'fsdp_size': -1,
[36m(TaskRunner pid=3733940)[0m                                             'param_offload': False,
[36m(TaskRunner pid=3733940)[0m                                             'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=3733940)[0m                             'input_tokenizer': '/home/wangyc/verl/Qwen/Qwen2.5-7B-Instruct',
[36m(TaskRunner pid=3733940)[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',
[36m(TaskRunner pid=3733940)[0m                             'use_remove_padding': False},
[36m(TaskRunner pid=3733940)[0m                   'reward_manager': 'naive',
[36m(TaskRunner pid=3733940)[0m                   'strategy': 'fsdp',
[36m(TaskRunner pid=3733940)[0m                   'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=3733940)[0m                   'use_dynamic_bsz': False},
[36m(TaskRunner pid=3733940)[0m  'trainer': {'balance_batch': True,
[36m(TaskRunner pid=3733940)[0m              'critic_warmup': 0,
[36m(TaskRunner pid=3733940)[0m              'default_hdfs_dir': None,
[36m(TaskRunner pid=3733940)[0m              'default_local_dir': 'checkpoints/qwen2.5-7b-grpo-hard-mcq',
[36m(TaskRunner pid=3733940)[0m              'del_local_ckpt_after_load': False,
[36m(TaskRunner pid=3733940)[0m              'experiment_name': 'qwen2.5-7b-grpo-mcq',
[36m(TaskRunner pid=3733940)[0m              'log_val_generations': 0,
[36m(TaskRunner pid=3733940)[0m              'logger': ['console'],
[36m(TaskRunner pid=3733940)[0m              'max_actor_ckpt_to_keep': None,
[36m(TaskRunner pid=3733940)[0m              'max_critic_ckpt_to_keep': None,
[36m(TaskRunner pid=3733940)[0m              'n_gpus_per_node': 8,
[36m(TaskRunner pid=3733940)[0m              'nnodes': 1,
[36m(TaskRunner pid=3733940)[0m              'project_name': 'Lawyer-Zero',
[36m(TaskRunner pid=3733940)[0m              'ray_wait_register_center_timeout': 300,
[36m(TaskRunner pid=3733940)[0m              'resume_from_path': None,
[36m(TaskRunner pid=3733940)[0m              'resume_mode': 'auto',
[36m(TaskRunner pid=3733940)[0m              'save_freq': 50,
[36m(TaskRunner pid=3733940)[0m              'test_freq': 10,
[36m(TaskRunner pid=3733940)[0m              'total_epochs': 2,
[36m(TaskRunner pid=3733940)[0m              'total_training_steps': None,
[36m(TaskRunner pid=3733940)[0m              'val_before_train': True}}
[36m(TaskRunner pid=3733940)[0m WARNING: val_batch_size is deprecated. Validation datasets are sent to inference engines as a whole batch, which will schedule the memory themselves.
[36m(TaskRunner pid=3733940)[0m [validate_config] All configuration checks passed successfully!
[36m(TaskRunner pid=3733940)[0m dataset len: 8448
[36m(TaskRunner pid=3733940)[0m dataset len: 2113
[36m(TaskRunner pid=3733940)[0m Size of train dataloader: 66
[36m(TaskRunner pid=3733940)[0m Total training steps: 132
[36m(WorkerDict pid=3737531)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=3743667)[0m Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:03<00:10,  3.43s/it]
[36m(WorkerDict pid=3743655)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(WorkerDict pid=3743655)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3743655)[0m Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3743667)[0m Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:10<00:03,  3.43s/it][32m [repeated 16x across cluster][0m
[36m(WorkerDict pid=3743667)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:13<00:00,  3.30s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:13<00:00,  3.35s/it]
[36m(WorkerDict pid=3743667)[0m [rank7]:[W501 05:53:33.149675788 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 7]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[36m(WorkerDict pid=3743655)[0m Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:11<00:03,  3.88s/it][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3737531)[0m   "architectures": [
[36m(WorkerDict pid=3737531)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=3737531)[0m   ],
[36m(WorkerDict pid=3737531)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=3737531)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=3737531)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=3737531)[0m   "hidden_size": 3584,
[36m(WorkerDict pid=3737531)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=3737531)[0m   "intermediate_size": 18944,
[36m(WorkerDict pid=3737531)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=3737531)[0m   "max_window_layers": 28,
[36m(WorkerDict pid=3737531)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=3737531)[0m   "num_attention_heads": 28,
[36m(WorkerDict pid=3737531)[0m   "num_hidden_layers": 28,
[36m(WorkerDict pid=3737531)[0m   "num_key_value_heads": 4,
[36m(WorkerDict pid=3737531)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=3737531)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=3737531)[0m   "rope_scaling": null,
[36m(WorkerDict pid=3737531)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=3737531)[0m   "sliding_window": 131072,
[36m(WorkerDict pid=3737531)[0m   "tie_word_embeddings": false,
[36m(WorkerDict pid=3737531)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=3737531)[0m   "transformers_version": "4.51.3",
[36m(WorkerDict pid=3737531)[0m   "use_cache": true,
[36m(WorkerDict pid=3737531)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=3737531)[0m   "vocab_size": 152064
[36m(WorkerDict pid=3737531)[0m }
[36m(WorkerDict pid=3737531)[0m 
[36m(WorkerDict pid=3743667)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
[36m(WorkerDict pid=3737531)[0m NCCL version 2.21.5+cuda12.4
[36m(WorkerDict pid=3737531)[0m Qwen2ForCausalLM contains 7.62B parameters
[36m(WorkerDict pid=3737531)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f5fdbb228c0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f5fdbb227a0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
Error executing job with overrides: ['algorithm.adv_estimator=grpo', 'data.train_files=/home/wangyc/verl/data/jec-qa-1-multi-choice/train.parquet', 'data.val_files=/home/wangyc/verl/data/jec-qa-1-multi-choice/test.parquet', 'data.train_batch_size=128', 'data.val_batch_size=1312', 'data.max_prompt_length=1024', 'data.max_response_length=1024', 'actor_rollout_ref.model.path=/home/wangyc/verl/Qwen/Qwen2.5-7B-Instruct', 'actor_rollout_ref.actor.optim.lr=1e-6', 'actor_rollout_ref.model.use_remove_padding=True', 'actor_rollout_ref.actor.ppo_mini_batch_size=64', 'actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=16', 'actor_rollout_ref.actor.loss_agg_mode=seq-mean-token-sum-norm', 'actor_rollout_ref.actor.use_kl_loss=False', 'actor_rollout_ref.model.enable_gradient_checkpointing=True', 'actor_rollout_ref.actor.fsdp_config.param_offload=False', 'actor_rollout_ref.actor.fsdp_config.optimizer_offload=False', 'actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=32', 'actor_rollout_ref.rollout.tensor_model_parallel_size=2', 'actor_rollout_ref.rollout.name=vllm', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.6', 'actor_rollout_ref.rollout.n=6', 'actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=32', 'actor_rollout_ref.ref.fsdp_config.param_offload=True', 'algorithm.kl_ctrl.kl_coef=0', 'algorithm.norm_adv_by_std_in_grpo=False', 'trainer.critic_warmup=0', "trainer.logger=['console']", 'trainer.project_name=Lawyer-Zero', 'trainer.experiment_name=qwen2.5-7b-grpo-mcq', 'trainer.n_gpus_per_node=8', 'trainer.nnodes=1', 'trainer.default_local_dir=checkpoints/qwen2.5-7b-grpo-hard-mcq', 'trainer.save_freq=50', 'trainer.test_freq=10', 'trainer.total_epochs=2']
Traceback (most recent call last):
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/wangyc/verl/verl/trainer/main_ppo.py", line 212, in <module>
    main()
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/wangyc/verl/verl/trainer/main_ppo.py", line 63, in main
    run_ppo(config)
  File "/home/wangyc/verl/verl/trainer/main_ppo.py", line 80, in run_ppo
    ray.get(runner.run.remote(config))
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/ray/_private/worker.py", line 2771, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/ray/_private/worker.py", line 919, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::TaskRunner.run()[39m (pid=3733940, ip=222.29.51.203, actor_id=519aa57665f4975f920f952d01000000, repr=<main_ppo.TaskRunner object at 0x7f93369070d0>)
  File "/home/wangyc/verl/verl/trainer/main_ppo.py", line 207, in run
    trainer.init_workers()
  File "/home/wangyc/verl/verl/trainer/ppo/ray_trainer.py", line 734, in init_workers
    self.actor_rollout_wg.init_model()
  File "/home/wangyc/verl/verl/single_controller/ray/base.py", line 43, in func
    output = ray.get(output)
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::WorkerDict.actor_rollout_init_model()[39m (pid=3743658, ip=222.29.51.203, actor_id=e25d3ae8d0ffba8b4ff9ba2401000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7ed00092b100>)
  File "/home/wangyc/verl/verl/single_controller/ray/base.py", line 440, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
  File "/home/wangyc/verl/verl/single_controller/base/decorator.py", line 413, in inner
    return func(*args, **kwargs)
  File "/home/wangyc/verl/verl/workers/fsdp_workers.py", line 440, in init_model
    self._build_model_optimizer(
  File "/home/wangyc/verl/verl/workers/fsdp_workers.py", line 276, in _build_model_optimizer
    actor_module_fsdp = FSDP(
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 483, in __init__
    _auto_wrap(
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_wrap_utils.py", line 101, in _auto_wrap
    _recursive_wrap(**recursive_wrap_kwargs, **root_kwargs)  # type: ignore[arg-type]
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/wrap.py", line 545, in _recursive_wrap
    wrapped_child, num_wrapped_params = _recursive_wrap(
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/wrap.py", line 545, in _recursive_wrap
    wrapped_child, num_wrapped_params = _recursive_wrap(
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/wrap.py", line 545, in _recursive_wrap
    wrapped_child, num_wrapped_params = _recursive_wrap(
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/wrap.py", line 563, in _recursive_wrap
    return _wrap(module, wrapper_cls, **kwargs), nonwrapped_numel
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/wrap.py", line 492, in _wrap
    return wrapper_cls(module, **kwargs)
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 509, in __init__
    _init_param_handle_from_module(
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 629, in _init_param_handle_from_module
    _sync_module_params_and_buffers(
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 1126, in _sync_module_params_and_buffers
    _sync_params_and_buffers(
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/utils.py", line 334, in _sync_params_and_buffers
    dist._broadcast_coalesced(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 372.00 MiB. GPU 0 has a total capacity of 44.53 GiB of which 285.50 MiB is free. Process 3238387 has 42.70 GiB memory in use. Including non-PyTorch memory, this process has 1.54 GiB memory in use. Of the allocated memory 894.05 MiB is allocated by PyTorch, and 7.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(TaskRunner pid=3733940)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_init_model()[39m (pid=3743667, ip=222.29.51.203, actor_id=02422ce97bc01a27bebaeddc01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7ee9727fbbe0>)
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/verl/verl/single_controller/ray/base.py", line 440, in func
[36m(TaskRunner pid=3733940)[0m     return getattr(self.worker_dict[key], name)(*args, **kwargs)
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/verl/verl/single_controller/base/decorator.py", line 413, in inner
[36m(TaskRunner pid=3733940)[0m     return func(*args, **kwargs)
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/verl/verl/workers/fsdp_workers.py", line 440, in init_model
[36m(TaskRunner pid=3733940)[0m     self._build_model_optimizer(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/verl/verl/workers/fsdp_workers.py", line 276, in _build_model_optimizer
[36m(TaskRunner pid=3733940)[0m     actor_module_fsdp = FSDP(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 483, in __init__
[36m(TaskRunner pid=3733940)[0m     _auto_wrap(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_wrap_utils.py", line 101, in _auto_wrap
[36m(TaskRunner pid=3733940)[0m     _recursive_wrap(**recursive_wrap_kwargs, **root_kwargs)  # type: ignore[arg-type]
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/wrap.py", line 545, in _recursive_wrap
[36m(TaskRunner pid=3733940)[0m     wrapped_child, num_wrapped_params = _recursive_wrap(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/wrap.py", line 545, in _recursive_wrap
[36m(TaskRunner pid=3733940)[0m     wrapped_child, num_wrapped_params = _recursive_wrap(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/wrap.py", line 545, in _recursive_wrap
[36m(TaskRunner pid=3733940)[0m     wrapped_child, num_wrapped_params = _recursive_wrap(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/wrap.py", line 563, in _recursive_wrap
[36m(TaskRunner pid=3733940)[0m     return _wrap(module, wrapper_cls, **kwargs), nonwrapped_numel
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/wrap.py", line 492, in _wrap
[36m(TaskRunner pid=3733940)[0m     return wrapper_cls(module, **kwargs)
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 509, in __init__
[36m(TaskRunner pid=3733940)[0m     _init_param_handle_from_module(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 629, in _init_param_handle_from_module
[36m(TaskRunner pid=3733940)[0m     _sync_module_params_and_buffers(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 1126, in _sync_module_params_and_buffers
[36m(TaskRunner pid=3733940)[0m     _sync_params_and_buffers(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/utils.py", line 334, in _sync_params_and_buffers
[36m(TaskRunner pid=3733940)[0m     dist._broadcast_coalesced(
[36m(TaskRunner pid=3733940)[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 372.00 MiB. GPU 0 has a total capacity of 44.53 GiB of which 333.50 MiB is free. Process 3238622 has 42.67 GiB memory in use. Including non-PyTorch memory, this process has 1.52 GiB memory in use. Of the allocated memory 894.05 MiB is allocated by PyTorch, and 7.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(TaskRunner pid=3733940)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_init_model()[39m (pid=3743663, ip=222.29.51.203, actor_id=eea5e766664c87f4cac21f1401000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7eec5c832f50>)
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/verl/verl/single_controller/ray/base.py", line 440, in func
[36m(TaskRunner pid=3733940)[0m     return getattr(self.worker_dict[key], name)(*args, **kwargs)
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/verl/verl/single_controller/base/decorator.py", line 413, in inner
[36m(TaskRunner pid=3733940)[0m     return func(*args, **kwargs)
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/verl/verl/workers/fsdp_workers.py", line 440, in init_model
[36m(TaskRunner pid=3733940)[0m     self._build_model_optimizer(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/verl/verl/workers/fsdp_workers.py", line 276, in _build_model_optimizer
[36m(TaskRunner pid=3733940)[0m     actor_module_fsdp = FSDP(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 483, in __init__
[36m(TaskRunner pid=3733940)[0m     _auto_wrap(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_wrap_utils.py", line 101, in _auto_wrap
[36m(TaskRunner pid=3733940)[0m     _recursive_wrap(**recursive_wrap_kwargs, **root_kwargs)  # type: ignore[arg-type]
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/wrap.py", line 545, in _recursive_wrap
[36m(TaskRunner pid=3733940)[0m     wrapped_child, num_wrapped_params = _recursive_wrap(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/wrap.py", line 545, in _recursive_wrap
[36m(TaskRunner pid=3733940)[0m     wrapped_child, num_wrapped_params = _recursive_wrap(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/wrap.py", line 545, in _recursive_wrap
[36m(TaskRunner pid=3733940)[0m     wrapped_child, num_wrapped_params = _recursive_wrap(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/wrap.py", line 563, in _recursive_wrap
[36m(TaskRunner pid=3733940)[0m     return _wrap(module, wrapper_cls, **kwargs), nonwrapped_numel
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/wrap.py", line 492, in _wrap
[36m(TaskRunner pid=3733940)[0m     return wrapper_cls(module, **kwargs)
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 509, in __init__
[36m(TaskRunner pid=3733940)[0m     _init_param_handle_from_module(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 629, in _init_param_handle_from_module
[36m(TaskRunner pid=3733940)[0m     _sync_module_params_and_buffers(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 1126, in _sync_module_params_and_buffers
[36m(TaskRunner pid=3733940)[0m     _sync_params_and_buffers(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/utils.py", line 334, in _sync_params_and_buffers
[36m(TaskRunner pid=3733940)[0m     dist._broadcast_coalesced(
[36m(TaskRunner pid=3733940)[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 372.00 MiB. GPU 0 has a total capacity of 44.53 GiB of which 285.50 MiB is free. Process 3238586 has 42.70 GiB memory in use. Including non-PyTorch memory, this process has 1.54 GiB memory in use. Of the allocated memory 894.05 MiB is allocated by PyTorch, and 7.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(TaskRunner pid=3733940)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_init_model()[39m (pid=3743655, ip=222.29.51.203, actor_id=0c7a7ef97cffa65db2184afc01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7fa41b2a9d80>)
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/verl/verl/single_controller/ray/base.py", line 440, in func
[36m(TaskRunner pid=3733940)[0m     return getattr(self.worker_dict[key], name)(*args, **kwargs)
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/verl/verl/single_controller/base/decorator.py", line 413, in inner
[36m(TaskRunner pid=3733940)[0m     return func(*args, **kwargs)
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/verl/verl/workers/fsdp_workers.py", line 440, in init_model
[36m(TaskRunner pid=3733940)[0m     self._build_model_optimizer(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/verl/verl/workers/fsdp_workers.py", line 276, in _build_model_optimizer
[36m(TaskRunner pid=3733940)[0m     actor_module_fsdp = FSDP(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 483, in __init__
[36m(TaskRunner pid=3733940)[0m     _auto_wrap(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_wrap_utils.py", line 101, in _auto_wrap
[36m(TaskRunner pid=3733940)[0m     _recursive_wrap(**recursive_wrap_kwargs, **root_kwargs)  # type: ignore[arg-type]
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/wrap.py", line 545, in _recursive_wrap
[36m(TaskRunner pid=3733940)[0m     wrapped_child, num_wrapped_params = _recursive_wrap(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/wrap.py", line 545, in _recursive_wrap
[36m(TaskRunner pid=3733940)[0m     wrapped_child, num_wrapped_params = _recursive_wrap(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/wrap.py", line 545, in _recursive_wrap
[36m(TaskRunner pid=3733940)[0m     wrapped_child, num_wrapped_params = _recursive_wrap(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/wrap.py", line 563, in _recursive_wrap
[36m(TaskRunner pid=3733940)[0m     return _wrap(module, wrapper_cls, **kwargs), nonwrapped_numel
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/wrap.py", line 492, in _wrap
[36m(TaskRunner pid=3733940)[0m     return wrapper_cls(module, **kwargs)
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 509, in __init__
[36m(TaskRunner pid=3733940)[0m     _init_param_handle_from_module(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 629, in _init_param_handle_from_module
[36m(TaskRunner pid=3733940)[0m     _sync_module_params_and_buffers(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 1126, in _sync_module_params_and_buffers
[36m(TaskRunner pid=3733940)[0m     _sync_params_and_buffers(
[36m(TaskRunner pid=3733940)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/utils.py", line 334, in _sync_params_and_buffers
[36m(TaskRunner pid=3733940)[0m     dist._broadcast_coalesced(
[36m(TaskRunner pid=3733940)[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 372.00 MiB. GPU 0 has a total capacity of 44.53 GiB of which 325.50 MiB is free. Process 3238113 has 42.68 GiB memory in use. Including non-PyTorch memory, this process has 1.52 GiB memory in use. Of the allocated memory 894.05 MiB is allocated by PyTorch, and 7.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(WorkerDict pid=3743655)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:15<00:00,  3.69s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:15<00:00,  3.77s/it][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3743655)[0m [rank4]:[W501 05:53:35.163807290 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 4]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3743655)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=3743653)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f20562068c0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f20562067a0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 7x across cluster][0m
