2025-05-10 04:12:19,499	INFO worker.py:1832 -- Started a local Ray instance. View the dashboard at [1m[32m127.0.0.1:8265 [39m[22m
[36m(TaskRunner pid=801968)[0m {'actor_rollout_ref': {'actor': {'checkpoint': {'contents': ['model',
[36m(TaskRunner pid=801968)[0m                                                              'optimizer',
[36m(TaskRunner pid=801968)[0m                                                              'extra']},
[36m(TaskRunner pid=801968)[0m                                  'clip_ratio': 0.2,
[36m(TaskRunner pid=801968)[0m                                  'clip_ratio_c': 3.0,
[36m(TaskRunner pid=801968)[0m                                  'clip_ratio_high': 0.2,
[36m(TaskRunner pid=801968)[0m                                  'clip_ratio_low': 0.2,
[36m(TaskRunner pid=801968)[0m                                  'entropy_coeff': 0,
[36m(TaskRunner pid=801968)[0m                                  'fsdp_config': {'fsdp_size': -1,
[36m(TaskRunner pid=801968)[0m                                                  'optimizer_offload': False,
[36m(TaskRunner pid=801968)[0m                                                  'param_offload': False,
[36m(TaskRunner pid=801968)[0m                                                  'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=801968)[0m                                  'grad_clip': 1.0,
[36m(TaskRunner pid=801968)[0m                                  'kl_loss_coef': 0.001,
[36m(TaskRunner pid=801968)[0m                                  'kl_loss_type': 'low_var_kl',
[36m(TaskRunner pid=801968)[0m                                  'loss_agg_mode': 'seq-mean-token-sum-norm',
[36m(TaskRunner pid=801968)[0m                                  'optim': {'lr': 1e-06,
[36m(TaskRunner pid=801968)[0m                                            'lr_warmup_steps': -1,
[36m(TaskRunner pid=801968)[0m                                            'lr_warmup_steps_ratio': 0.0,
[36m(TaskRunner pid=801968)[0m                                            'min_lr_ratio': None,
[36m(TaskRunner pid=801968)[0m                                            'total_training_steps': -1,
[36m(TaskRunner pid=801968)[0m                                            'warmup_style': 'constant',
[36m(TaskRunner pid=801968)[0m                                            'weight_decay': 0.01},
[36m(TaskRunner pid=801968)[0m                                  'ppo_epochs': 1,
[36m(TaskRunner pid=801968)[0m                                  'ppo_max_token_len_per_gpu': 16384,
[36m(TaskRunner pid=801968)[0m                                  'ppo_micro_batch_size': None,
[36m(TaskRunner pid=801968)[0m                                  'ppo_micro_batch_size_per_gpu': 8,
[36m(TaskRunner pid=801968)[0m                                  'ppo_mini_batch_size': 32,
[36m(TaskRunner pid=801968)[0m                                  'shuffle': False,
[36m(TaskRunner pid=801968)[0m                                  'strategy': 'fsdp',
[36m(TaskRunner pid=801968)[0m                                  'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=801968)[0m                                  'use_dynamic_bsz': False,
[36m(TaskRunner pid=801968)[0m                                  'use_kl_loss': True,
[36m(TaskRunner pid=801968)[0m                                  'use_torch_compile': True},
[36m(TaskRunner pid=801968)[0m                        'hybrid_engine': True,
[36m(TaskRunner pid=801968)[0m                        'model': {'enable_gradient_checkpointing': True,
[36m(TaskRunner pid=801968)[0m                                  'external_lib': None,
[36m(TaskRunner pid=801968)[0m                                  'override_config': {},
[36m(TaskRunner pid=801968)[0m                                  'path': '/home/wangyc/verl/Qwen/Qwen2.5-7B-Instruct',
[36m(TaskRunner pid=801968)[0m                                  'use_liger': False,
[36m(TaskRunner pid=801968)[0m                                  'use_remove_padding': True},
[36m(TaskRunner pid=801968)[0m                        'ref': {'fsdp_config': {'param_offload': True,
[36m(TaskRunner pid=801968)[0m                                                'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=801968)[0m                                'log_prob_max_token_len_per_gpu': 16384,
[36m(TaskRunner pid=801968)[0m                                'log_prob_micro_batch_size': None,
[36m(TaskRunner pid=801968)[0m                                'log_prob_micro_batch_size_per_gpu': 32,
[36m(TaskRunner pid=801968)[0m                                'log_prob_use_dynamic_bsz': False,
[36m(TaskRunner pid=801968)[0m                                'strategy': 'fsdp',
[36m(TaskRunner pid=801968)[0m                                'ulysses_sequence_parallel_size': 1},
[36m(TaskRunner pid=801968)[0m                        'rollout': {'disable_log_stats': True,
[36m(TaskRunner pid=801968)[0m                                    'do_sample': True,
[36m(TaskRunner pid=801968)[0m                                    'dtype': 'bfloat16',
[36m(TaskRunner pid=801968)[0m                                    'enable_chunked_prefill': True,
[36m(TaskRunner pid=801968)[0m                                    'enforce_eager': True,
[36m(TaskRunner pid=801968)[0m                                    'engine_kwargs': {'swap_space': None},
[36m(TaskRunner pid=801968)[0m                                    'free_cache_engine': True,
[36m(TaskRunner pid=801968)[0m                                    'gpu_memory_utilization': 0.6,
[36m(TaskRunner pid=801968)[0m                                    'ignore_eos': False,
[36m(TaskRunner pid=801968)[0m                                    'load_format': 'dummy_dtensor',
[36m(TaskRunner pid=801968)[0m                                    'log_prob_max_token_len_per_gpu': 16384,
[36m(TaskRunner pid=801968)[0m                                    'log_prob_micro_batch_size': None,
[36m(TaskRunner pid=801968)[0m                                    'log_prob_micro_batch_size_per_gpu': 32,
[36m(TaskRunner pid=801968)[0m                                    'log_prob_use_dynamic_bsz': False,
[36m(TaskRunner pid=801968)[0m                                    'max_model_len': None,
[36m(TaskRunner pid=801968)[0m                                    'max_num_batched_tokens': 8192,
[36m(TaskRunner pid=801968)[0m                                    'max_num_seqs': 1024,
[36m(TaskRunner pid=801968)[0m                                    'n': 6,
[36m(TaskRunner pid=801968)[0m                                    'name': 'vllm',
[36m(TaskRunner pid=801968)[0m                                    'prompt_length': 1024,
[36m(TaskRunner pid=801968)[0m                                    'response_length': 1024,
[36m(TaskRunner pid=801968)[0m                                    'temperature': 1.0,
[36m(TaskRunner pid=801968)[0m                                    'tensor_model_parallel_size': 2,
[36m(TaskRunner pid=801968)[0m                                    'top_k': -1,
[36m(TaskRunner pid=801968)[0m                                    'top_p': 1,
[36m(TaskRunner pid=801968)[0m                                    'use_fire_sampling': False,
[36m(TaskRunner pid=801968)[0m                                    'val_kwargs': {'do_sample': False,
[36m(TaskRunner pid=801968)[0m                                                   'n': 1,
[36m(TaskRunner pid=801968)[0m                                                   'temperature': 0,
[36m(TaskRunner pid=801968)[0m                                                   'top_k': -1,
[36m(TaskRunner pid=801968)[0m                                                   'top_p': 1.0}}},
[36m(TaskRunner pid=801968)[0m  'algorithm': {'adv_estimator': 'grpo',
[36m(TaskRunner pid=801968)[0m                'gamma': 1.0,
[36m(TaskRunner pid=801968)[0m                'kl_ctrl': {'horizon': 10000,
[36m(TaskRunner pid=801968)[0m                            'kl_coef': 0.001,
[36m(TaskRunner pid=801968)[0m                            'target_kl': 0.1,
[36m(TaskRunner pid=801968)[0m                            'type': 'fixed'},
[36m(TaskRunner pid=801968)[0m DeprecationWarning: `ray.state.available_resources_per_node` is a private attribute and access will be removed in a future Ray version.
[36m(TaskRunner pid=801968)[0m WARNING:2025-05-10 04:12:29,991:Waiting for register center actor eOTdow_register_center to be ready. Elapsed time: 0 seconds out of 300 seconds.
[36m(WorkerDict pid=810833)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=810833)[0m Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 63.64it/s]
[36m(WorkerDict pid=810842)[0m Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[36m(TaskRunner pid=801968)[0m                'kl_penalty': 'kl',
[36m(TaskRunner pid=801968)[0m                'lam': 1.0,
[36m(TaskRunner pid=801968)[0m                'norm_adv_by_std_in_grpo': False,
[36m(TaskRunner pid=801968)[0m                'use_kl_in_reward': False},
[36m(TaskRunner pid=801968)[0m  'critic': {'checkpoint': {'contents': ['model', 'optimizer', 'extra']},
[36m(TaskRunner pid=801968)[0m             'cliprange_value': 0.5,
[36m(TaskRunner pid=801968)[0m             'forward_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=801968)[0m             'forward_micro_batch_size': None,
[36m(TaskRunner pid=801968)[0m             'forward_micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=801968)[0m             'grad_clip': 1.0,
[36m(TaskRunner pid=801968)[0m             'model': {'enable_gradient_checkpointing': True,
[36m(TaskRunner pid=801968)[0m                       'external_lib': None,
[36m(TaskRunner pid=801968)[0m                       'fsdp_config': {'fsdp_size': -1,
[36m(TaskRunner pid=801968)[0m                                       'optimizer_offload': False,
[36m(TaskRunner pid=801968)[0m                                       'param_offload': False,
[36m(TaskRunner pid=801968)[0m                                       'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=801968)[0m                       'override_config': {},
[36m(TaskRunner pid=801968)[0m                       'path': '~/models/deepseek-llm-7b-chat',
[36m(TaskRunner pid=801968)[0m                       'tokenizer_path': '/home/wangyc/verl/Qwen/Qwen2.5-7B-Instruct',
[36m(TaskRunner pid=801968)[0m                       'use_remove_padding': False},
[36m(TaskRunner pid=801968)[0m             'optim': {'lr': 1e-05,
[36m(TaskRunner pid=801968)[0m                       'lr_warmup_steps_ratio': 0.0,
[36m(TaskRunner pid=801968)[0m                       'min_lr_ratio': None,
[36m(TaskRunner pid=801968)[0m                       'total_training_steps': -1,
[36m(TaskRunner pid=801968)[0m                       'warmup_style': 'constant',
[36m(TaskRunner pid=801968)[0m                       'weight_decay': 0.01},
[36m(TaskRunner pid=801968)[0m             'ppo_epochs': 1,
[36m(TaskRunner pid=801968)[0m             'ppo_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=801968)[0m             'ppo_micro_batch_size': None,
[36m(TaskRunner pid=801968)[0m             'ppo_micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=801968)[0m             'ppo_mini_batch_size': 32,
[36m(TaskRunner pid=801968)[0m             'rollout_n': 6,
[36m(TaskRunner pid=801968)[0m             'shuffle': False,
[36m(TaskRunner pid=801968)[0m             'strategy': 'fsdp',
[36m(TaskRunner pid=801968)[0m             'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=801968)[0m             'use_dynamic_bsz': False},
[36m(TaskRunner pid=801968)[0m  'custom_reward_function': {'name': 'compute_score', 'path': None},
[36m(TaskRunner pid=801968)[0m  'data': {'custom_cls': {'name': None, 'path': None},
[36m(TaskRunner pid=801968)[0m           'filter_overlong_prompts': False,
[36m(TaskRunner pid=801968)[0m           'filter_overlong_prompts_workers': 1,
[36m(TaskRunner pid=801968)[0m           'image_key': 'images',
[36m(TaskRunner pid=801968)[0m           'max_prompt_length': 1024,
[36m(TaskRunner pid=801968)[0m           'max_response_length': 1024,
[36m(TaskRunner pid=801968)[0m           'prompt_key': 'prompt',
[36m(TaskRunner pid=801968)[0m           'return_raw_chat': False,
[36m(TaskRunner pid=801968)[0m           'return_raw_input_ids': False,
[36m(TaskRunner pid=801968)[0m           'reward_fn_key': 'data_source',
[36m(TaskRunner pid=801968)[0m           'shuffle': True,
[36m(TaskRunner pid=801968)[0m           'tokenizer': None,
[36m(TaskRunner pid=801968)[0m           'train_batch_size': 64,
[36m(TaskRunner pid=801968)[0m           'train_files': '/home/wangyc/verl/data/jec-qa-1-multi-choice/train.parquet',
[36m(TaskRunner pid=801968)[0m           'truncation': 'error',
[36m(TaskRunner pid=801968)[0m           'val_batch_size': 1312,
[36m(TaskRunner pid=801968)[0m           'val_files': '/home/wangyc/verl/data/jec-qa-1-multi-choice/test.parquet',
[36m(TaskRunner pid=801968)[0m           'video_key': 'videos'},
[36m(TaskRunner pid=801968)[0m  'ray_init': {'num_cpus': None},
[36m(TaskRunner pid=801968)[0m  'reward_model': {'enable': False,
[36m(TaskRunner pid=801968)[0m                   'forward_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=801968)[0m                   'max_length': None,
[36m(TaskRunner pid=801968)[0m                   'micro_batch_size': None,
[36m(TaskRunner pid=801968)[0m                   'micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=801968)[0m                   'model': {'external_lib': None,
[36m(TaskRunner pid=801968)[0m                             'fsdp_config': {'fsdp_size': -1,
[36m(TaskRunner pid=801968)[0m                                             'param_offload': False,
[36m(TaskRunner pid=801968)[0m                                             'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=801968)[0m                             'input_tokenizer': '/home/wangyc/verl/Qwen/Qwen2.5-7B-Instruct',
[36m(TaskRunner pid=801968)[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',
[36m(TaskRunner pid=801968)[0m                             'use_remove_padding': False},
[36m(TaskRunner pid=801968)[0m                   'reward_manager': 'naive',
[36m(TaskRunner pid=801968)[0m                   'strategy': 'fsdp',
[36m(TaskRunner pid=801968)[0m                   'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=801968)[0m                   'use_dynamic_bsz': False},
[36m(TaskRunner pid=801968)[0m  'trainer': {'balance_batch': True,
[36m(TaskRunner pid=801968)[0m              'critic_warmup': 0,
[36m(TaskRunner pid=801968)[0m              'default_hdfs_dir': None,
[36m(TaskRunner pid=801968)[0m              'default_local_dir': 'checkpoints/qwen2.5-7b-grpo-hard-mcq',
[36m(TaskRunner pid=801968)[0m              'del_local_ckpt_after_load': False,
[36m(TaskRunner pid=801968)[0m              'experiment_name': 'qwen2.5-7b-grpo-mcq',
[36m(TaskRunner pid=801968)[0m              'log_val_generations': 0,
[36m(TaskRunner pid=801968)[0m              'logger': ['console', 'wandb'],
[36m(TaskRunner pid=801968)[0m              'max_actor_ckpt_to_keep': None,
[36m(TaskRunner pid=801968)[0m              'max_critic_ckpt_to_keep': None,
[36m(TaskRunner pid=801968)[0m              'n_gpus_per_node': 8,
[36m(TaskRunner pid=801968)[0m              'nnodes': 1,
[36m(TaskRunner pid=801968)[0m              'project_name': 'Lawyer-Zero',
[36m(TaskRunner pid=801968)[0m              'ray_wait_register_center_timeout': 300,
[36m(TaskRunner pid=801968)[0m              'resume_from_path': None,
[36m(TaskRunner pid=801968)[0m              'resume_mode': 'auto',
[36m(TaskRunner pid=801968)[0m              'save_freq': 50,
[36m(TaskRunner pid=801968)[0m              'test_freq': 10,
[36m(TaskRunner pid=801968)[0m              'total_epochs': 2,
[36m(TaskRunner pid=801968)[0m              'total_training_steps': None,
[36m(TaskRunner pid=801968)[0m              'val_before_train': True}}
[36m(TaskRunner pid=801968)[0m WARNING: val_batch_size is deprecated. Validation datasets are sent to inference engines as a whole batch, which will schedule the memory themselves.
[36m(TaskRunner pid=801968)[0m [validate_config] All configuration checks passed successfully!
[36m(TaskRunner pid=801968)[0m dataset len: 8448
[36m(TaskRunner pid=801968)[0m dataset len: 2113
[36m(TaskRunner pid=801968)[0m Size of train dataloader: 132
[36m(TaskRunner pid=801968)[0m Total training steps: 264
[36m(WorkerDict pid=810833)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
[36m(WorkerDict pid=804890)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=804890)[0m   "architectures": [
[36m(WorkerDict pid=810833)[0m [rank1]:[W510 04:12:44.537954608 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[36m(WorkerDict pid=810842)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 66.15it/s]
[36m(WorkerDict pid=804890)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=810854)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=810854)[0m Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 63.84it/s][32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=804890)[0m Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][32m [repeated 4x across cluster][0m
[36m(WorkerDict pid=810854)[0m [rank7]:[W510 04:12:45.962716580 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 7]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=810847)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 65.82it/s][32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=810833)[0m Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:03<00:11,  3.84s/it]
[36m(WorkerDict pid=810854)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=810854)[0m Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=810833)[0m Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:10<00:03,  3.63s/it][32m [repeated 16x across cluster][0m
[36m(WorkerDict pid=810842)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:14<00:00,  3.50s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:14<00:00,  3.56s/it]
[36m(WorkerDict pid=810833)[0m /home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=810833)[0m   warnings.warn(
[36m(WorkerDict pid=810845)[0m Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:11<00:03,  3.90s/it][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=810845)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:15<00:00,  3.72s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:15<00:00,  3.81s/it][32m [repeated 7x across cluster][0m
[36m(TaskRunner pid=801968)[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
[36m(TaskRunner pid=801968)[0m wandb: Currently logged in as: eang333cms (eang333cms-peking-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(TaskRunner pid=801968)[0m wandb: Tracking run with wandb version 0.19.9
[36m(TaskRunner pid=801968)[0m wandb: Run data is saved locally in /home/wangyc/verl/wandb/run-20250510_041334-thwmw1w3
[36m(TaskRunner pid=801968)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(TaskRunner pid=801968)[0m wandb: Syncing run qwen2.5-7b-grpo-mcq
[36m(TaskRunner pid=801968)[0m wandb: â­ï¸ View project at https://wandb.ai/eang333cms-peking-university/Lawyer-Zero
[36m(TaskRunner pid=801968)[0m wandb: ğŸš€ View run at https://wandb.ai/eang333cms-peking-university/Lawyer-Zero/runs/thwmw1w3
[36m(WorkerDict pid=804890)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=804890)[0m   ],
[36m(WorkerDict pid=804890)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=804890)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=804890)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=804890)[0m   "hidden_size": 3584,
[36m(WorkerDict pid=804890)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=804890)[0m   "intermediate_size": 18944,
[36m(WorkerDict pid=804890)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=804890)[0m   "max_window_layers": 28,
[36m(WorkerDict pid=804890)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=804890)[0m   "num_attention_heads": 28,
[36m(WorkerDict pid=804890)[0m   "num_hidden_layers": 28,
[36m(WorkerDict pid=804890)[0m   "num_key_value_heads": 4,
[36m(WorkerDict pid=804890)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=804890)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=804890)[0m   "rope_scaling": null,
[36m(WorkerDict pid=804890)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=804890)[0m   "sliding_window": 131072,
[36m(WorkerDict pid=804890)[0m   "tie_word_embeddings": false,
[36m(WorkerDict pid=804890)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=804890)[0m   "transformers_version": "4.51.3",
[36m(WorkerDict pid=804890)[0m   "use_cache": true,
[36m(WorkerDict pid=804890)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=804890)[0m   "vocab_size": 152064
[36m(WorkerDict pid=804890)[0m }
[36m(WorkerDict pid=804890)[0m 
[36m(WorkerDict pid=804890)[0m NCCL version 2.21.5+cuda12.4
[36m(WorkerDict pid=804890)[0m Qwen2ForCausalLM contains 7.62B parameters
[36m(WorkerDict pid=804890)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f8f22e16710>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f8f22e165f0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=804890)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=810847)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(WorkerDict pid=804890)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=804890)[0m   "architectures": [
[36m(WorkerDict pid=804890)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=804890)[0m   ],
[36m(WorkerDict pid=804890)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=804890)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=804890)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=804890)[0m   "hidden_size": 3584,
[36m(WorkerDict pid=804890)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=804890)[0m   "intermediate_size": 18944,
[36m(WorkerDict pid=804890)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=804890)[0m   "max_window_layers": 28,
[36m(WorkerDict pid=804890)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=804890)[0m   "num_attention_heads": 28,
[36m(WorkerDict pid=804890)[0m   "num_hidden_layers": 28,
[36m(WorkerDict pid=804890)[0m   "num_key_value_heads": 4,
[36m(WorkerDict pid=804890)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=804890)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=804890)[0m   "rope_scaling": null,
[36m(WorkerDict pid=804890)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=804890)[0m   "sliding_window": 131072,
[36m(WorkerDict pid=804890)[0m   "tie_word_embeddings": false,
[36m(WorkerDict pid=804890)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=804890)[0m   "transformers_version": "4.51.3",
[36m(WorkerDict pid=804890)[0m   "use_cache": true,
[36m(WorkerDict pid=804890)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=804890)[0m   "vocab_size": 152064
[36m(WorkerDict pid=804890)[0m }
[36m(WorkerDict pid=804890)[0m 
[36m(WorkerDict pid=810847)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f6397f1e710>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f6397f1e5f0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=810847)[0m Actor use_remove_padding=True[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=810842)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
[36m(WorkerDict pid=810847)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
[36m(WorkerDict pid=804890)[0m Qwen2ForCausalLM contains 7.62B parameters
[36m(WorkerDict pid=804890)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f8f22e16710>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f8f22e165f0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=810851)[0m Total steps: 264, num_warmup_steps: 0
[36m(WorkerDict pid=810851)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=810845)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=810847)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f6397f1e710>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f6397f1e5f0>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=810833)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=804890)[0m Before building vllm rollout, memory allocated (GB): 3.5471720695495605, memory reserved (GB): 19.66015625
[36m(WorkerDict pid=810851)[0m WARNING 05-10 04:13:20 [cuda.py:96] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=804890)[0m Total steps: 264, num_warmup_steps: 0[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=804890)[0m Actor use_remove_padding=True[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=810851)[0m WARNING 05-10 04:13:21 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fc1a465c2b0>
[36m(WorkerDict pid=810851)[0m NCCL version 2.21.5+cuda12.4
[36m(WorkerDict pid=810833)[0m kwargs: {'n': 6, 'logprobs': 0, 'max_tokens': 1024, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
[36m(WorkerDict pid=804890)[0m WARNING 05-10 04:13:21 [cuda.py:96] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=804890)[0m WARNING 05-10 04:13:22 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f8d54990bb0>[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=810845)[0m NCCL version 2.21.5+cuda12.4[32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=804890)[0m After building vllm rollout, memory allocated (GB): 16.592182159423828, memory reserved (GB): 20.4765625
[36m(WorkerDict pid=804890)[0m After building sharding manager, memory allocated (GB): 16.592182159423828, memory reserved (GB): 20.4765625
[36m(TaskRunner pid=801968)[0m Using LocalLogger is deprecated. The constructor API will change 
[36m(TaskRunner pid=801968)[0m Checkpoint tracker file does not exist: %s /home/wangyc/verl/checkpoints/qwen2.5-7b-grpo-hard-mcq/latest_checkpointed_iteration.txt
[36m(TaskRunner pid=801968)[0m Training from scratch
[36m(TaskRunner pid=801968)[0m test_gen_batch meta info: {'eos_token_id': 151645, 'pad_token_id': 151643, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
[36m(WorkerDict pid=810854)[0m kwargs: {'n': 6, 'logprobs': 0, 'max_tokens': 1024, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}[32m [repeated 7x across cluster][0m
[36m(TaskRunner pid=801968)[0m validation generation end
[36m(TaskRunner pid=801968)[0m [prompt] system
[36m(TaskRunner pid=801968)[0m ç”¨æˆ·å’ŒåŠ©æ‰‹ä¹‹é—´çš„å¯¹è¯ã€‚ç”¨æˆ·æå‡ºä¸€ä¸ªé—®é¢˜ï¼Œç”±åŠ©æ‰‹æ¥å›ç­”ã€‚åŠ©æ‰‹é¦–å…ˆåœ¨è„‘æµ·ä¸­é€æ­¥æ€è€ƒæ¨ç†è¿‡ç¨‹ï¼Œç„¶åå‘ç”¨æˆ·æä¾›ç­”æ¡ˆã€‚æ¨ç†è¿‡ç¨‹å’Œç­”æ¡ˆåˆ†åˆ«ç”¨<æ€è€ƒ> </æ€è€ƒ>å’Œ<å›ç­”> </å›ç­”>æ ‡ç­¾æ‹¬èµ·æ¥ï¼Œå³ï¼Œ<æ€è€ƒ> æ¨ç†è¿‡ç¨‹ </æ€è€ƒ><å›ç­”> ç­”æ¡ˆ </å›ç­”>
[36m(TaskRunner pid=801968)[0m user
[36m(TaskRunner pid=801968)[0m ä½ æ˜¯ä¸€åæ³•å­¦ä¸“å®¶ã€‚ç°åœ¨è¯·ä½ è§£ç­”å¸æ³•è€ƒè¯•ä¸­çš„ä¸€é“é€‰æ‹©é¢˜ï¼Œè¯·ä½ æ‰¾å‡ºæ‰€æœ‰æ­£ç¡®çš„é€‰é¡¹ã€‚æ¯é“é¢˜å¯èƒ½æœ‰ä¸€ä¸ªæˆ–è€…å¤šä¸ªæ­£ç¡®ç­”æ¡ˆã€‚åœ¨è§£ç­”ä¹‹å‰ï¼Œä½ éœ€è¦å…ˆé’ˆå¯¹æ¯ä¸ªæä¾›çš„é€‰é¡¹ç»™å‡ºè¯¦ç»†çš„è§£é‡Šã€‚ä½ éœ€è¦åœ¨å›ç­”çš„æœ€åç”¨å¤§æ‹¬å·åœˆå‡ºç»™å‡ºçš„ç­”æ¡ˆï¼Œä¾‹å¦‚"{B}"æˆ–è€…"{ABD}"ã€‚
[36m(TaskRunner pid=801968)[0m 
[36m(TaskRunner pid=801968)[0m é—®é¢˜ï¼šç”²æŸä»¥ä¸ºå¢ƒå¤–çªƒå–ã€åˆºæ¢ã€æ”¶ä¹°å›½å®¶ç§˜å¯†çš„æ•…æ„ï¼Œä¹™ä»¥éæ³•è·å–å›½å®¶ç§˜å¯†çš„æ•…æ„ï¼Œå…±è°‹å…±åŒçªƒå–ã€åˆºæ¢ã€æ”¶ä¹°å›½å®¶ç§˜å¯†ï¼Œå¯¹äºç”²ä¹™äºŒäººçš„è¡Œä¸ºçš„è®¤å®šï¼Œä¸‹åˆ—æ­£ç¡®çš„é€‰é¡¹æ˜¯:
[36m(TaskRunner pid=801968)[0m 
[36m(TaskRunner pid=801968)[0m é€‰é¡¹ï¼š
[36m(TaskRunner pid=801968)[0m A: ç”²ä¸ä¹™å› æ•…æ„ä¸åŒï¼Œä¸æˆç«‹å…±åŒçŠ¯ç½ª
[36m(TaskRunner pid=801968)[0m B: ç”²ä¸ä¹™è™½æ•…æ„ä¸åŒï¼Œä»åœ¨éæ³•è·å–å›½å®¶ç§˜å¯†ç½ªçš„èŒƒå›´å†…æˆç«‹å…±åŒçŠ¯ç½ª
[36m(TaskRunner pid=801968)[0m C: å¯¹ç”²åº”ä»¥ä¸ºå¢ƒå¤–çªƒå–ã€åˆºæ¢ã€æ”¶ä¹°å›½å®¶ç§˜å¯†ç½ªè®ºå¤„
[36m(TaskRunner pid=801968)[0m D: å¯¹ä¹™ä»¥éæ³•è·å–å›½å®¶ç§˜å¯†ç½ªè®ºå¤„
[36m(TaskRunner pid=801968)[0m assistant
[36m(TaskRunner pid=801968)[0m 
[36m(TaskRunner pid=801968)[0m [response] <æ€è€ƒ> æœ¬é¢˜è€ƒæŸ¥å…±åŒçŠ¯ç½ªçš„è®¤å®šã€‚æ ¹æ®æˆ‘å›½åˆ‘æ³•ï¼Œå…±åŒçŠ¯ç½ªæ˜¯æŒ‡äºŒäººä»¥ä¸Šå…±åŒæ•…æ„çŠ¯ç½ªã€‚å…±åŒçŠ¯ç½ªçš„æˆç«‹éœ€è¦å…±åŒçš„çŠ¯ç½ªæ•…æ„å’Œå…±åŒçš„è¡Œä¸ºã€‚ç”²æŸä»¥ä¸ºå¢ƒå¤–çªƒå–ã€åˆºæ¢ã€æ”¶ä¹°å›½å®¶ç§˜å¯†çš„æ•…æ„ï¼Œä¹™ä»¥éæ³•è·å–å›½å®¶ç§˜å¯†çš„æ•…æ„ï¼Œè™½ç„¶ä¸¤äººçš„æ•…æ„ä¸åŒï¼Œä½†éƒ½æ¶‰åŠçªƒå–ã€åˆºæ¢ã€æ”¶ä¹°å›½å®¶ç§˜å¯†çš„è¡Œä¸ºï¼Œå› æ­¤åœ¨éæ³•è·å–å›½å®¶ç§˜å¯†ç½ªçš„èŒƒå›´å†…æˆç«‹å…±åŒçŠ¯ç½ªã€‚ç”²æŸçš„è¡Œä¸ºç¬¦åˆä»¥å¢ƒå¤–çªƒå–ã€åˆºæ¢ã€æ”¶ä¹°å›½å®¶ç§˜å¯†ç½ªçš„æ„æˆè¦ä»¶ï¼Œä¹™çš„è¡Œä¸ºç¬¦åˆéæ³•è·å–å›½å®¶ç§˜å¯†ç½ªçš„æ„æˆè¦ä»¶ã€‚å› æ­¤ï¼Œé€‰é¡¹Bå’ŒCã€Déƒ½æ˜¯æ­£ç¡®çš„ã€‚
[36m(TaskRunner pid=801968)[0m </æ€è€ƒ><å›ç­”> é€‰é¡¹Bã€Cã€Déƒ½æ˜¯æ­£ç¡®çš„ã€‚ç”²ä¸ä¹™è™½æ•…æ„ä¸åŒï¼Œä½†åœ¨éæ³•è·å–å›½å®¶ç§˜å¯†ç½ªçš„èŒƒå›´å†…æˆç«‹å…±åŒçŠ¯ç½ªã€‚å¯¹ç”²åº”ä»¥ä¸ºå¢ƒå¤–çªƒå–ã€åˆºæ¢ã€æ”¶ä¹°å›½å®¶ç§˜å¯†ç½ªè®ºå¤„ï¼Œå¯¹ä¹™ä»¥éæ³•è·å–å›½å®¶ç§˜å¯†ç½ªè®ºå¤„ã€‚{BCD}
[36m(TaskRunner pid=801968)[0m [ground_truth] ['B', 'C', 'D']
[36m(TaskRunner pid=801968)[0m [score] 1.0
[36m(TaskRunner pid=801968)[0m [format_reward] 0.0
[36m(TaskRunner pid=801968)[0m [answer_reward] 1.0
[36m(TaskRunner pid=801968)[0m [acc] 1.0
[36m(TaskRunner pid=801968)[0m [precision] 1.0
[36m(TaskRunner pid=801968)[0m [recall] 1.0
[36m(TaskRunner pid=801968)[0m [f1] 1.0
[36m(TaskRunner pid=801968)[0m ("Initial validation metrics: {'val-aux/jec-qa-1-multi-choice/reward/mean@1': "
[36m(TaskRunner pid=801968)[0m  'np.float64(1.0120681495504023), '
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/reward/std@1': np.float64(0.0), "
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/reward/best@1/mean': "
[36m(TaskRunner pid=801968)[0m  'np.float64(1.0120681495504023), '
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/reward/best@1/std': np.float64(0.0), "
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/reward/worst@1/mean': "
[36m(TaskRunner pid=801968)[0m  'np.float64(1.0120681495504023), '
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/reward/worst@1/std': np.float64(0.0), "
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/score/mean@1': "
[36m(TaskRunner pid=801968)[0m  "np.float64(1.0120681495504023), 'val-aux/jec-qa-1-multi-choice/score/std@1': "
[36m(TaskRunner pid=801968)[0m  "np.float64(0.0), 'val-aux/jec-qa-1-multi-choice/score/best@1/mean': "
[36m(TaskRunner pid=801968)[0m  'np.float64(1.0120681495504023), '
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/score/best@1/std': np.float64(0.0), "
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/score/worst@1/mean': "
[36m(TaskRunner pid=801968)[0m  'np.float64(1.0120681495504023), '
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/score/worst@1/std': np.float64(0.0), "
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/format_reward/mean@1': "
[36m(TaskRunner pid=801968)[0m  'np.float64(0.5669663984855655), '
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/format_reward/std@1': np.float64(0.0), "
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/format_reward/best@1/mean': "
[36m(TaskRunner pid=801968)[0m  'np.float64(0.5669663984855655), '
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/format_reward/best@1/std': np.float64(0.0), "
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/format_reward/worst@1/mean': "
[36m(TaskRunner pid=801968)[0m  'np.float64(0.5669663984855655), '
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/format_reward/worst@1/std': np.float64(0.0), "
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/answer_reward/mean@1': "
[36m(TaskRunner pid=801968)[0m  'np.float64(0.44510175106483674), '
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/answer_reward/std@1': np.float64(0.0), "
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/answer_reward/best@1/mean': "
[36m(TaskRunner pid=801968)[0m  'np.float64(0.44510175106483674), '
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/answer_reward/best@1/std': np.float64(0.0), "
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/answer_reward/worst@1/mean': "
[36m(TaskRunner pid=801968)[0m  'np.float64(0.44510175106483674), '
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/answer_reward/worst@1/std': np.float64(0.0), "
[36m(TaskRunner pid=801968)[0m  "'val-core/jec-qa-1-multi-choice/acc/mean@1': "
[36m(TaskRunner pid=801968)[0m  "np.float64(0.30430667297681024), 'val-aux/jec-qa-1-multi-choice/acc/std@1': "
[36m(TaskRunner pid=801968)[0m  "np.float64(0.0), 'val-core/jec-qa-1-multi-choice/acc/best@1/mean': "
[36m(TaskRunner pid=801968)[0m  'np.float64(0.30430667297681024), '
[36m(TaskRunner pid=801968)[0m  "'val-core/jec-qa-1-multi-choice/acc/best@1/std': np.float64(0.0), "
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/acc/worst@1/mean': "
[36m(TaskRunner pid=801968)[0m  'np.float64(0.30430667297681024), '
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/acc/worst@1/std': np.float64(0.0), "
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/precision/mean@1': "
[36m(TaskRunner pid=801968)[0m  'np.float64(0.586953778198454), '
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/precision/std@1': np.float64(0.0), "
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/precision/best@1/mean': "
[36m(TaskRunner pid=801968)[0m  'np.float64(0.586953778198454), '
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/precision/best@1/std': "
[36m(TaskRunner pid=801968)[0m  'np.float64(1.2071639370924264e-17), '
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/precision/worst@1/mean': "
[36m(TaskRunner pid=801968)[0m  'np.float64(0.586953778198454), '
[36m(TaskRunner pid=801968)[0m Training Progress:   0%|          | 0/264 [00:00<?, ?it/s]
[36m(WorkerDict pid=810854)[0m /home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=810854)[0m   warnings.warn([32m [repeated 7x across cluster][0m
[36m(TaskRunner pid=801968)[0m Training Progress:   0%|          | 1/264 [01:02<4:35:06, 62.76s/it]
[36m(TaskRunner pid=801968)[0m Training Progress:   1%|          | 2/264 [01:58<4:17:08, 58.89s/it]
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/precision/worst@1/std': "
[36m(TaskRunner pid=801968)[0m  'np.float64(1.2071639370924264e-17), '
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/recall/mean@1': "
[36m(TaskRunner pid=801968)[0m  'np.float64(0.7003076194983436), '
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/recall/std@1': np.float64(0.0), "
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/recall/best@1/mean': "
[36m(TaskRunner pid=801968)[0m  'np.float64(0.7003076194983436), '
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/recall/best@1/std': "
[36m(TaskRunner pid=801968)[0m  'np.float64(6.856796247684947e-18), '
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/recall/worst@1/mean': "
[36m(TaskRunner pid=801968)[0m  'np.float64(0.7003076194983436), '
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/recall/worst@1/std': "
[36m(TaskRunner pid=801968)[0m  'np.float64(6.856796247684947e-18), '
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/f1/mean@1': np.float64(0.6091677371374484), "
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/f1/std@1': np.float64(0.0), "
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/f1/best@1/mean': "
[36m(TaskRunner pid=801968)[0m  'np.float64(0.6091677371374483), '
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/f1/best@1/std': "
[36m(TaskRunner pid=801968)[0m  'np.float64(3.407381123849569e-17), '
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/f1/worst@1/mean': "
[36m(TaskRunner pid=801968)[0m  'np.float64(0.6091677371374483), '
[36m(TaskRunner pid=801968)[0m  "'val-aux/jec-qa-1-multi-choice/f1/worst@1/std': "
[36m(TaskRunner pid=801968)[0m  'np.float64(3.407381123849569e-17)}')
[36m(TaskRunner pid=801968)[0m step:0 - val-aux/jec-qa-1-multi-choice/reward/mean@1:1.012 - val-aux/jec-qa-1-multi-choice/reward/std@1:0.000 - val-aux/jec-qa-1-multi-choice/reward/best@1/mean:1.012 - val-aux/jec-qa-1-multi-choice/reward/best@1/std:0.000 - val-aux/jec-qa-1-multi-choice/reward/worst@1/mean:1.012 - val-aux/jec-qa-1-multi-choice/reward/worst@1/std:0.000 - val-aux/jec-qa-1-multi-choice/score/mean@1:1.012 - val-aux/jec-qa-1-multi-choice/score/std@1:0.000 - val-aux/jec-qa-1-multi-choice/score/best@1/mean:1.012 - val-aux/jec-qa-1-multi-choice/score/best@1/std:0.000 - val-aux/jec-qa-1-multi-choice/score/worst@1/mean:1.012 - val-aux/jec-qa-1-multi-choice/score/worst@1/std:0.000 - val-aux/jec-qa-1-multi-choice/format_reward/mean@1:0.567 - val-aux/jec-qa-1-multi-choice/format_reward/std@1:0.000 - val-aux/jec-qa-1-multi-choice/format_reward/best@1/mean:0.567 - val-aux/jec-qa-1-multi-choice/format_reward/best@1/std:0.000 - val-aux/jec-qa-1-multi-choice/format_reward/worst@1/mean:0.567 - val-aux/jec-qa-1-multi-choice/format_reward/worst@1/std:0.000 - val-aux/jec-qa-1-multi-choice/answer_reward/mean@1:0.445 - val-aux/jec-qa-1-multi-choice/answer_reward/std@1:0.000 - val-aux/jec-qa-1-multi-choice/answer_reward/best@1/mean:0.445 - val-aux/jec-qa-1-multi-choice/answer_reward/best@1/std:0.000 - val-aux/jec-qa-1-multi-choice/answer_reward/worst@1/mean:0.445 - val-aux/jec-qa-1-multi-choice/answer_reward/worst@1/std:0.000 - val-core/jec-qa-1-multi-choice/acc/mean@1:0.304 - val-aux/jec-qa-1-multi-choice/acc/std@1:0.000 - val-core/jec-qa-1-multi-choice/acc/best@1/mean:0.304 - val-core/jec-qa-1-multi-choice/acc/best@1/std:0.000 - val-aux/jec-qa-1-multi-choice/acc/worst@1/mean:0.304 - val-aux/jec-qa-1-multi-choice/acc/worst@1/std:0.000 - val-aux/jec-qa-1-multi-choice/precision/mean@1:0.587 - val-aux/jec-qa-1-multi-choice/precision/std@1:0.000 - val-aux/jec-qa-1-multi-choice/precision/best@1/mean:0.587 - val-aux/jec-qa-1-multi-choice/precision/best@1/std:0.000 - val-aux/jec-qa-1-multi-choice/precision/worst@1/mean:0.587 - val-aux/jec-qa-1-multi-choice/precision/worst@1/std:0.000 - val-aux/jec-qa-1-multi-choice/recall/mean@1:0.700 - val-aux/jec-qa-1-multi-choice/recall/std@1:0.000 - val-aux/jec-qa-1-multi-choice/recall/best@1/mean:0.700 - val-aux/jec-qa-1-multi-choice/recall/best@1/std:0.000 - val-aux/jec-qa-1-multi-choice/recall/worst@1/mean:0.700 - val-aux/jec-qa-1-multi-choice/recall/worst@1/std:0.000 - val-aux/jec-qa-1-multi-choice/f1/mean@1:0.609 - val-aux/jec-qa-1-multi-choice/f1/std@1:0.000 - val-aux/jec-qa-1-multi-choice/f1/best@1/mean:0.609 - val-aux/jec-qa-1-multi-choice/f1/best@1/std:0.000 - val-aux/jec-qa-1-multi-choice/f1/worst@1/mean:0.609 - val-aux/jec-qa-1-multi-choice/f1/worst@1/std:0.000
[36m(TaskRunner pid=801968)[0m list(reward_extra_infos_dict.keys())=['score', 'format_reward', 'answer_reward', 'acc', 'precision', 'recall', 'f1']
[36m(TaskRunner pid=801968)[0m step:1 - global_seqlen/min:29390.000 - global_seqlen/max:31157.000 - global_seqlen/minmax_diff:1767.000 - global_seqlen/balanced_min:30321.000 - global_seqlen/balanced_max:30322.000 - global_seqlen/mean:30321.250 - actor/entropy_loss:141.813 - actor/kl_loss:0.002 - actor/kl_coef:0.001 - actor/pg_loss:0.215 - actor/pg_clipfrac:0.001 - actor/ppo_kl:0.001 - actor/pg_clipfrac_lower:0.000 - actor/grad_norm:1.841 - perf/mfu/actor:0.262 - perf/max_memory_allocated_gb:36.123 - perf/max_memory_reserved_gb:50.168 - perf/cpu_memory_used_gb:46.294 - actor/lr:0.000 - critic/score/mean:0.928 - critic/score/max:2.000 - critic/score/min:0.000 - critic/rewards/mean:0.928 - critic/rewards/max:2.000 - critic/rewards/min:0.000 - critic/advantages/mean:0.002 - critic/advantages/max:1.583 - critic/advantages/min:-1.333 - critic/returns/mean:0.002 - critic/returns/max:1.583 - critic/returns/min:-1.333 - response_length/mean:321.193 - response_length/max:722.000 - response_length/min:116.000 - response_length/clip_ratio:0.000 - prompt_length/mean:310.500 - prompt_length/max:427.000 - prompt_length/min:213.000 - prompt_length/clip_ratio:0.000 - timing_s/gen:20.045 - timing_s/old_log_prob:3.801 - timing_s/ref:8.669 - timing_s/adv:0.180 - timing_s/update_actor:29.709 - timing_s/step:62.424 - timing_per_token_ms/update_actor:0.122 - timing_per_token_ms/gen:0.163 - timing_per_token_ms/adv:0.001 - timing_per_token_ms/ref:0.036 - perf/total_num_tokens:242570.000 - perf/time_per_step:62.424 - perf/throughput:485.728
[36m(TaskRunner pid=801968)[0m list(reward_extra_infos_dict.keys())=['score', 'format_reward', 'answer_reward', 'acc', 'precision', 'recall', 'f1']
[36m(TaskRunner pid=801968)[0m step:2 - global_seqlen/min:27062.000 - global_seqlen/max:32064.000 - global_seqlen/minmax_diff:5002.000 - global_seqlen/balanced_min:28880.000 - global_seqlen/balanced_max:28881.000 - global_seqlen/mean:28880.500 - actor/entropy_loss:135.433 - actor/kl_loss:0.003 - actor/kl_coef:0.001 - actor/pg_loss:-0.245 - actor/pg_clipfrac:0.002 - actor/ppo_kl:-0.000 - actor/pg_clipfrac_lower:0.000 - actor/grad_norm:1.600 - perf/mfu/actor:0.254 - perf/max_memory_allocated_gb:36.123 - perf/max_memory_reserved_gb:50.805 - perf/cpu_memory_used_gb:46.413 - actor/lr:0.000 - critic/score/mean:0.874 - critic/score/max:2.000 - critic/score/min:0.000 - critic/rewards/mean:0.874 - critic/rewards/max:2.000 - critic/rewards/min:0.000 - critic/advantages/mean:0.011 - critic/advantages/max:1.500 - critic/advantages/min:-1.083 - critic/returns/mean:0.011 - critic/returns/max:1.500 - critic/returns/min:-1.083 - response_length/mean:303.646 - response_length/max:665.000 - response_length/min:122.000 - response_length/clip_ratio:0.000 - prompt_length/mean:298.031 - prompt_length/max:471.000 - prompt_length/min:209.000 - prompt_length/clip_ratio:0.000 - timing_s/gen:19.595 - timing_s/old_log_prob:3.423 - timing_s/ref:3.695 - timing_s/adv:0.174 - timing_s/update_actor:29.256 - timing_s/step:56.156 - timing_per_token_ms/update_actor:0.127 - timing_per_token_ms/gen:0.168 - timing_per_token_ms/adv:0.001 - timing_per_token_ms/ref:0.016 - perf/total_num_tokens:231044.000 - perf/time_per_step:56.156 - perf/throughput:514.293
[36m(TaskRunner pid=801968)[0m list(reward_extra_infos_dict.keys())=['score', 'format_reward', 'answer_reward', 'acc', 'precision', 'recall', 'f1']
[36m(TaskRunner pid=801968)[0m Training Progress:   1%|          | 3/264 [02:55<4:10:59, 57.70s/it]
[36m(TaskRunner pid=801968)[0m step:3 - global_seqlen/min:29076.000 - global_seqlen/max:32167.000 - global_seqlen/minmax_diff:3091.000 - global_seqlen/balanced_min:30418.000 - global_seqlen/balanced_max:30419.000 - global_seqlen/mean:30418.375 - actor/entropy_loss:145.377 - actor/kl_loss:0.027 - actor/kl_coef:0.001 - actor/pg_loss:0.077 - actor/pg_clipfrac:0.003 - actor/ppo_kl:-0.000 - actor/pg_clipfrac_lower:0.000 - actor/grad_norm:1.556 - perf/mfu/actor:0.265 - perf/max_memory_allocated_gb:36.356 - perf/max_memory_reserved_gb:51.354 - perf/cpu_memory_used_gb:46.486 - actor/lr:0.000 - critic/score/mean:1.065 - critic/score/max:2.000 - critic/score/min:0.000 - critic/rewards/mean:1.065 - critic/rewards/max:2.000 - critic/rewards/min:0.000 - critic/advantages/mean:-0.009 - critic/advantages/max:1.333 - critic/advantages/min:-1.500 - critic/returns/mean:-0.009 - critic/returns/max:1.333 - critic/returns/min:-1.500 - response_length/mean:324.529 - response_length/max:665.000 - response_length/min:137.000 - response_length/clip_ratio:0.000 - prompt_length/mean:309.188 - prompt_length/max:446.000 - prompt_length/min:210.000 - prompt_length/clip_ratio:0.000 - timing_s/gen:19.333 - timing_s/old_log_prob:3.532 - timing_s/ref:3.761 - timing_s/adv:0.173 - timing_s/update_actor:29.461 - timing_s/step:56.271 - timing_per_token_ms/update_actor:0.121 - timing_per_token_ms/gen:0.155 - timing_per_token_ms/adv:0.001 - timing_per_token_ms/ref:0.015 - perf/total_num_tokens:243347.000 - perf/time_per_step:56.271 - perf/throughput:540.568
[36m(TaskRunner pid=801968)[0m list(reward_extra_infos_dict.keys())=['score', 'format_reward', 'answer_reward', 'acc', 'precision', 'recall', 'f1']
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff3d6ba272ec34e49b03564e0e01000000 Worker ID: cf2f8f6600aa3d0f2dfe78de4cd3e425842ce4e200e80b20e2bd44b5 Node ID: 91979f87d6f1b7c0f63d14d53c142969a382dfc69cd8fb12eda48382 Worker IP address: 222.29.51.203 Worker port: 46335 Worker PID: 810839 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
Error executing job with overrides: ['algorithm.adv_estimator=grpo', 'data.train_files=/home/wangyc/verl/data/jec-qa-1-multi-choice/train.parquet', 'data.val_files=/home/wangyc/verl/data/jec-qa-1-multi-choice/test.parquet', 'data.train_batch_size=64', 'data.val_batch_size=1312', 'data.max_prompt_length=1024', 'data.max_response_length=1024', 'actor_rollout_ref.model.path=/home/wangyc/verl/Qwen/Qwen2.5-7B-Instruct', 'actor_rollout_ref.actor.optim.lr=1e-6', 'actor_rollout_ref.model.use_remove_padding=True', 'actor_rollout_ref.actor.ppo_mini_batch_size=32', 'actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=8', 'actor_rollout_ref.actor.loss_agg_mode=seq-mean-token-sum-norm', 'actor_rollout_ref.actor.use_kl_loss=True', 'actor_rollout_ref.actor.kl_loss_coef=0.001', 'actor_rollout_ref.actor.kl_loss_type=low_var_kl', 'actor_rollout_ref.actor.entropy_coeff=0', 'actor_rollout_ref.model.enable_gradient_checkpointing=True', 'actor_rollout_ref.actor.fsdp_config.param_offload=False', 'actor_rollout_ref.actor.fsdp_config.optimizer_offload=False', 'actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=32', 'actor_rollout_ref.rollout.tensor_model_parallel_size=2', 'actor_rollout_ref.rollout.name=vllm', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.6', 'actor_rollout_ref.rollout.n=6', 'actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=32', 'actor_rollout_ref.ref.fsdp_config.param_offload=True', 'algorithm.use_kl_in_reward=False', 'algorithm.norm_adv_by_std_in_grpo=False', 'trainer.critic_warmup=0', "trainer.logger=['console','wandb']", 'trainer.project_name=Lawyer-Zero', 'trainer.experiment_name=qwen2.5-7b-grpo-mcq', 'trainer.n_gpus_per_node=8', 'trainer.nnodes=1', 'trainer.default_local_dir=checkpoints/qwen2.5-7b-grpo-hard-mcq', 'trainer.save_freq=50', 'trainer.test_freq=10', 'trainer.total_epochs=2']
Traceback (most recent call last):
  File "/home/wangyc/verl/verl/trainer/main_ppo.py", line 63, in main
    run_ppo(config)
  File "/home/wangyc/verl/verl/trainer/main_ppo.py", line 80, in run_ppo
    ray.get(runner.run.remote(config))
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/ray/_private/worker.py", line 2771, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/ray/_private/worker.py", line 919, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ActorDiedError): [36mray::TaskRunner.run()[39m (pid=801968, ip=222.29.51.203, actor_id=40b40fe70ef223f5657eda4301000000, repr=<main_ppo.TaskRunner object at 0x7fbe0c8eea10>)
  File "/home/wangyc/verl/verl/trainer/main_ppo.py", line 208, in run
    trainer.fit()
  File "/home/wangyc/verl/verl/trainer/ppo/ray_trainer.py", line 1048, in fit
    actor_output = self.actor_rollout_wg.update_actor(batch)
  File "/home/wangyc/verl/verl/single_controller/ray/base.py", line 43, in func
    output = ray.get(output)
ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.
	class_name: create_colocated_worker_cls.<locals>.WorkerDict
	actor_id: 3d6ba272ec34e49b03564e0e01000000
	pid: 810839
	name: eOTdowWorkerDict_0:2
	namespace: 418eadc0-82ca-4514-aa2c-aa9c0d730109
	ip: 222.29.51.203
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[36m(WorkerDict pid=810839)[0m /home/wangyc/miniconda3/envs/verl/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
[36m(WorkerDict pid=810839)[0m   warnings.warn('resource_tracker: There appear to be %d '
[36m(WorkerDict pid=810851)[0m /home/wangyc/miniconda3/envs/verl/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=810851)[0m   warnings.warn('resource_tracker: There appear to be %d '[32m [repeated 3x across cluster][0m
[33m(raylet)[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffa5df10db9f0e5499c1144e4001000000 Worker ID: c2a96287933e576f47995788895f73d6bf927f7631d4990fc2868ff2 Node ID: 91979f87d6f1b7c0f63d14d53c142969a382dfc69cd8fb12eda48382 Worker IP address: 222.29.51.203 Worker port: 46273 Worker PID: 804890 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.[32m [repeated 7x across cluster][0m
