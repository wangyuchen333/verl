2025-05-09 13:05:10,207	INFO worker.py:1832 -- Started a local Ray instance. View the dashboard at [1m[32m127.0.0.1:8265 [39m[22m
[36m(TaskRunner pid=3162693)[0m {'actor_rollout_ref': {'actor': {'checkpoint': {'contents': ['model',
[36m(TaskRunner pid=3162693)[0m                                                              'optimizer',
[36m(TaskRunner pid=3162693)[0m                                                              'extra']},
[36m(TaskRunner pid=3162693)[0m                                  'clip_ratio': 0.2,
[36m(TaskRunner pid=3162693)[0m                                  'clip_ratio_c': 3.0,
[36m(TaskRunner pid=3162693)[0m                                  'clip_ratio_high': 0.2,
[36m(TaskRunner pid=3162693)[0m                                  'clip_ratio_low': 0.2,
[36m(TaskRunner pid=3162693)[0m                                  'entropy_coeff': 0,
[36m(TaskRunner pid=3162693)[0m                                  'fsdp_config': {'fsdp_size': -1,
[36m(TaskRunner pid=3162693)[0m                                                  'optimizer_offload': False,
[36m(TaskRunner pid=3162693)[0m                                                  'param_offload': False,
[36m(TaskRunner pid=3162693)[0m                                                  'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=3162693)[0m                                  'grad_clip': 1.0,
[36m(TaskRunner pid=3162693)[0m                                  'kl_loss_coef': 0.001,
[36m(TaskRunner pid=3162693)[0m                                  'kl_loss_type': 'low_var_kl',
[36m(TaskRunner pid=3162693)[0m                                  'loss_agg_mode': 'seq-mean-token-sum-norm',
[36m(TaskRunner pid=3162693)[0m                                  'optim': {'lr': 1e-06,
[36m(TaskRunner pid=3162693)[0m                                            'lr_warmup_steps': -1,
[36m(TaskRunner pid=3162693)[0m                                            'lr_warmup_steps_ratio': 0.0,
[36m(TaskRunner pid=3162693)[0m                                            'min_lr_ratio': None,
[36m(TaskRunner pid=3162693)[0m                                            'total_training_steps': -1,
[36m(TaskRunner pid=3162693)[0m                                            'warmup_style': 'constant',
[36m(TaskRunner pid=3162693)[0m                                            'weight_decay': 0.01},
[36m(TaskRunner pid=3162693)[0m                                  'ppo_epochs': 1,
[36m(TaskRunner pid=3162693)[0m                                  'ppo_max_token_len_per_gpu': 16384,
[36m(TaskRunner pid=3162693)[0m                                  'ppo_micro_batch_size': None,
[36m(TaskRunner pid=3162693)[0m                                  'ppo_micro_batch_size_per_gpu': 8,
[36m(TaskRunner pid=3162693)[0m                                  'ppo_mini_batch_size': 32,
[36m(TaskRunner pid=3162693)[0m                                  'shuffle': False,
[36m(TaskRunner pid=3162693)[0m                                  'strategy': 'fsdp',
[36m(TaskRunner pid=3162693)[0m                                  'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=3162693)[0m                                  'use_dynamic_bsz': False,
[36m(TaskRunner pid=3162693)[0m                                  'use_kl_loss': True,
[36m(TaskRunner pid=3162693)[0m                                  'use_torch_compile': True},
[36m(TaskRunner pid=3162693)[0m                        'hybrid_engine': True,
[36m(TaskRunner pid=3162693)[0m                        'model': {'enable_gradient_checkpointing': True,
[36m(TaskRunner pid=3162693)[0m                                  'external_lib': None,
[36m(TaskRunner pid=3162693)[0m                                  'override_config': {},
[36m(TaskRunner pid=3162693)[0m                                  'path': '/home/wangyc/verl/Qwen/Qwen2.5-7B-Instruct',
[36m(TaskRunner pid=3162693)[0m                                  'use_liger': False,
[36m(TaskRunner pid=3162693)[0m                                  'use_remove_padding': True},
[36m(TaskRunner pid=3162693)[0m                        'ref': {'fsdp_config': {'param_offload': True,
[36m(TaskRunner pid=3162693)[0m                                                'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=3162693)[0m                                'log_prob_max_token_len_per_gpu': 16384,
[36m(TaskRunner pid=3162693)[0m                                'log_prob_micro_batch_size': None,
[36m(TaskRunner pid=3162693)[0m                                'log_prob_micro_batch_size_per_gpu': 32,
[36m(TaskRunner pid=3162693)[0m                                'log_prob_use_dynamic_bsz': False,
[36m(TaskRunner pid=3162693)[0m                                'strategy': 'fsdp',
[36m(TaskRunner pid=3162693)[0m                                'ulysses_sequence_parallel_size': 1},
[36m(TaskRunner pid=3162693)[0m                        'rollout': {'disable_log_stats': True,
[36m(TaskRunner pid=3162693)[0m                                    'do_sample': True,
[36m(TaskRunner pid=3162693)[0m                                    'dtype': 'bfloat16',
[36m(TaskRunner pid=3162693)[0m                                    'enable_chunked_prefill': True,
[36m(TaskRunner pid=3162693)[0m                                    'enforce_eager': True,
[36m(TaskRunner pid=3162693)[0m                                    'engine_kwargs': {'swap_space': None},
[36m(TaskRunner pid=3162693)[0m                                    'free_cache_engine': True,
[36m(TaskRunner pid=3162693)[0m                                    'gpu_memory_utilization': 0.6,
[36m(TaskRunner pid=3162693)[0m                                    'ignore_eos': False,
[36m(TaskRunner pid=3162693)[0m                                    'load_format': 'dummy_dtensor',
[36m(TaskRunner pid=3162693)[0m                                    'log_prob_max_token_len_per_gpu': 16384,
[36m(TaskRunner pid=3162693)[0m                                    'log_prob_micro_batch_size': None,
[36m(TaskRunner pid=3162693)[0m                                    'log_prob_micro_batch_size_per_gpu': 32,
[36m(TaskRunner pid=3162693)[0m                                    'log_prob_use_dynamic_bsz': False,
[36m(TaskRunner pid=3162693)[0m                                    'max_model_len': None,
[36m(TaskRunner pid=3162693)[0m                                    'max_num_batched_tokens': 8192,
[36m(TaskRunner pid=3162693)[0m                                    'max_num_seqs': 1024,
[36m(TaskRunner pid=3162693)[0m                                    'n': 6,
[36m(TaskRunner pid=3162693)[0m                                    'name': 'vllm',
[36m(TaskRunner pid=3162693)[0m                                    'prompt_length': 1024,
[36m(TaskRunner pid=3162693)[0m                                    'response_length': 1024,
[36m(TaskRunner pid=3162693)[0m                                    'temperature': 1.0,
[36m(TaskRunner pid=3162693)[0m                                    'tensor_model_parallel_size': 2,
[36m(TaskRunner pid=3162693)[0m                                    'top_k': -1,
[36m(TaskRunner pid=3162693)[0m                                    'top_p': 1,
[36m(TaskRunner pid=3162693)[0m                                    'use_fire_sampling': False,
[36m(TaskRunner pid=3162693)[0m                                    'val_kwargs': {'do_sample': False,
[36m(TaskRunner pid=3162693)[0m                                                   'n': 1,
[36m(TaskRunner pid=3162693)[0m                                                   'temperature': 0,
[36m(TaskRunner pid=3162693)[0m                                                   'top_k': -1,
[36m(TaskRunner pid=3162693)[0m                                                   'top_p': 1.0}}},
[36m(TaskRunner pid=3162693)[0m  'algorithm': {'adv_estimator': 'grpo',
[36m(TaskRunner pid=3162693)[0m                'gamma': 1.0,
[36m(TaskRunner pid=3162693)[0m                'kl_ctrl': {'horizon': 10000,
[36m(TaskRunner pid=3162693)[0m                            'kl_coef': 0.001,
[36m(TaskRunner pid=3162693)[0m                            'target_kl': 0.1,
[36m(TaskRunner pid=3162693)[0m DeprecationWarning: `ray.state.available_resources_per_node` is a private attribute and access will be removed in a future Ray version.
[36m(TaskRunner pid=3162693)[0m WARNING:2025-05-09 13:05:21,359:Waiting for register center actor cg7aWu_register_center to be ready. Elapsed time: 0 seconds out of 300 seconds.
[36m(WorkerDict pid=3180577)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=3180577)[0m Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[36m(TaskRunner pid=3162693)[0m                            'type': 'fixed'},
[36m(TaskRunner pid=3162693)[0m                'kl_penalty': 'kl',
[36m(TaskRunner pid=3162693)[0m                'lam': 1.0,
[36m(TaskRunner pid=3162693)[0m                'norm_adv_by_std_in_grpo': False,
[36m(TaskRunner pid=3162693)[0m                'use_kl_in_reward': False},
[36m(TaskRunner pid=3162693)[0m  'critic': {'checkpoint': {'contents': ['model', 'optimizer', 'extra']},
[36m(TaskRunner pid=3162693)[0m             'cliprange_value': 0.5,
[36m(TaskRunner pid=3162693)[0m             'forward_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=3162693)[0m             'forward_micro_batch_size': None,
[36m(TaskRunner pid=3162693)[0m             'forward_micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=3162693)[0m             'grad_clip': 1.0,
[36m(TaskRunner pid=3162693)[0m             'model': {'enable_gradient_checkpointing': True,
[36m(TaskRunner pid=3162693)[0m                       'external_lib': None,
[36m(TaskRunner pid=3162693)[0m                       'fsdp_config': {'fsdp_size': -1,
[36m(TaskRunner pid=3162693)[0m                                       'optimizer_offload': False,
[36m(TaskRunner pid=3162693)[0m                                       'param_offload': False,
[36m(TaskRunner pid=3162693)[0m                                       'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=3162693)[0m                       'override_config': {},
[36m(TaskRunner pid=3162693)[0m                       'path': '~/models/deepseek-llm-7b-chat',
[36m(TaskRunner pid=3162693)[0m                       'tokenizer_path': '/home/wangyc/verl/Qwen/Qwen2.5-7B-Instruct',
[36m(TaskRunner pid=3162693)[0m                       'use_remove_padding': False},
[36m(TaskRunner pid=3162693)[0m             'optim': {'lr': 1e-05,
[36m(TaskRunner pid=3162693)[0m                       'lr_warmup_steps_ratio': 0.0,
[36m(TaskRunner pid=3162693)[0m                       'min_lr_ratio': None,
[36m(TaskRunner pid=3162693)[0m                       'total_training_steps': -1,
[36m(TaskRunner pid=3162693)[0m                       'warmup_style': 'constant',
[36m(TaskRunner pid=3162693)[0m                       'weight_decay': 0.01},
[36m(TaskRunner pid=3162693)[0m             'ppo_epochs': 1,
[36m(TaskRunner pid=3162693)[0m             'ppo_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=3162693)[0m             'ppo_micro_batch_size': None,
[36m(TaskRunner pid=3162693)[0m             'ppo_micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=3162693)[0m             'ppo_mini_batch_size': 32,
[36m(TaskRunner pid=3162693)[0m             'rollout_n': 6,
[36m(TaskRunner pid=3162693)[0m             'shuffle': False,
[36m(TaskRunner pid=3162693)[0m             'strategy': 'fsdp',
[36m(TaskRunner pid=3162693)[0m             'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=3162693)[0m             'use_dynamic_bsz': False},
[36m(TaskRunner pid=3162693)[0m  'custom_reward_function': {'name': 'compute_score', 'path': None},
[36m(TaskRunner pid=3162693)[0m  'data': {'custom_cls': {'name': None, 'path': None},
[36m(TaskRunner pid=3162693)[0m           'filter_overlong_prompts': False,
[36m(TaskRunner pid=3162693)[0m           'filter_overlong_prompts_workers': 1,
[36m(TaskRunner pid=3162693)[0m           'image_key': 'images',
[36m(TaskRunner pid=3162693)[0m           'max_prompt_length': 1024,
[36m(TaskRunner pid=3162693)[0m           'max_response_length': 1024,
[36m(TaskRunner pid=3162693)[0m           'prompt_key': 'prompt',
[36m(TaskRunner pid=3162693)[0m           'return_raw_chat': False,
[36m(TaskRunner pid=3162693)[0m           'return_raw_input_ids': False,
[36m(TaskRunner pid=3162693)[0m           'reward_fn_key': 'data_source',
[36m(TaskRunner pid=3162693)[0m           'shuffle': True,
[36m(TaskRunner pid=3162693)[0m           'tokenizer': None,
[36m(TaskRunner pid=3162693)[0m           'train_batch_size': 64,
[36m(TaskRunner pid=3162693)[0m           'train_files': '/home/wangyc/verl/data/jec-qa-1-multi-choice/train.parquet',
[36m(TaskRunner pid=3162693)[0m           'truncation': 'error',
[36m(TaskRunner pid=3162693)[0m           'val_batch_size': 1312,
[36m(TaskRunner pid=3162693)[0m           'val_files': '/home/wangyc/verl/data/jec-qa-1-multi-choice/test.parquet',
[36m(TaskRunner pid=3162693)[0m           'video_key': 'videos'},
[36m(TaskRunner pid=3162693)[0m  'ray_init': {'num_cpus': None},
[36m(TaskRunner pid=3162693)[0m  'reward_model': {'enable': False,
[36m(TaskRunner pid=3162693)[0m                   'forward_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=3162693)[0m                   'max_length': None,
[36m(TaskRunner pid=3162693)[0m                   'micro_batch_size': None,
[36m(TaskRunner pid=3162693)[0m                   'micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=3162693)[0m                   'model': {'external_lib': None,
[36m(TaskRunner pid=3162693)[0m                             'fsdp_config': {'fsdp_size': -1,
[36m(TaskRunner pid=3162693)[0m                                             'param_offload': False,
[36m(TaskRunner pid=3162693)[0m                                             'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=3162693)[0m                             'input_tokenizer': '/home/wangyc/verl/Qwen/Qwen2.5-7B-Instruct',
[36m(TaskRunner pid=3162693)[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',
[36m(TaskRunner pid=3162693)[0m                             'use_remove_padding': False},
[36m(TaskRunner pid=3162693)[0m                   'reward_manager': 'naive',
[36m(TaskRunner pid=3162693)[0m                   'strategy': 'fsdp',
[36m(TaskRunner pid=3162693)[0m                   'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=3162693)[0m                   'use_dynamic_bsz': False},
[36m(TaskRunner pid=3162693)[0m  'trainer': {'balance_batch': True,
[36m(TaskRunner pid=3162693)[0m              'critic_warmup': 0,
[36m(TaskRunner pid=3162693)[0m              'default_hdfs_dir': None,
[36m(TaskRunner pid=3162693)[0m              'default_local_dir': 'checkpoints/qwen2.5-7b-grpo-hard-mcq',
[36m(TaskRunner pid=3162693)[0m              'del_local_ckpt_after_load': False,
[36m(TaskRunner pid=3162693)[0m              'experiment_name': 'qwen2.5-7b-grpo-mcq',
[36m(TaskRunner pid=3162693)[0m              'log_val_generations': 0,
[36m(TaskRunner pid=3162693)[0m              'logger': ['console', 'wandb'],
[36m(TaskRunner pid=3162693)[0m              'max_actor_ckpt_to_keep': None,
[36m(TaskRunner pid=3162693)[0m              'max_critic_ckpt_to_keep': None,
[36m(TaskRunner pid=3162693)[0m              'n_gpus_per_node': 4,
[36m(TaskRunner pid=3162693)[0m              'nnodes': 1,
[36m(TaskRunner pid=3162693)[0m              'project_name': 'Lawyer-Zero',
[36m(TaskRunner pid=3162693)[0m              'ray_wait_register_center_timeout': 300,
[36m(TaskRunner pid=3162693)[0m              'resume_from_path': None,
[36m(TaskRunner pid=3162693)[0m              'resume_mode': 'auto',
[36m(TaskRunner pid=3162693)[0m              'save_freq': 50,
[36m(TaskRunner pid=3162693)[0m              'test_freq': 10,
[36m(TaskRunner pid=3162693)[0m              'total_epochs': 2,
[36m(TaskRunner pid=3162693)[0m              'total_training_steps': None,
[36m(TaskRunner pid=3162693)[0m              'val_before_train': True}}
[36m(TaskRunner pid=3162693)[0m WARNING: val_batch_size is deprecated. Validation datasets are sent to inference engines as a whole batch, which will schedule the memory themselves.
[36m(TaskRunner pid=3162693)[0m [validate_config] All configuration checks passed successfully!
[36m(TaskRunner pid=3162693)[0m dataset len: 8448
[36m(TaskRunner pid=3162693)[0m dataset len: 2113
[36m(TaskRunner pid=3162693)[0m Size of train dataloader: 132
[36m(TaskRunner pid=3162693)[0m Total training steps: 264
[36m(WorkerDict pid=3173524)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=3180577)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 64.86it/s]
[36m(WorkerDict pid=3180577)[0m [rank3]:[W509 13:05:36.596053627 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[36m(WorkerDict pid=3173524)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=3180574)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3173524)[0m Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][32m [repeated 4x across cluster][0m
[36m(WorkerDict pid=3180574)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 49.44it/s][32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3180574)[0m [rank1]:[W509 13:05:37.761869944 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3180577)[0m Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:03<00:10,  3.45s/it]
[36m(WorkerDict pid=3180576)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3180574)[0m Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3180577)[0m Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:09<00:03,  3.26s/it][32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=3180577)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:12<00:00,  3.13s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:12<00:00,  3.20s/it]
[36m(WorkerDict pid=3173524)[0m /home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=3173524)[0m   warnings.warn(
[36m(WorkerDict pid=3180574)[0m Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:10<00:03,  3.37s/it][32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3180574)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:13<00:00,  3.21s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:13<00:00,  3.28s/it][32m [repeated 3x across cluster][0m
[36m(TaskRunner pid=3162693)[0m wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
[36m(TaskRunner pid=3162693)[0m wandb: Currently logged in as: eang333cms (eang333cms-peking-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(TaskRunner pid=3162693)[0m wandb: Tracking run with wandb version 0.19.9
[36m(TaskRunner pid=3162693)[0m wandb: Run data is saved locally in /home/wangyc/verl/wandb/run-20250509_130622-h8dufmm1
[36m(TaskRunner pid=3162693)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(TaskRunner pid=3162693)[0m wandb: Syncing run qwen2.5-7b-grpo-mcq
[36m(TaskRunner pid=3162693)[0m wandb: â­ï¸ View project at https://wandb.ai/eang333cms-peking-university/Lawyer-Zero
[36m(TaskRunner pid=3162693)[0m wandb: ðŸš€ View run at https://wandb.ai/eang333cms-peking-university/Lawyer-Zero/runs/h8dufmm1
[36m(WorkerDict pid=3173524)[0m   "architectures": [
[36m(WorkerDict pid=3173524)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=3173524)[0m   ],
[36m(WorkerDict pid=3173524)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=3173524)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=3173524)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=3173524)[0m   "hidden_size": 3584,
[36m(WorkerDict pid=3173524)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=3173524)[0m   "intermediate_size": 18944,
[36m(WorkerDict pid=3173524)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=3173524)[0m   "max_window_layers": 28,
[36m(WorkerDict pid=3173524)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=3173524)[0m   "num_attention_heads": 28,
[36m(WorkerDict pid=3173524)[0m   "num_hidden_layers": 28,
[36m(WorkerDict pid=3173524)[0m   "num_key_value_heads": 4,
[36m(WorkerDict pid=3173524)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=3173524)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=3173524)[0m   "rope_scaling": null,
[36m(WorkerDict pid=3173524)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=3173524)[0m   "sliding_window": 131072,
[36m(WorkerDict pid=3173524)[0m   "tie_word_embeddings": false,
[36m(WorkerDict pid=3173524)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=3173524)[0m   "transformers_version": "4.51.3",
[36m(WorkerDict pid=3173524)[0m   "use_cache": true,
[36m(WorkerDict pid=3173524)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=3173524)[0m   "vocab_size": 152064
[36m(WorkerDict pid=3173524)[0m }
[36m(WorkerDict pid=3173524)[0m 
[36m(WorkerDict pid=3180576)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
[36m(WorkerDict pid=3173524)[0m NCCL version 2.21.5+cuda12.4
[36m(WorkerDict pid=3173524)[0m Qwen2ForCausalLM contains 7.62B parameters
[36m(WorkerDict pid=3173524)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f994047e830>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f994047e710>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=3173524)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3180574)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(WorkerDict pid=3180577)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f66fde32830>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f66fde32710>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3173524)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=3173524)[0m   "architectures": [
[36m(WorkerDict pid=3173524)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=3173524)[0m   ],
[36m(WorkerDict pid=3173524)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=3173524)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=3173524)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=3173524)[0m   "hidden_size": 3584,
[36m(WorkerDict pid=3173524)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=3173524)[0m   "intermediate_size": 18944,
[36m(WorkerDict pid=3173524)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=3173524)[0m   "max_window_layers": 28,
[36m(WorkerDict pid=3173524)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=3173524)[0m   "num_attention_heads": 28,
[36m(WorkerDict pid=3173524)[0m   "num_hidden_layers": 28,
[36m(WorkerDict pid=3173524)[0m   "num_key_value_heads": 4,
[36m(WorkerDict pid=3173524)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=3173524)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=3173524)[0m   "rope_scaling": null,
[36m(WorkerDict pid=3173524)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=3173524)[0m   "sliding_window": 131072,
[36m(WorkerDict pid=3173524)[0m   "tie_word_embeddings": false,
[36m(WorkerDict pid=3173524)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=3173524)[0m   "transformers_version": "4.51.3",
[36m(WorkerDict pid=3173524)[0m   "use_cache": true,
[36m(WorkerDict pid=3173524)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=3173524)[0m   "vocab_size": 152064
[36m(WorkerDict pid=3173524)[0m }
[36m(WorkerDict pid=3173524)[0m 
[36m(WorkerDict pid=3180577)[0m Actor use_remove_padding=True[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3180576)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
[36m(WorkerDict pid=3180577)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
[36m(WorkerDict pid=3173524)[0m Qwen2ForCausalLM contains 7.62B parameters
[36m(WorkerDict pid=3173524)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f994047e830>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f994047e710>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=3180574)[0m Total steps: 264, num_warmup_steps: 0
[36m(WorkerDict pid=3180574)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3180574)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention[32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=3180577)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f66fde32830>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f66fde32710>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3180576)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=3173524)[0m Before building vllm rollout, memory allocated (GB): 7.093081951141357, memory reserved (GB): 22.55078125
[36m(WorkerDict pid=3180577)[0m WARNING 05-09 13:06:09 [cuda.py:96] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=3173524)[0m Total steps: 264, num_warmup_steps: 0[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3173524)[0m Actor use_remove_padding=True[32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=3180577)[0m WARNING 05-09 13:06:09 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f661479abc0>
[36m(WorkerDict pid=3180576)[0m NCCL version 2.21.5+cuda12.4
[36m(WorkerDict pid=3173524)[0m kwargs: {'n': 6, 'logprobs': 0, 'max_tokens': 1024, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
[36m(WorkerDict pid=3173524)[0m After building vllm rollout, memory allocated (GB): 18.051849365234375, memory reserved (GB): 20.41796875
[36m(WorkerDict pid=3173524)[0m After building sharding manager, memory allocated (GB): 18.051849365234375, memory reserved (GB): 20.41796875
[36m(WorkerDict pid=3173524)[0m WARNING 05-09 13:06:09 [cuda.py:96] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3173524)[0m WARNING 05-09 13:06:10 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f984c4cec80>[32m [repeated 3x across cluster][0m
[36m(TaskRunner pid=3162693)[0m Using LocalLogger is deprecated. The constructor API will change 
[36m(TaskRunner pid=3162693)[0m Checkpoint tracker file does not exist: %s /home/wangyc/verl/checkpoints/qwen2.5-7b-grpo-hard-mcq/latest_checkpointed_iteration.txt
[36m(TaskRunner pid=3162693)[0m Training Progress:   0%|          | 0/264 [00:00<?, ?it/s]
[36m(WorkerDict pid=3180576)[0m /home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=3180576)[0m   warnings.warn([32m [repeated 3x across cluster][0m
[36m(TaskRunner pid=3162693)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_generate_sequences()[39m (pid=3180576, ip=222.29.51.203, actor_id=b0d415167f8d9951e954108f01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7fa8b4dbd630>)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/verl/verl/single_controller/ray/base.py", line 440, in func
[36m(TaskRunner pid=3162693)[0m     return getattr(self.worker_dict[key], name)(*args, **kwargs)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/verl/verl/single_controller/base/decorator.py", line 413, in inner
[36m(TaskRunner pid=3162693)[0m     return func(*args, **kwargs)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/verl/verl/workers/fsdp_workers.py", line 560, in generate_sequences
[36m(TaskRunner pid=3162693)[0m     with self.rollout_sharding_manager:
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/verl/verl/utils/debug/performance.py", line 61, in f
[36m(TaskRunner pid=3162693)[0m     return self.log(decorated_function, *args, **kwargs)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/verl/verl/utils/debug/performance.py", line 70, in log
[36m(TaskRunner pid=3162693)[0m     output = func(*args, **kwargs)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/verl/verl/workers/sharding_manager/fsdp_vllm.py", line 90, in __enter__
[36m(TaskRunner pid=3162693)[0m     params = self.module.state_dict()
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2216, in state_dict
[36m(TaskRunner pid=3162693)[0m     module.state_dict(
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2216, in state_dict
[36m(TaskRunner pid=3162693)[0m     module.state_dict(
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2216, in state_dict
[36m(TaskRunner pid=3162693)[0m     module.state_dict(
[36m(TaskRunner pid=3162693)[0m   [Previous line repeated 1 more time]
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2222, in state_dict
[36m(TaskRunner pid=3162693)[0m     hook_result = hook(self, destination, prefix, local_metadata)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[36m(TaskRunner pid=3162693)[0m     return func(*args, **kwargs)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py", line 724, in _post_state_dict_hook
[36m(TaskRunner pid=3162693)[0m     processed_state_dict = _post_state_dict_hook_fn[fsdp_state._state_dict_type](
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py", line 569, in _sharded_post_state_dict_hook
[36m(TaskRunner pid=3162693)[0m     return _common_unshard_post_state_dict_hook(
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py", line 238, in _common_unshard_post_state_dict_hook
[36m(TaskRunner pid=3162693)[0m     param_hook(state_dict, prefix, fqn)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py", line 559, in param_hook
[36m(TaskRunner pid=3162693)[0m     sharded_tensor = _ext_chunk_dtensor(
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_fsdp_extensions.py", line 150, in _ext_chunk_dtensor
[36m(TaskRunner pid=3162693)[0m     return chunk_dtensor_fn(
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_shard_utils.py", line 110, in _create_chunk_dtensor
[36m(TaskRunner pid=3162693)[0m     return DTensor.from_local(
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/tensor/_api.py", line 544, in redistribute
[36m(TaskRunner pid=3162693)[0m     return Redistribute.apply(self, device_mesh, placements, async_op)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[36m(TaskRunner pid=3162693)[0m     return super().apply(*args, **kwargs)  # type: ignore[misc]
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/tensor/_redistribute.py", line 306, in forward
[36m(TaskRunner pid=3162693)[0m     output = redistribute_local_tensor(
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/tensor/_redistribute.py", line 230, in redistribute_local_tensor
[36m(TaskRunner pid=3162693)[0m     new_local_tensor = target_placement._replicate_to_shard(
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/tensor/placement_types.py", line 272, in _replicate_to_shard
[36m(TaskRunner pid=3162693)[0m     return shards[shard_index].clone()
[36m(TaskRunner pid=3162693)[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacity of 44.53 GiB of which 31.50 MiB is free. Including non-PyTorch memory, this process has 15.37 GiB memory in use. Process 3257307 has 29.09 GiB memory in use. Of the allocated memory 25.44 GiB is allocated by PyTorch, with 158.97 MiB allocated in private pools (e.g., CUDA Graphs), and 145.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(TaskRunner pid=3162693)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_generate_sequences()[39m (pid=3180574, ip=222.29.51.203, actor_id=94ceb6638e1b841e2c8a8ce801000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7fc3ec1a9d50>)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/verl/verl/single_controller/ray/base.py", line 440, in func
[36m(TaskRunner pid=3162693)[0m     return getattr(self.worker_dict[key], name)(*args, **kwargs)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/verl/verl/single_controller/base/decorator.py", line 413, in inner
[36m(TaskRunner pid=3162693)[0m     return func(*args, **kwargs)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/verl/verl/workers/fsdp_workers.py", line 560, in generate_sequences
[36m(TaskRunner pid=3162693)[0m     with self.rollout_sharding_manager:
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/verl/verl/utils/debug/performance.py", line 61, in f
[36m(TaskRunner pid=3162693)[0m     return self.log(decorated_function, *args, **kwargs)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/verl/verl/utils/debug/performance.py", line 70, in log
[36m(TaskRunner pid=3162693)[0m     output = func(*args, **kwargs)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/verl/verl/workers/sharding_manager/fsdp_vllm.py", line 90, in __enter__
[36m(TaskRunner pid=3162693)[0m     params = self.module.state_dict()
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2216, in state_dict
[36m(TaskRunner pid=3162693)[0m     module.state_dict(
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2216, in state_dict
[36m(TaskRunner pid=3162693)[0m     module.state_dict(
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2216, in state_dict
[36m(TaskRunner pid=3162693)[0m     module.state_dict(
[36m(TaskRunner pid=3162693)[0m   [Previous line repeated 1 more time]
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2222, in state_dict
[36m(TaskRunner pid=3162693)[0m     hook_result = hook(self, destination, prefix, local_metadata)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[36m(TaskRunner pid=3162693)[0m     return func(*args, **kwargs)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py", line 724, in _post_state_dict_hook
[36m(TaskRunner pid=3162693)[0m     processed_state_dict = _post_state_dict_hook_fn[fsdp_state._state_dict_type](
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py", line 569, in _sharded_post_state_dict_hook
[36m(TaskRunner pid=3162693)[0m     return _common_unshard_post_state_dict_hook(
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py", line 238, in _common_unshard_post_state_dict_hook
[36m(TaskRunner pid=3162693)[0m     param_hook(state_dict, prefix, fqn)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py", line 559, in param_hook
[36m(TaskRunner pid=3162693)[0m     sharded_tensor = _ext_chunk_dtensor(
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_fsdp_extensions.py", line 150, in _ext_chunk_dtensor
[36m(TaskRunner pid=3162693)[0m     return chunk_dtensor_fn(
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_shard_utils.py", line 110, in _create_chunk_dtensor
[36m(TaskRunner pid=3162693)[0m     return DTensor.from_local(
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/tensor/_api.py", line 544, in redistribute
[36m(TaskRunner pid=3162693)[0m     return Redistribute.apply(self, device_mesh, placements, async_op)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[36m(TaskRunner pid=3162693)[0m     return super().apply(*args, **kwargs)  # type: ignore[misc]
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/tensor/_redistribute.py", line 306, in forward
[36m(TaskRunner pid=3162693)[0m     output = redistribute_local_tensor(
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/tensor/_redistribute.py", line 230, in redistribute_local_tensor
[36m(TaskRunner pid=3162693)[0m     new_local_tensor = target_placement._replicate_to_shard(
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/tensor/placement_types.py", line 272, in _replicate_to_shard
[36m(TaskRunner pid=3162693)[0m     return shards[shard_index].clone()
[36m(TaskRunner pid=3162693)[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacity of 44.53 GiB of which 31.50 MiB is free. Including non-PyTorch memory, this process has 15.37 GiB memory in use. Process 3257300 has 29.09 GiB memory in use. Of the allocated memory 25.44 GiB is allocated by PyTorch, with 158.97 MiB allocated in private pools (e.g., CUDA Graphs), and 145.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(TaskRunner pid=3162693)[0m /home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:387: UserWarning: Run (h8dufmm1) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.
[36m(TaskRunner pid=3162693)[0m   return func(self, *args, **kwargs)
[36m(TaskRunner pid=3162693)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_generate_sequences()[39m (pid=3180577, ip=222.29.51.203, actor_id=363a1e272010d512e113e16601000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f6697c1a260>)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/verl/verl/single_controller/ray/base.py", line 440, in func
[36m(TaskRunner pid=3162693)[0m     return getattr(self.worker_dict[key], name)(*args, **kwargs)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/verl/verl/single_controller/base/decorator.py", line 413, in inner
[36m(TaskRunner pid=3162693)[0m     return func(*args, **kwargs)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/verl/verl/workers/fsdp_workers.py", line 560, in generate_sequences
[36m(TaskRunner pid=3162693)[0m     with self.rollout_sharding_manager:
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/verl/verl/utils/debug/performance.py", line 61, in f
[36m(TaskRunner pid=3162693)[0m     return self.log(decorated_function, *args, **kwargs)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/verl/verl/utils/debug/performance.py", line 70, in log
[36m(TaskRunner pid=3162693)[0m     output = func(*args, **kwargs)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/verl/verl/workers/sharding_manager/fsdp_vllm.py", line 90, in __enter__
[36m(TaskRunner pid=3162693)[0m     params = self.module.state_dict()
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2216, in state_dict
[36m(TaskRunner pid=3162693)[0m     module.state_dict(
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2216, in state_dict
[36m(TaskRunner pid=3162693)[0m     module.state_dict(
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2216, in state_dict
[36m(TaskRunner pid=3162693)[0m     module.state_dict(
[36m(TaskRunner pid=3162693)[0m   [Previous line repeated 1 more time]
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2222, in state_dict
[36m(TaskRunner pid=3162693)[0m     hook_result = hook(self, destination, prefix, local_metadata)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[36m(TaskRunner pid=3162693)[0m     return func(*args, **kwargs)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py", line 724, in _post_state_dict_hook
[36m(TaskRunner pid=3162693)[0m     processed_state_dict = _post_state_dict_hook_fn[fsdp_state._state_dict_type](
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py", line 569, in _sharded_post_state_dict_hook
[36m(TaskRunner pid=3162693)[0m     return _common_unshard_post_state_dict_hook(
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py", line 238, in _common_unshard_post_state_dict_hook
[36m(TaskRunner pid=3162693)[0m     param_hook(state_dict, prefix, fqn)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py", line 559, in param_hook
[36m(TaskRunner pid=3162693)[0m     sharded_tensor = _ext_chunk_dtensor(
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_fsdp_extensions.py", line 150, in _ext_chunk_dtensor
[36m(TaskRunner pid=3162693)[0m     return chunk_dtensor_fn(
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_shard_utils.py", line 110, in _create_chunk_dtensor
[36m(TaskRunner pid=3162693)[0m     return DTensor.from_local(
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/tensor/_api.py", line 544, in redistribute
[36m(TaskRunner pid=3162693)[0m     return Redistribute.apply(self, device_mesh, placements, async_op)
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
[36m(TaskRunner pid=3162693)[0m     return super().apply(*args, **kwargs)  # type: ignore[misc]
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/tensor/_redistribute.py", line 306, in forward
[36m(TaskRunner pid=3162693)[0m     output = redistribute_local_tensor(
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/tensor/_redistribute.py", line 230, in redistribute_local_tensor
[36m(TaskRunner pid=3162693)[0m     new_local_tensor = target_placement._replicate_to_shard(
[36m(TaskRunner pid=3162693)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/tensor/placement_types.py", line 272, in _replicate_to_shard
[36m(TaskRunner pid=3162693)[0m     return shards[shard_index].clone()
[36m(TaskRunner pid=3162693)[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacity of 44.53 GiB of which 25.50 MiB is free. Including non-PyTorch memory, this process has 15.58 GiB memory in use. Process 3257314 has 28.88 GiB memory in use. Of the allocated memory 25.72 GiB is allocated by PyTorch, with 158.97 MiB allocated in private pools (e.g., CUDA Graphs), and 121.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(TaskRunner pid=3162693)[0m wandb: uploading config.yaml
[36m(TaskRunner pid=3162693)[0m wandb:                                                                                
[36m(TaskRunner pid=3162693)[0m wandb: 
[36m(TaskRunner pid=3162693)[0m wandb: Run history:
[36m(TaskRunner pid=3162693)[0m wandb: val-aux/jec-qa-1-multi-choice/extra_info/score/std@1 â–
[36m(TaskRunner pid=3162693)[0m wandb:     val-aux/jec-qa-1-multi-choice/extra_info/score@1 â–
[36m(TaskRunner pid=3162693)[0m wandb:           val-aux/jec-qa-1-multi-choice/reward/std@1 â–
[36m(TaskRunner pid=3162693)[0m wandb:    val-aux/jec-qa-1-multi-choice/reward/worst@1/mean â–
[36m(TaskRunner pid=3162693)[0m wandb:     val-aux/jec-qa-1-multi-choice/reward/worst@1/std â–
[36m(TaskRunner pid=3162693)[0m wandb:      val-aux/jec-qa-1-multi-choice/score/best@1/mean â–
[36m(TaskRunner pid=3162693)[0m wandb:       val-aux/jec-qa-1-multi-choice/score/best@1/std â–
[36m(TaskRunner pid=3162693)[0m wandb:           val-aux/jec-qa-1-multi-choice/score/mean@1 â–
[36m(TaskRunner pid=3162693)[0m wandb:            val-aux/jec-qa-1-multi-choice/score/std@1 â–
[36m(TaskRunner pid=3162693)[0m wandb:     val-aux/jec-qa-1-multi-choice/score/worst@1/mean â–
[36m(TaskRunner pid=3162693)[0m wandb:      val-aux/jec-qa-1-multi-choice/score/worst@1/std â–
[36m(TaskRunner pid=3162693)[0m wandb:    val-core/jec-qa-1-multi-choice/reward/best@1/mean â–
[36m(TaskRunner pid=3162693)[0m wandb:     val-core/jec-qa-1-multi-choice/reward/best@1/std â–
[36m(TaskRunner pid=3162693)[0m wandb:         val-core/jec-qa-1-multi-choice/reward/mean@1 â–
[36m(TaskRunner pid=3162693)[0m wandb: 
[36m(TaskRunner pid=3162693)[0m wandb: Run summary:
[36m(TaskRunner pid=3162693)[0m wandb: val-aux/jec-qa-1-multi-choice/extra_info/score/std@1 0
[36m(TaskRunner pid=3162693)[0m wandb:     val-aux/jec-qa-1-multi-choice/extra_info/score@1 0
[36m(TaskRunner pid=3162693)[0m wandb:           val-aux/jec-qa-1-multi-choice/reward/std@1 0
[36m(TaskRunner pid=3162693)[0m wandb:    val-aux/jec-qa-1-multi-choice/reward/worst@1/mean 1.00781
[36m(TaskRunner pid=3162693)[0m wandb:     val-aux/jec-qa-1-multi-choice/reward/worst@1/std 0
[36m(TaskRunner pid=3162693)[0m wandb:      val-aux/jec-qa-1-multi-choice/score/best@1/mean 1.00781
[36m(TaskRunner pid=3162693)[0m wandb:       val-aux/jec-qa-1-multi-choice/score/best@1/std 0
[36m(TaskRunner pid=3162693)[0m wandb:           val-aux/jec-qa-1-multi-choice/score/mean@1 1.00781
[36m(TaskRunner pid=3162693)[0m wandb:            val-aux/jec-qa-1-multi-choice/score/std@1 0
[36m(TaskRunner pid=3162693)[0m wandb:     val-aux/jec-qa-1-multi-choice/score/worst@1/mean 1.00781
[36m(TaskRunner pid=3162693)[0m wandb:      val-aux/jec-qa-1-multi-choice/score/worst@1/std 0
[36m(TaskRunner pid=3162693)[0m wandb:    val-core/jec-qa-1-multi-choice/reward/best@1/mean 1.00781
[36m(TaskRunner pid=3162693)[0m wandb:     val-core/jec-qa-1-multi-choice/reward/best@1/std 0
[36m(TaskRunner pid=3162693)[0m wandb:         val-core/jec-qa-1-multi-choice/reward/mean@1 1.00781
[36m(TaskRunner pid=3162693)[0m wandb: 
[36m(TaskRunner pid=3162693)[0m wandb: ðŸš€ View run qwen2.5-7b-grpo-mcq at: https://wandb.ai/eang333cms-peking-university/Lawyer-Zero/runs/h8dufmm1
[36m(TaskRunner pid=3162693)[0m wandb: â­ï¸ View project at: https://wandb.ai/eang333cms-peking-university/Lawyer-Zero
[36m(TaskRunner pid=3162693)[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[36m(TaskRunner pid=3162693)[0m wandb: Find logs at: ./wandb/run-20250509_130622-h8dufmm1/logs
[36m(TaskRunner pid=3162693)[0m Training Progress:   0%|          | 0/264 [00:02<?, ?it/s]
[36m(TaskRunner pid=3162693)[0m Training from scratch
[36m(TaskRunner pid=3162693)[0m test_gen_batch meta info: {'eos_token_id': 151645, 'pad_token_id': 151643, 'recompute_log_prob': False, 'do_sample': False, 'validate': True}
[36m(WorkerDict pid=3180577)[0m kwargs: {'n': 6, 'logprobs': 0, 'max_tokens': 1024, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}[32m [repeated 3x across cluster][0m
[36m(TaskRunner pid=3162693)[0m validation generation end
[36m(TaskRunner pid=3162693)[0m [prompt] system
[36m(TaskRunner pid=3162693)[0m ç”¨æˆ·å’ŒåŠ©æ‰‹ä¹‹é—´çš„å¯¹è¯ã€‚ç”¨æˆ·æå‡ºä¸€ä¸ªé—®é¢˜ï¼Œç”±åŠ©æ‰‹æ¥å›žç­”ã€‚åŠ©æ‰‹é¦–å…ˆåœ¨è„‘æµ·ä¸­é€æ­¥æ€è€ƒæŽ¨ç†è¿‡ç¨‹ï¼Œç„¶åŽå‘ç”¨æˆ·æä¾›ç­”æ¡ˆã€‚æŽ¨ç†è¿‡ç¨‹å’Œç­”æ¡ˆåˆ†åˆ«ç”¨<æ€è€ƒ> </æ€è€ƒ>å’Œ<å›žç­”> </å›žç­”>æ ‡ç­¾æ‹¬èµ·æ¥ï¼Œå³ï¼Œ<æ€è€ƒ> æŽ¨ç†è¿‡ç¨‹ </æ€è€ƒ><å›žç­”> ç­”æ¡ˆ </å›žç­”>
[36m(TaskRunner pid=3162693)[0m user
[36m(TaskRunner pid=3162693)[0m ä½ æ˜¯ä¸€åæ³•å­¦ä¸“å®¶ã€‚çŽ°åœ¨è¯·ä½ è§£ç­”å¸æ³•è€ƒè¯•ä¸­çš„ä¸€é“é€‰æ‹©é¢˜ï¼Œè¯·ä½ æ‰¾å‡ºæ‰€æœ‰æ­£ç¡®çš„é€‰é¡¹ã€‚æ¯é“é¢˜å¯èƒ½æœ‰ä¸€ä¸ªæˆ–è€…å¤šä¸ªæ­£ç¡®ç­”æ¡ˆã€‚åœ¨è§£ç­”ä¹‹å‰ï¼Œä½ éœ€è¦å…ˆé’ˆå¯¹æ¯ä¸ªæä¾›çš„é€‰é¡¹ç»™å‡ºè¯¦ç»†çš„è§£é‡Šã€‚ä½ éœ€è¦åœ¨å›žç­”çš„æœ€åŽç”¨å¤§æ‹¬å·åœˆå‡ºç»™å‡ºçš„ç­”æ¡ˆï¼Œä¾‹å¦‚"{B}"æˆ–è€…"{ABD}"ã€‚
[36m(TaskRunner pid=3162693)[0m 
[36m(TaskRunner pid=3162693)[0m é—®é¢˜ï¼šç”²æŸä»¥ä¸ºå¢ƒå¤–çªƒå–ã€åˆºæŽ¢ã€æ”¶ä¹°å›½å®¶ç§˜å¯†çš„æ•…æ„ï¼Œä¹™ä»¥éžæ³•èŽ·å–å›½å®¶ç§˜å¯†çš„æ•…æ„ï¼Œå…±è°‹å…±åŒçªƒå–ã€åˆºæŽ¢ã€æ”¶ä¹°å›½å®¶ç§˜å¯†ï¼Œå¯¹äºŽç”²ä¹™äºŒäººçš„è¡Œä¸ºçš„è®¤å®šï¼Œä¸‹åˆ—æ­£ç¡®çš„é€‰é¡¹æ˜¯:
[36m(TaskRunner pid=3162693)[0m 
[36m(TaskRunner pid=3162693)[0m é€‰é¡¹ï¼š
[36m(TaskRunner pid=3162693)[0m A: ç”²ä¸Žä¹™å› æ•…æ„ä¸åŒï¼Œä¸æˆç«‹å…±åŒçŠ¯ç½ª
[36m(TaskRunner pid=3162693)[0m B: ç”²ä¸Žä¹™è™½æ•…æ„ä¸åŒï¼Œä»åœ¨éžæ³•èŽ·å–å›½å®¶ç§˜å¯†ç½ªçš„èŒƒå›´å†…æˆç«‹å…±åŒçŠ¯ç½ª
[36m(TaskRunner pid=3162693)[0m C: å¯¹ç”²åº”ä»¥ä¸ºå¢ƒå¤–çªƒå–ã€åˆºæŽ¢ã€æ”¶ä¹°å›½å®¶ç§˜å¯†ç½ªè®ºå¤„
[36m(TaskRunner pid=3162693)[0m D: å¯¹ä¹™ä»¥éžæ³•èŽ·å–å›½å®¶ç§˜å¯†ç½ªè®ºå¤„
[36m(TaskRunner pid=3162693)[0m assistant
[36m(TaskRunner pid=3162693)[0m 
[36m(TaskRunner pid=3162693)[0m [response] <æ€è€ƒ> æœ¬é¢˜è€ƒæŸ¥å…±åŒçŠ¯ç½ªçš„è®¤å®šã€‚æ ¹æ®æˆ‘å›½åˆ‘æ³•ï¼Œå…±åŒçŠ¯ç½ªæ˜¯æŒ‡äºŒäººä»¥ä¸Šå…±åŒæ•…æ„çŠ¯ç½ªã€‚å…±åŒçŠ¯ç½ªçš„æˆç«‹éœ€è¦æ»¡è¶³å…±åŒçš„çŠ¯ç½ªæ•…æ„å’Œå…±åŒçš„çŠ¯ç½ªè¡Œä¸ºã€‚ç”²æŸä»¥ä¸ºå¢ƒå¤–çªƒå–ã€åˆºæŽ¢ã€æ”¶ä¹°å›½å®¶ç§˜å¯†çš„æ•…æ„ï¼Œä¹™ä»¥éžæ³•èŽ·å–å›½å®¶ç§˜å¯†çš„æ•…æ„ï¼Œè™½ç„¶ä¸¤äººçš„æ•…æ„ä¸åŒï¼Œä½†ä»–ä»¬çš„è¡Œä¸ºéƒ½å±žäºŽéžæ³•èŽ·å–å›½å®¶ç§˜å¯†çš„è¡Œä¸ºï¼Œå› æ­¤åœ¨éžæ³•èŽ·å–å›½å®¶ç§˜å¯†ç½ªçš„èŒƒå›´å†…æˆç«‹å…±åŒçŠ¯ç½ªã€‚ç”²æŸçš„è¡Œä¸ºè¿˜å¯èƒ½æž„æˆæ›´ä¸ºä¸¥é‡çš„ç½ªåï¼Œå³ä»¥ä¸ºå¢ƒå¤–çªƒå–ã€åˆºæŽ¢ã€æ”¶ä¹°ã€éžæ³•æä¾›å›½å®¶ç§˜å¯†ç½ªã€‚å› æ­¤ï¼Œé€‰é¡¹Bæ­£ç¡®ï¼Œé€‰é¡¹Aé”™è¯¯ã€‚å¯¹äºŽç”²ä¹™äºŒäººçš„è¡Œä¸ºï¼Œåº”åˆ†åˆ«å®šç½ªï¼Œç”²æŸåº”ä»¥ä¸ºå¢ƒå¤–çªƒå–ã€åˆºæŽ¢ã€æ”¶ä¹°å›½å®¶ç§˜å¯†ç½ªè®ºå¤„ï¼Œä¹™ä»¥éžæ³•èŽ·å–å›½å®¶ç§˜å¯†ç½ªè®ºå¤„ï¼Œå› æ­¤é€‰é¡¹Cå’ŒDæ­£ç¡®ã€‚
[36m(TaskRunner pid=3162693)[0m </æ€è€ƒ><å›žç­”> é€‰é¡¹Bã€Cã€Dæ­£ç¡®ã€‚ 
[36m(TaskRunner pid=3162693)[0m 
[36m(TaskRunner pid=3162693)[0m é€‰é¡¹Aé”™è¯¯ï¼Œå› ä¸ºç”²ä¹™è™½ç„¶æ•…æ„ä¸åŒï¼Œä½†ä»–ä»¬çš„è¡Œä¸ºéƒ½å±žäºŽéžæ³•èŽ·å–å›½å®¶ç§˜å¯†çš„è¡Œä¸ºï¼Œå› æ­¤åœ¨éžæ³•èŽ·å–å›½å®¶ç§˜å¯†ç½ªçš„èŒƒå›´å†…æˆç«‹å…±åŒçŠ¯ç½ªã€‚ 
[36m(TaskRunner pid=3162693)[0m 
[36m(TaskRunner pid=3162693)[0m é€‰é¡¹Bæ­£ç¡®ï¼Œå› ä¸ºç”²ä¹™è™½ç„¶æ•…æ„ä¸åŒï¼Œä½†ä»–ä»¬çš„è¡Œä¸ºéƒ½å±žäºŽéžæ³•èŽ·å–å›½å®¶ç§˜å¯†çš„è¡Œä¸ºï¼Œå› æ­¤åœ¨éžæ³•èŽ·å–å›½å®¶ç§˜å¯†ç½ªçš„èŒƒå›´å†…æˆç«‹å…±åŒçŠ¯ç½ªã€‚ 
[36m(TaskRunner pid=3162693)[0m 
[36m(TaskRunner pid=3162693)[0m é€‰é¡¹Cæ­£ç¡®ï¼Œå› ä¸ºç”²æŸä»¥ä¸ºå¢ƒå¤–çªƒå–ã€åˆºæŽ¢ã€æ”¶ä¹°å›½å®¶ç§˜å¯†çš„æ•…æ„ï¼Œå…¶è¡Œä¸ºè¿˜å¯èƒ½æž„æˆæ›´ä¸ºä¸¥é‡çš„ç½ªåï¼Œå³ä»¥ä¸ºå¢ƒå¤–çªƒå–ã€åˆºæŽ¢ã€æ”¶ä¹°ã€éžæ³•æä¾›å›½å®¶ç§˜å¯†ç½ªã€‚ 
[36m(TaskRunner pid=3162693)[0m 
[36m(TaskRunner pid=3162693)[0m é€‰é¡¹Dæ­£ç¡®ï¼Œå› ä¸ºä¹™ä»¥éžæ³•èŽ·å–å›½å®¶ç§˜å¯†çš„æ•…æ„ï¼Œå…¶è¡Œä¸ºåº”ä»¥éžæ³•èŽ·å–å›½å®¶ç§˜å¯†ç½ªè®ºå¤„ã€‚ 
[36m(TaskRunner pid=3162693)[0m 
[36m(TaskRunner pid=3162693)[0m å› æ­¤ï¼Œæ­£ç¡®ç­”æ¡ˆä¸º{BCD}ã€‚
[36m(TaskRunner pid=3162693)[0m [ground_truth] ['B', 'C', 'D']
[36m(TaskRunner pid=3162693)[0m [score] 1.0
[36m(TaskRunner pid=3162693)[0m [extra_info] {'format_reward': 0.0, 'answer_reward': 1.0, 'acc': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0}
[36m(TaskRunner pid=3162693)[0m ("Initial validation metrics: {'val-core/jec-qa-1-multi-choice/reward/mean@1': "
[36m(TaskRunner pid=3162693)[0m  'np.float64(1.0078088026502603), '
[36m(TaskRunner pid=3162693)[0m  "'val-aux/jec-qa-1-multi-choice/reward/std@1': np.float64(0.0), "
[36m(TaskRunner pid=3162693)[0m  "'val-core/jec-qa-1-multi-choice/reward/best@1/mean': "
[36m(TaskRunner pid=3162693)[0m  'np.float64(1.0078088026502603), '
[36m(TaskRunner pid=3162693)[0m  "'val-core/jec-qa-1-multi-choice/reward/best@1/std': np.float64(0.0), "
[36m(TaskRunner pid=3162693)[0m  "'val-aux/jec-qa-1-multi-choice/reward/worst@1/mean': "
[36m(TaskRunner pid=3162693)[0m  'np.float64(1.0078088026502603), '
[36m(TaskRunner pid=3162693)[0m  "'val-aux/jec-qa-1-multi-choice/reward/worst@1/std': np.float64(0.0), "
[36m(TaskRunner pid=3162693)[0m  "'val-aux/jec-qa-1-multi-choice/score/mean@1': "
[36m(TaskRunner pid=3162693)[0m  "np.float64(1.0078088026502603), 'val-aux/jec-qa-1-multi-choice/score/std@1': "
[36m(TaskRunner pid=3162693)[0m  "np.float64(0.0), 'val-aux/jec-qa-1-multi-choice/score/best@1/mean': "
[36m(TaskRunner pid=3162693)[0m  'np.float64(1.0078088026502603), '
[36m(TaskRunner pid=3162693)[0m  "'val-aux/jec-qa-1-multi-choice/score/best@1/std': np.float64(0.0), "
[36m(TaskRunner pid=3162693)[0m  "'val-aux/jec-qa-1-multi-choice/score/worst@1/mean': "
[36m(TaskRunner pid=3162693)[0m  'np.float64(1.0078088026502603), '
[36m(TaskRunner pid=3162693)[0m  "'val-aux/jec-qa-1-multi-choice/score/worst@1/std': np.float64(0.0), "
[36m(TaskRunner pid=3162693)[0m  "'val-aux/jec-qa-1-multi-choice/extra_info/score@1': np.float64(0.0), "
[36m(TaskRunner pid=3162693)[0m  "'val-aux/jec-qa-1-multi-choice/extra_info/score/std@1': np.float64(0.0)}")
[36m(TaskRunner pid=3162693)[0m step:0 - val-core/jec-qa-1-multi-choice/reward/mean@1:1.008 - val-aux/jec-qa-1-multi-choice/reward/std@1:0.000 - val-core/jec-qa-1-multi-choice/reward/best@1/mean:1.008 - val-core/jec-qa-1-multi-choice/reward/best@1/std:0.000 - val-aux/jec-qa-1-multi-choice/reward/worst@1/mean:1.008 - val-aux/jec-qa-1-multi-choice/reward/worst@1/std:0.000 - val-aux/jec-qa-1-multi-choice/score/mean@1:1.008 - val-aux/jec-qa-1-multi-choice/score/std@1:0.000 - val-aux/jec-qa-1-multi-choice/score/best@1/mean:1.008 - val-aux/jec-qa-1-multi-choice/score/best@1/std:0.000 - val-aux/jec-qa-1-multi-choice/score/worst@1/mean:1.008 - val-aux/jec-qa-1-multi-choice/score/worst@1/std:0.000 - val-aux/jec-qa-1-multi-choice/extra_info/score@1:0.000 - val-aux/jec-qa-1-multi-choice/extra_info/score/std@1:0.000
Error executing job with overrides: ['algorithm.adv_estimator=grpo', 'data.train_files=/home/wangyc/verl/data/jec-qa-1-multi-choice/train.parquet', 'data.val_files=/home/wangyc/verl/data/jec-qa-1-multi-choice/test.parquet', 'data.train_batch_size=64', 'data.val_batch_size=1312', 'data.max_prompt_length=1024', 'data.max_response_length=1024', 'actor_rollout_ref.model.path=/home/wangyc/verl/Qwen/Qwen2.5-7B-Instruct', 'actor_rollout_ref.actor.optim.lr=1e-6', 'actor_rollout_ref.model.use_remove_padding=True', 'actor_rollout_ref.actor.ppo_mini_batch_size=32', 'actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=8', 'actor_rollout_ref.actor.loss_agg_mode=seq-mean-token-sum-norm', 'actor_rollout_ref.actor.use_kl_loss=True', 'actor_rollout_ref.actor.kl_loss_coef=0.001', 'actor_rollout_ref.actor.kl_loss_type=low_var_kl', 'actor_rollout_ref.actor.entropy_coeff=0', 'actor_rollout_ref.model.enable_gradient_checkpointing=True', 'actor_rollout_ref.actor.fsdp_config.param_offload=False', 'actor_rollout_ref.actor.fsdp_config.optimizer_offload=False', 'actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=32', 'actor_rollout_ref.rollout.tensor_model_parallel_size=2', 'actor_rollout_ref.rollout.name=vllm', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.6', 'actor_rollout_ref.rollout.n=6', 'actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=32', 'actor_rollout_ref.ref.fsdp_config.param_offload=True', 'algorithm.use_kl_in_reward=False', 'algorithm.norm_adv_by_std_in_grpo=False', 'trainer.critic_warmup=0', "trainer.logger=['console','wandb']", 'trainer.project_name=Lawyer-Zero', 'trainer.experiment_name=qwen2.5-7b-grpo-mcq', 'trainer.n_gpus_per_node=4', 'trainer.nnodes=1', 'trainer.default_local_dir=checkpoints/qwen2.5-7b-grpo-hard-mcq', 'trainer.save_freq=50', 'trainer.test_freq=10', 'trainer.total_epochs=2']
Traceback (most recent call last):
  File "/home/wangyc/verl/verl/trainer/main_ppo.py", line 63, in main
    run_ppo(config)
  File "/home/wangyc/verl/verl/trainer/main_ppo.py", line 80, in run_ppo
    ray.get(runner.run.remote(config))
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/ray/_private/worker.py", line 2771, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/ray/_private/worker.py", line 919, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::TaskRunner.run()[39m (pid=3162693, ip=222.29.51.203, actor_id=b685f3d413d1595a093952ee01000000, repr=<main_ppo.TaskRunner object at 0x7f7ff251e9b0>)
  File "/home/wangyc/verl/verl/trainer/main_ppo.py", line 208, in run
    trainer.fit()
  File "/home/wangyc/verl/verl/trainer/ppo/ray_trainer.py", line 928, in fit
    gen_batch_output = self.actor_rollout_wg.generate_sequences(gen_batch)
  File "/home/wangyc/verl/verl/single_controller/ray/base.py", line 43, in func
    output = ray.get(output)
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::WorkerDict.actor_rollout_generate_sequences()[39m (pid=3173524, ip=222.29.51.203, actor_id=22980d8c80ff9ea65f36ff8d01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f98548f9630>)
  File "/home/wangyc/verl/verl/single_controller/ray/base.py", line 440, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
  File "/home/wangyc/verl/verl/single_controller/base/decorator.py", line 413, in inner
    return func(*args, **kwargs)
  File "/home/wangyc/verl/verl/workers/fsdp_workers.py", line 560, in generate_sequences
    with self.rollout_sharding_manager:
  File "/home/wangyc/verl/verl/utils/debug/performance.py", line 61, in f
    return self.log(decorated_function, *args, **kwargs)
  File "/home/wangyc/verl/verl/utils/debug/performance.py", line 70, in log
    output = func(*args, **kwargs)
  File "/home/wangyc/verl/verl/workers/sharding_manager/fsdp_vllm.py", line 90, in __enter__
    params = self.module.state_dict()
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2216, in state_dict
    module.state_dict(
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2216, in state_dict
    module.state_dict(
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2216, in state_dict
    module.state_dict(
  [Previous line repeated 1 more time]
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2222, in state_dict
    hook_result = hook(self, destination, prefix, local_metadata)
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py", line 724, in _post_state_dict_hook
    processed_state_dict = _post_state_dict_hook_fn[fsdp_state._state_dict_type](
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py", line 569, in _sharded_post_state_dict_hook
    return _common_unshard_post_state_dict_hook(
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py", line 238, in _common_unshard_post_state_dict_hook
    param_hook(state_dict, prefix, fqn)
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py", line 559, in param_hook
    sharded_tensor = _ext_chunk_dtensor(
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_fsdp_extensions.py", line 150, in _ext_chunk_dtensor
    return chunk_dtensor_fn(
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_shard_utils.py", line 102, in _create_chunk_dtensor
    tensor = tensor.detach().clone()
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 44.53 GiB of which 203.50 MiB is free. Including non-PyTorch memory, this process has 15.22 GiB memory in use. Process 3251361 has 29.07 GiB memory in use. Of the allocated memory 25.12 GiB is allocated by PyTorch, with 158.97 MiB allocated in private pools (e.g., CUDA Graphs), and 331.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
