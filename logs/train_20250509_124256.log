2025-05-09 12:43:08,619	INFO worker.py:1832 -- Started a local Ray instance. View the dashboard at [1m[32m127.0.0.1:8265 [39m[22m
[36m(TaskRunner pid=2738500)[0m {'actor_rollout_ref': {'actor': {'checkpoint': {'contents': ['model',
[36m(TaskRunner pid=2738500)[0m                                                              'optimizer',
[36m(TaskRunner pid=2738500)[0m                                                              'extra']},
[36m(TaskRunner pid=2738500)[0m                                  'clip_ratio': 0.2,
[36m(TaskRunner pid=2738500)[0m                                  'clip_ratio_c': 3.0,
[36m(TaskRunner pid=2738500)[0m                                  'clip_ratio_high': 0.2,
[36m(TaskRunner pid=2738500)[0m                                  'clip_ratio_low': 0.2,
[36m(TaskRunner pid=2738500)[0m                                  'entropy_coeff': 0,
[36m(TaskRunner pid=2738500)[0m                                  'fsdp_config': {'fsdp_size': -1,
[36m(TaskRunner pid=2738500)[0m                                                  'optimizer_offload': False,
[36m(TaskRunner pid=2738500)[0m                                                  'param_offload': False,
[36m(TaskRunner pid=2738500)[0m                                                  'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=2738500)[0m                                  'grad_clip': 1.0,
[36m(TaskRunner pid=2738500)[0m                                  'kl_loss_coef': 0.001,
[36m(TaskRunner pid=2738500)[0m                                  'kl_loss_type': 'low_var_kl',
[36m(TaskRunner pid=2738500)[0m                                  'loss_agg_mode': 'seq-mean-token-sum-norm',
[36m(TaskRunner pid=2738500)[0m                                  'optim': {'lr': 1e-06,
[36m(TaskRunner pid=2738500)[0m                                            'lr_warmup_steps': -1,
[36m(TaskRunner pid=2738500)[0m                                            'lr_warmup_steps_ratio': 0.0,
[36m(TaskRunner pid=2738500)[0m                                            'min_lr_ratio': None,
[36m(TaskRunner pid=2738500)[0m                                            'total_training_steps': -1,
[36m(TaskRunner pid=2738500)[0m                                            'warmup_style': 'constant',
[36m(TaskRunner pid=2738500)[0m                                            'weight_decay': 0.01},
[36m(TaskRunner pid=2738500)[0m                                  'ppo_epochs': 1,
[36m(TaskRunner pid=2738500)[0m                                  'ppo_max_token_len_per_gpu': 16384,
[36m(TaskRunner pid=2738500)[0m                                  'ppo_micro_batch_size': None,
[36m(TaskRunner pid=2738500)[0m                                  'ppo_micro_batch_size_per_gpu': 16,
[36m(TaskRunner pid=2738500)[0m                                  'ppo_mini_batch_size': 64,
[36m(TaskRunner pid=2738500)[0m                                  'shuffle': False,
[36m(TaskRunner pid=2738500)[0m                                  'strategy': 'fsdp',
[36m(TaskRunner pid=2738500)[0m                                  'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=2738500)[0m                                  'use_dynamic_bsz': False,
[36m(TaskRunner pid=2738500)[0m                                  'use_kl_loss': True,
[36m(TaskRunner pid=2738500)[0m                                  'use_torch_compile': True},
[36m(TaskRunner pid=2738500)[0m                        'hybrid_engine': True,
[36m(TaskRunner pid=2738500)[0m                        'model': {'enable_gradient_checkpointing': True,
[36m(TaskRunner pid=2738500)[0m                                  'external_lib': None,
[36m(TaskRunner pid=2738500)[0m                                  'override_config': {},
[36m(TaskRunner pid=2738500)[0m                                  'path': '/home/wangyc/verl/Qwen/Qwen2.5-7B-Instruct',
[36m(TaskRunner pid=2738500)[0m                                  'use_liger': False,
[36m(TaskRunner pid=2738500)[0m                                  'use_remove_padding': True},
[36m(TaskRunner pid=2738500)[0m                        'ref': {'fsdp_config': {'param_offload': True,
[36m(TaskRunner pid=2738500)[0m                                                'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=2738500)[0m                                'log_prob_max_token_len_per_gpu': 16384,
[36m(TaskRunner pid=2738500)[0m                                'log_prob_micro_batch_size': None,
[36m(TaskRunner pid=2738500)[0m                                'log_prob_micro_batch_size_per_gpu': 32,
[36m(TaskRunner pid=2738500)[0m                                'log_prob_use_dynamic_bsz': False,
[36m(TaskRunner pid=2738500)[0m                                'strategy': 'fsdp',
[36m(TaskRunner pid=2738500)[0m                                'ulysses_sequence_parallel_size': 1},
[36m(TaskRunner pid=2738500)[0m                        'rollout': {'disable_log_stats': True,
[36m(TaskRunner pid=2738500)[0m                                    'do_sample': True,
[36m(TaskRunner pid=2738500)[0m                                    'dtype': 'bfloat16',
[36m(TaskRunner pid=2738500)[0m                                    'enable_chunked_prefill': True,
[36m(TaskRunner pid=2738500)[0m                                    'enforce_eager': True,
[36m(TaskRunner pid=2738500)[0m                                    'engine_kwargs': {'swap_space': None},
[36m(TaskRunner pid=2738500)[0m                                    'free_cache_engine': True,
[36m(TaskRunner pid=2738500)[0m                                    'gpu_memory_utilization': 0.6,
[36m(TaskRunner pid=2738500)[0m                                    'ignore_eos': False,
[36m(TaskRunner pid=2738500)[0m                                    'load_format': 'dummy_dtensor',
[36m(TaskRunner pid=2738500)[0m                                    'log_prob_max_token_len_per_gpu': 16384,
[36m(TaskRunner pid=2738500)[0m                                    'log_prob_micro_batch_size': None,
[36m(TaskRunner pid=2738500)[0m                                    'log_prob_micro_batch_size_per_gpu': 32,
[36m(TaskRunner pid=2738500)[0m                                    'log_prob_use_dynamic_bsz': False,
[36m(TaskRunner pid=2738500)[0m                                    'max_model_len': None,
[36m(TaskRunner pid=2738500)[0m                                    'max_num_batched_tokens': 8192,
[36m(TaskRunner pid=2738500)[0m                                    'max_num_seqs': 1024,
[36m(TaskRunner pid=2738500)[0m                                    'n': 6,
[36m(TaskRunner pid=2738500)[0m                                    'name': 'vllm',
[36m(TaskRunner pid=2738500)[0m                                    'prompt_length': 1024,
[36m(TaskRunner pid=2738500)[0m                                    'response_length': 1024,
[36m(TaskRunner pid=2738500)[0m                                    'temperature': 1.0,
[36m(TaskRunner pid=2738500)[0m                                    'tensor_model_parallel_size': 2,
[36m(TaskRunner pid=2738500)[0m                                    'top_k': -1,
[36m(TaskRunner pid=2738500)[0m                                    'top_p': 1,
[36m(TaskRunner pid=2738500)[0m                                    'use_fire_sampling': False,
[36m(TaskRunner pid=2738500)[0m                                    'val_kwargs': {'do_sample': False,
[36m(TaskRunner pid=2738500)[0m                                                   'n': 1,
[36m(TaskRunner pid=2738500)[0m                                                   'temperature': 0,
[36m(TaskRunner pid=2738500)[0m                                                   'top_k': -1,
[36m(TaskRunner pid=2738500)[0m                                                   'top_p': 1.0}}},
[36m(TaskRunner pid=2738500)[0m  'algorithm': {'adv_estimator': 'grpo',
[36m(TaskRunner pid=2738500)[0m                'gamma': 1.0,
[36m(TaskRunner pid=2738500)[0m                'kl_ctrl': {'horizon': 10000,
[36m(TaskRunner pid=2738500)[0m                            'kl_coef': 0.001,
[36m(TaskRunner pid=2738500)[0m                            'target_kl': 0.1,
[36m(TaskRunner pid=2738500)[0m DeprecationWarning: `ray.state.available_resources_per_node` is a private attribute and access will be removed in a future Ray version.
[36m(TaskRunner pid=2738500)[0m WARNING:2025-05-09 12:43:20,251:Waiting for register center actor ao1NqR_register_center to be ready. Elapsed time: 0 seconds out of 300 seconds.
[36m(WorkerDict pid=2747899)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=2747899)[0m Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[36m(TaskRunner pid=2738500)[0m                            'type': 'fixed'},
[36m(TaskRunner pid=2738500)[0m                'kl_penalty': 'kl',
[36m(TaskRunner pid=2738500)[0m                'lam': 1.0,
[36m(TaskRunner pid=2738500)[0m                'norm_adv_by_std_in_grpo': False,
[36m(TaskRunner pid=2738500)[0m                'use_kl_in_reward': False},
[36m(TaskRunner pid=2738500)[0m  'critic': {'checkpoint': {'contents': ['model', 'optimizer', 'extra']},
[36m(TaskRunner pid=2738500)[0m             'cliprange_value': 0.5,
[36m(TaskRunner pid=2738500)[0m             'forward_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=2738500)[0m             'forward_micro_batch_size': None,
[36m(TaskRunner pid=2738500)[0m             'forward_micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=2738500)[0m             'grad_clip': 1.0,
[36m(TaskRunner pid=2738500)[0m             'model': {'enable_gradient_checkpointing': True,
[36m(TaskRunner pid=2738500)[0m                       'external_lib': None,
[36m(TaskRunner pid=2738500)[0m                       'fsdp_config': {'fsdp_size': -1,
[36m(TaskRunner pid=2738500)[0m                                       'optimizer_offload': False,
[36m(TaskRunner pid=2738500)[0m                                       'param_offload': False,
[36m(TaskRunner pid=2738500)[0m                                       'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=2738500)[0m                       'override_config': {},
[36m(TaskRunner pid=2738500)[0m                       'path': '~/models/deepseek-llm-7b-chat',
[36m(TaskRunner pid=2738500)[0m                       'tokenizer_path': '/home/wangyc/verl/Qwen/Qwen2.5-7B-Instruct',
[36m(TaskRunner pid=2738500)[0m                       'use_remove_padding': False},
[36m(TaskRunner pid=2738500)[0m             'optim': {'lr': 1e-05,
[36m(TaskRunner pid=2738500)[0m                       'lr_warmup_steps_ratio': 0.0,
[36m(TaskRunner pid=2738500)[0m                       'min_lr_ratio': None,
[36m(TaskRunner pid=2738500)[0m                       'total_training_steps': -1,
[36m(TaskRunner pid=2738500)[0m                       'warmup_style': 'constant',
[36m(TaskRunner pid=2738500)[0m                       'weight_decay': 0.01},
[36m(TaskRunner pid=2738500)[0m             'ppo_epochs': 1,
[36m(TaskRunner pid=2738500)[0m             'ppo_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=2738500)[0m             'ppo_micro_batch_size': None,
[36m(TaskRunner pid=2738500)[0m             'ppo_micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=2738500)[0m             'ppo_mini_batch_size': 64,
[36m(TaskRunner pid=2738500)[0m             'rollout_n': 6,
[36m(TaskRunner pid=2738500)[0m             'shuffle': False,
[36m(TaskRunner pid=2738500)[0m             'strategy': 'fsdp',
[36m(TaskRunner pid=2738500)[0m             'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=2738500)[0m             'use_dynamic_bsz': False},
[36m(TaskRunner pid=2738500)[0m  'custom_reward_function': {'name': 'compute_score', 'path': None},
[36m(TaskRunner pid=2738500)[0m  'data': {'custom_cls': {'name': None, 'path': None},
[36m(TaskRunner pid=2738500)[0m           'filter_overlong_prompts': False,
[36m(TaskRunner pid=2738500)[0m           'filter_overlong_prompts_workers': 1,
[36m(TaskRunner pid=2738500)[0m           'image_key': 'images',
[36m(TaskRunner pid=2738500)[0m           'max_prompt_length': 1024,
[36m(TaskRunner pid=2738500)[0m           'max_response_length': 1024,
[36m(TaskRunner pid=2738500)[0m           'prompt_key': 'prompt',
[36m(TaskRunner pid=2738500)[0m           'return_raw_chat': False,
[36m(TaskRunner pid=2738500)[0m           'return_raw_input_ids': False,
[36m(TaskRunner pid=2738500)[0m           'reward_fn_key': 'data_source',
[36m(TaskRunner pid=2738500)[0m           'shuffle': True,
[36m(TaskRunner pid=2738500)[0m           'tokenizer': None,
[36m(TaskRunner pid=2738500)[0m           'train_batch_size': 128,
[36m(TaskRunner pid=2738500)[0m           'train_files': '/home/wangyc/verl/data/jec-qa-1-multi-choice/train.parquet',
[36m(TaskRunner pid=2738500)[0m           'truncation': 'error',
[36m(TaskRunner pid=2738500)[0m           'val_batch_size': 1312,
[36m(TaskRunner pid=2738500)[0m           'val_files': '/home/wangyc/verl/data/jec-qa-1-multi-choice/test.parquet',
[36m(TaskRunner pid=2738500)[0m           'video_key': 'videos'},
[36m(TaskRunner pid=2738500)[0m  'ray_init': {'num_cpus': None},
[36m(TaskRunner pid=2738500)[0m  'reward_model': {'enable': False,
[36m(TaskRunner pid=2738500)[0m                   'forward_max_token_len_per_gpu': 32768,
[36m(TaskRunner pid=2738500)[0m                   'max_length': None,
[36m(TaskRunner pid=2738500)[0m                   'micro_batch_size': None,
[36m(TaskRunner pid=2738500)[0m                   'micro_batch_size_per_gpu': None,
[36m(TaskRunner pid=2738500)[0m                   'model': {'external_lib': None,
[36m(TaskRunner pid=2738500)[0m                             'fsdp_config': {'fsdp_size': -1,
[36m(TaskRunner pid=2738500)[0m                                             'param_offload': False,
[36m(TaskRunner pid=2738500)[0m                                             'wrap_policy': {'min_num_params': 0}},
[36m(TaskRunner pid=2738500)[0m                             'input_tokenizer': '/home/wangyc/verl/Qwen/Qwen2.5-7B-Instruct',
[36m(TaskRunner pid=2738500)[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',
[36m(TaskRunner pid=2738500)[0m                             'use_remove_padding': False},
[36m(TaskRunner pid=2738500)[0m                   'reward_manager': 'naive',
[36m(TaskRunner pid=2738500)[0m                   'strategy': 'fsdp',
[36m(TaskRunner pid=2738500)[0m                   'ulysses_sequence_parallel_size': 1,
[36m(TaskRunner pid=2738500)[0m                   'use_dynamic_bsz': False},
[36m(TaskRunner pid=2738500)[0m  'trainer': {'balance_batch': True,
[36m(TaskRunner pid=2738500)[0m              'critic_warmup': 0,
[36m(TaskRunner pid=2738500)[0m              'default_hdfs_dir': None,
[36m(TaskRunner pid=2738500)[0m              'default_local_dir': 'checkpoints/qwen2.5-7b-grpo-hard-mcq',
[36m(TaskRunner pid=2738500)[0m              'del_local_ckpt_after_load': False,
[36m(TaskRunner pid=2738500)[0m              'experiment_name': 'qwen2.5-7b-grpo-mcq',
[36m(TaskRunner pid=2738500)[0m              'log_val_generations': 0,
[36m(TaskRunner pid=2738500)[0m              'logger': ['console', 'wandb'],
[36m(TaskRunner pid=2738500)[0m              'max_actor_ckpt_to_keep': None,
[36m(TaskRunner pid=2738500)[0m              'max_critic_ckpt_to_keep': None,
[36m(TaskRunner pid=2738500)[0m              'n_gpus_per_node': 8,
[36m(TaskRunner pid=2738500)[0m              'nnodes': 1,
[36m(TaskRunner pid=2738500)[0m              'project_name': 'Lawyer-Zero',
[36m(TaskRunner pid=2738500)[0m              'ray_wait_register_center_timeout': 300,
[36m(TaskRunner pid=2738500)[0m              'resume_from_path': None,
[36m(TaskRunner pid=2738500)[0m              'resume_mode': 'auto',
[36m(TaskRunner pid=2738500)[0m              'save_freq': 50,
[36m(TaskRunner pid=2738500)[0m              'test_freq': 10,
[36m(TaskRunner pid=2738500)[0m              'total_epochs': 2,
[36m(TaskRunner pid=2738500)[0m              'total_training_steps': None,
[36m(TaskRunner pid=2738500)[0m              'val_before_train': True}}
[36m(TaskRunner pid=2738500)[0m WARNING: val_batch_size is deprecated. Validation datasets are sent to inference engines as a whole batch, which will schedule the memory themselves.
[36m(TaskRunner pid=2738500)[0m [validate_config] All configuration checks passed successfully!
[36m(TaskRunner pid=2738500)[0m dataset len: 8448
[36m(TaskRunner pid=2738500)[0m dataset len: 2113
[36m(TaskRunner pid=2738500)[0m Size of train dataloader: 66
[36m(TaskRunner pid=2738500)[0m Total training steps: 132
[36m(WorkerDict pid=2747912)[0m Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 66.12it/s]
[36m(WorkerDict pid=2747899)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 64.71it/s]
[36m(WorkerDict pid=2747921)[0m [rank6]:[W509 12:43:35.294916780 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 6]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[36m(WorkerDict pid=2741586)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=2747917)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=2741586)[0m Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][32m [repeated 4x across cluster][0m
[36m(WorkerDict pid=2747909)[0m Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 66.33it/s][32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2747917)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 65.07it/s][32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2747917)[0m [rank5]:[W509 12:43:35.493391977 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 5]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=2747917)[0m Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:03<00:10,  3.49s/it]
[36m(WorkerDict pid=2747909)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=2747909)[0m Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=2747917)[0m Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:10<00:03,  3.56s/it][32m [repeated 16x across cluster][0m
[36m(WorkerDict pid=2747917)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:13<00:00,  3.45s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:13<00:00,  3.49s/it]
[36m(TaskRunner pid=2738500)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_init_model()[39m (pid=2747909, ip=222.29.51.203, actor_id=985db40993b65f726d08b16201000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f68a4ac08b0>)
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/verl/verl/single_controller/ray/base.py", line 440, in func
[36m(TaskRunner pid=2738500)[0m     return getattr(self.worker_dict[key], name)(*args, **kwargs)
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/verl/verl/single_controller/base/decorator.py", line 413, in inner
[36m(TaskRunner pid=2738500)[0m     return func(*args, **kwargs)
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/verl/verl/workers/fsdp_workers.py", line 440, in init_model
[36m(TaskRunner pid=2738500)[0m     self._build_model_optimizer(
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/verl/verl/workers/fsdp_workers.py", line 276, in _build_model_optimizer
[36m(TaskRunner pid=2738500)[0m     actor_module_fsdp = FSDP(
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 509, in __init__
[36m(TaskRunner pid=2738500)[0m     _init_param_handle_from_module(
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 589, in _init_param_handle_from_module
[36m(TaskRunner pid=2738500)[0m     _materialize_with_param_init_fn(
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 898, in _materialize_with_param_init_fn
[36m(TaskRunner pid=2738500)[0m     param_init_fn(module)
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/verl/verl/utils/fsdp_utils.py", line 35, in init_fn
[36m(TaskRunner pid=2738500)[0m     x = x.to_empty(device=torch.cuda.current_device(), recurse=False)
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1195, in to_empty
[36m(TaskRunner pid=2738500)[0m     return self._apply(
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
[36m(TaskRunner pid=2738500)[0m     param_applied = fn(param)
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1196, in <lambda>
[36m(TaskRunner pid=2738500)[0m     lambda t: torch.empty_like(t, device=device), recurse=recurse
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/_prims_common/wrappers.py", line 291, in _fn
[36m(TaskRunner pid=2738500)[0m     result = fn(*args, **kwargs)
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/_refs/__init__.py", line 5003, in empty_like
[36m(TaskRunner pid=2738500)[0m     return torch.empty_permuted(
[36m(TaskRunner pid=2738500)[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.03 GiB. GPU 0 has a total capacity of 44.53 GiB of which 1.23 GiB is free. Process 1579126 has 37.54 GiB memory in use. Including non-PyTorch memory, this process has 5.73 GiB memory in use. Of the allocated memory 5.09 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(WorkerDict pid=2747912)[0m Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:11<00:03,  3.78s/it][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=2747912)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:14<00:00,  3.62s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:14<00:00,  3.68s/it][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=2747921)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
[36m(WorkerDict pid=2741586)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=2741586)[0m   "architectures": [
[36m(WorkerDict pid=2741586)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=2741586)[0m   ],
[36m(WorkerDict pid=2741586)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=2741586)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=2741586)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=2741586)[0m   "hidden_size": 3584,
[36m(WorkerDict pid=2741586)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=2741586)[0m   "intermediate_size": 18944,
[36m(WorkerDict pid=2741586)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=2741586)[0m   "max_window_layers": 28,
[36m(WorkerDict pid=2741586)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=2741586)[0m   "num_attention_heads": 28,
[36m(WorkerDict pid=2741586)[0m   "num_hidden_layers": 28,
[36m(WorkerDict pid=2741586)[0m   "num_key_value_heads": 4,
[36m(WorkerDict pid=2741586)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=2741586)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=2741586)[0m   "rope_scaling": null,
[36m(WorkerDict pid=2741586)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=2741586)[0m   "sliding_window": 131072,
[36m(WorkerDict pid=2741586)[0m   "tie_word_embeddings": false,
[36m(WorkerDict pid=2741586)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=2741586)[0m   "transformers_version": "4.51.3",
[36m(WorkerDict pid=2741586)[0m   "use_cache": true,
[36m(WorkerDict pid=2741586)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=2741586)[0m   "vocab_size": 152064
[36m(WorkerDict pid=2741586)[0m }
[36m(WorkerDict pid=2741586)[0m 
[36m(WorkerDict pid=2741586)[0m NCCL version 2.21.5+cuda12.4
[36m(WorkerDict pid=2741586)[0m Qwen2ForCausalLM contains 7.62B parameters
[36m(WorkerDict pid=2741586)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f6ba8cfa830>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f6ba8cfa710>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=2747907)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=2747917)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(WorkerDict pid=2747917)[0m wrap_policy: functools.partial(<function _or_policy at 0x7ed43cc46830>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7ed43cc46710>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=2741586)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=2741586)[0m   "architectures": [
[36m(WorkerDict pid=2741586)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=2741586)[0m   ],
[36m(WorkerDict pid=2741586)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=2741586)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=2741586)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=2741586)[0m   "hidden_size": 3584,
[36m(WorkerDict pid=2741586)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=2741586)[0m   "intermediate_size": 18944,
[36m(WorkerDict pid=2741586)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=2741586)[0m   "max_window_layers": 28,
[36m(WorkerDict pid=2741586)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=2741586)[0m   "num_attention_heads": 28,
[36m(WorkerDict pid=2741586)[0m   "num_hidden_layers": 28,
[36m(WorkerDict pid=2741586)[0m   "num_key_value_heads": 4,
[36m(WorkerDict pid=2741586)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=2741586)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=2741586)[0m   "rope_scaling": null,
[36m(WorkerDict pid=2741586)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=2741586)[0m   "sliding_window": 131072,
[36m(WorkerDict pid=2741586)[0m   "tie_word_embeddings": false,
[36m(WorkerDict pid=2741586)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=2741586)[0m   "transformers_version": "4.51.3",
[36m(WorkerDict pid=2741586)[0m   "use_cache": true,
[36m(WorkerDict pid=2741586)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=2741586)[0m   "vocab_size": 152064
[36m(WorkerDict pid=2741586)[0m }
[36m(WorkerDict pid=2741586)[0m 
[36m(WorkerDict pid=2747899)[0m Actor use_remove_padding=True[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=2747909)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
[36m(WorkerDict pid=2747899)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
[36m(WorkerDict pid=2741586)[0m Qwen2ForCausalLM contains 7.62B parameters
[36m(WorkerDict pid=2741586)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f6ba8cfa830>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f6ba8cfa710>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=2747921)[0m Total steps: 132, num_warmup_steps: 0
[36m(WorkerDict pid=2747921)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=2747912)[0m Monkey patch _flash_attention_forward in transformers.integrations.flash_attention[32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=2747917)[0m wrap_policy: functools.partial(<function _or_policy at 0x7ed43cc46830>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7ed43cc46710>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=2747918)[0m Actor use_remove_padding=True
Error executing job with overrides: ['algorithm.adv_estimator=grpo', 'data.train_files=/home/wangyc/verl/data/jec-qa-1-multi-choice/train.parquet', 'data.val_files=/home/wangyc/verl/data/jec-qa-1-multi-choice/test.parquet', 'data.train_batch_size=128', 'data.val_batch_size=1312', 'data.max_prompt_length=1024', 'data.max_response_length=1024', 'actor_rollout_ref.model.path=/home/wangyc/verl/Qwen/Qwen2.5-7B-Instruct', 'actor_rollout_ref.actor.optim.lr=1e-6', 'actor_rollout_ref.model.use_remove_padding=True', 'actor_rollout_ref.actor.ppo_mini_batch_size=64', 'actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=16', 'actor_rollout_ref.actor.loss_agg_mode=seq-mean-token-sum-norm', 'actor_rollout_ref.actor.use_kl_loss=True', 'actor_rollout_ref.actor.kl_loss_coef=0.001', 'actor_rollout_ref.actor.kl_loss_type=low_var_kl', 'actor_rollout_ref.actor.entropy_coeff=0', 'actor_rollout_ref.model.enable_gradient_checkpointing=True', 'actor_rollout_ref.actor.fsdp_config.param_offload=False', 'actor_rollout_ref.actor.fsdp_config.optimizer_offload=False', 'actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=32', 'actor_rollout_ref.rollout.tensor_model_parallel_size=2', 'actor_rollout_ref.rollout.name=vllm', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.6', 'actor_rollout_ref.rollout.n=6', 'actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=32', 'actor_rollout_ref.ref.fsdp_config.param_offload=True', 'algorithm.use_kl_in_reward=False', 'algorithm.norm_adv_by_std_in_grpo=False', 'trainer.critic_warmup=0', "trainer.logger=['console','wandb']", 'trainer.project_name=Lawyer-Zero', 'trainer.experiment_name=qwen2.5-7b-grpo-mcq', 'trainer.n_gpus_per_node=8', 'trainer.nnodes=1', 'trainer.default_local_dir=checkpoints/qwen2.5-7b-grpo-hard-mcq', 'trainer.save_freq=50', 'trainer.test_freq=10', 'trainer.total_epochs=2']
Traceback (most recent call last):
  File "/home/wangyc/verl/verl/trainer/main_ppo.py", line 63, in main
    run_ppo(config)
  File "/home/wangyc/verl/verl/trainer/main_ppo.py", line 80, in run_ppo
    ray.get(runner.run.remote(config))
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/ray/_private/worker.py", line 2771, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/ray/_private/worker.py", line 919, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::TaskRunner.run()[39m (pid=2738500, ip=222.29.51.203, actor_id=521644a1d93c3f1decb6da7601000000, repr=<main_ppo.TaskRunner object at 0x7f83c415ea10>)
  File "/home/wangyc/verl/verl/trainer/main_ppo.py", line 207, in run
    trainer.init_workers()
  File "/home/wangyc/verl/verl/trainer/ppo/ray_trainer.py", line 734, in init_workers
    self.actor_rollout_wg.init_model()
  File "/home/wangyc/verl/verl/single_controller/ray/base.py", line 43, in func
    output = ray.get(output)
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::WorkerDict.actor_rollout_init_model()[39m (pid=2747899, ip=222.29.51.203, actor_id=128cb7a5d9b437c62fcce1d201000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f089eb14e20>)
  File "/home/wangyc/verl/verl/single_controller/ray/base.py", line 440, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
  File "/home/wangyc/verl/verl/single_controller/base/decorator.py", line 413, in inner
    return func(*args, **kwargs)
  File "/home/wangyc/verl/verl/workers/fsdp_workers.py", line 440, in init_model
    self._build_model_optimizer(
  File "/home/wangyc/verl/verl/workers/fsdp_workers.py", line 276, in _build_model_optimizer
    actor_module_fsdp = FSDP(
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 509, in __init__
    _init_param_handle_from_module(
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 589, in _init_param_handle_from_module
    _materialize_with_param_init_fn(
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 898, in _materialize_with_param_init_fn
    param_init_fn(module)
  File "/home/wangyc/verl/verl/utils/fsdp_utils.py", line 35, in init_fn
    x = x.to_empty(device=torch.cuda.current_device(), recurse=False)
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1195, in to_empty
    return self._apply(
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    param_applied = fn(param)
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1196, in <lambda>
    lambda t: torch.empty_like(t, device=device), recurse=recurse
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/_prims_common/wrappers.py", line 291, in _fn
    result = fn(*args, **kwargs)
  File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/_refs/__init__.py", line 5003, in empty_like
    return torch.empty_permuted(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.03 GiB. GPU 0 has a total capacity of 44.53 GiB of which 1.21 GiB is free. Process 581236 has 37.54 GiB memory in use. Including non-PyTorch memory, this process has 5.76 GiB memory in use. Of the allocated memory 5.09 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[36m(TaskRunner pid=2738500)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_init_model()[39m (pid=2747907, ip=222.29.51.203, actor_id=eccc2c0e80e6fce83954dd5001000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7fc8b1eace20>)
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/verl/verl/single_controller/ray/base.py", line 440, in func
[36m(TaskRunner pid=2738500)[0m     return getattr(self.worker_dict[key], name)(*args, **kwargs)
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/verl/verl/single_controller/base/decorator.py", line 413, in inner
[36m(TaskRunner pid=2738500)[0m     return func(*args, **kwargs)
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/verl/verl/workers/fsdp_workers.py", line 440, in init_model
[36m(TaskRunner pid=2738500)[0m     self._build_model_optimizer(
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/verl/verl/workers/fsdp_workers.py", line 276, in _build_model_optimizer
[36m(TaskRunner pid=2738500)[0m     actor_module_fsdp = FSDP(
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 509, in __init__
[36m(TaskRunner pid=2738500)[0m     _init_param_handle_from_module(
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 589, in _init_param_handle_from_module
[36m(TaskRunner pid=2738500)[0m     _materialize_with_param_init_fn(
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 898, in _materialize_with_param_init_fn
[36m(TaskRunner pid=2738500)[0m     param_init_fn(module)
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/verl/verl/utils/fsdp_utils.py", line 35, in init_fn
[36m(TaskRunner pid=2738500)[0m     x = x.to_empty(device=torch.cuda.current_device(), recurse=False)
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1195, in to_empty
[36m(TaskRunner pid=2738500)[0m     return self._apply(
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
[36m(TaskRunner pid=2738500)[0m     param_applied = fn(param)
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1196, in <lambda>
[36m(TaskRunner pid=2738500)[0m     lambda t: torch.empty_like(t, device=device), recurse=recurse
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/_prims_common/wrappers.py", line 291, in _fn
[36m(TaskRunner pid=2738500)[0m     result = fn(*args, **kwargs)
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/_refs/__init__.py", line 5003, in empty_like
[36m(TaskRunner pid=2738500)[0m     return torch.empty_permuted(
[36m(TaskRunner pid=2738500)[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.03 GiB. GPU 0 has a total capacity of 44.53 GiB of which 1.21 GiB is free. Process 1002884 has 37.54 GiB memory in use. Including non-PyTorch memory, this process has 5.76 GiB memory in use. Of the allocated memory 5.09 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(TaskRunner pid=2738500)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_init_model()[39m (pid=2741586, ip=222.29.51.203, actor_id=1c1276e253b5e558a26abef601000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f6ab2d21630>)
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/verl/verl/single_controller/ray/base.py", line 440, in func
[36m(TaskRunner pid=2738500)[0m     return getattr(self.worker_dict[key], name)(*args, **kwargs)
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/verl/verl/single_controller/base/decorator.py", line 413, in inner
[36m(TaskRunner pid=2738500)[0m     return func(*args, **kwargs)
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/verl/verl/workers/fsdp_workers.py", line 440, in init_model
[36m(TaskRunner pid=2738500)[0m     self._build_model_optimizer(
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/verl/verl/workers/fsdp_workers.py", line 276, in _build_model_optimizer
[36m(TaskRunner pid=2738500)[0m     actor_module_fsdp = FSDP(
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 509, in __init__
[36m(TaskRunner pid=2738500)[0m     _init_param_handle_from_module(
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 612, in _init_param_handle_from_module
[36m(TaskRunner pid=2738500)[0m     _move_module_to_device(
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 1005, in _move_module_to_device
[36m(TaskRunner pid=2738500)[0m     _move_states_to_device(params_to_move, bufs_to_move, device_from_device_id)
[36m(TaskRunner pid=2738500)[0m   File "/home/wangyc/miniconda3/envs/verl/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 1035, in _move_states_to_device
[36m(TaskRunner pid=2738500)[0m     param.data = param.to(device_from_device_id)
[36m(TaskRunner pid=2738500)[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.03 GiB. GPU 0 has a total capacity of 44.53 GiB of which 1.02 GiB is free. Process 560075 has 37.54 GiB memory in use. Including non-PyTorch memory, this process has 5.94 GiB memory in use. Of the allocated memory 3.05 GiB is allocated by PyTorch, and 2.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(WorkerDict pid=2747917)[0m Total steps: 132, num_warmup_steps: 0[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2747917)[0m Actor use_remove_padding=True[32m [repeated 2x across cluster][0m
